{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself before we start using other frameworks that hide some of the computation from you. It builds on the first lab, where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
        "\n",
        "All the frameworks for deep learning you will meet from now on use automatic differentiation (autodiff), so you do not have to code the backward step yourself. In this version of this lab, you will develop your own autodif implementation. We also have an optional [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_Python/2.2-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution.  If you already know how to program, most things about Python and NumPy can be found with Google searches.\n",
        "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Nanograd automatic differentiation framework\n",
        "2. Finite difference method\n",
        "3. Data generation\n",
        "4. Defining and initializing the network\n",
        "5. Forward pass\n",
        "6. Training loop \n",
        "7. Testing your model\n",
        "8. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyXeAA-HuT7s"
      },
      "source": [
        "# Nanograd automatic differention framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6UWKCLKubgA"
      },
      "source": [
        "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd4CoEBNzNWS"
      },
      "outputs": [],
      "source": [
        "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/3a1bf9e9e724da813bfccf91a6f309abdade9f39/nanograd.py\n",
        "\n",
        "from math import exp, log\n",
        "\n",
        "class Var:\n",
        "    \"\"\"\n",
        "    A variable which holds a float and enables gradient computations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, val: float, grad_fn=lambda: []):\n",
        "        assert type(val) == float\n",
        "        self.v = val\n",
        "        self.grad_fn = grad_fn\n",
        "        self.grad = 0.0\n",
        "\n",
        "    def backprop(self, bp):\n",
        "        self.grad += bp\n",
        "        for input, grad in self.grad_fn():\n",
        "            input.backprop(grad * bp)\n",
        "\n",
        "    def backward(self):\n",
        "        self.backprop(1.0)\n",
        "\n",
        "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
        "\n",
        "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        assert type(power) in {float, int}, \"power must be float or int\"\n",
        "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
        "\n",
        "    def __neg__(self: 'Var') -> 'Var':\n",
        "        return Var(-1.0) * self\n",
        "\n",
        "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self + (-other)\n",
        "\n",
        "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self * other ** -1\n",
        "\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "\n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
        "\n",
        "    def relu(self):\n",
        "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
        "\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])\n",
        "    \n",
        "    def tanh(self):\n",
        "        t = (exp(2 * self.v) - 1) / (exp(2 * self.v) + 1)\n",
        "        return Var(t, lambda: [(self, 1 - t**2)])\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        s = 1 / (1 + exp(-self.v))\n",
        "        return Var(s, lambda: [(self, s * (1 - s))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDX67D6jzcte"
      },
      "source": [
        "A few examples illustrate how we can use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk6PeLc3zwPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3416717b-7e2b-43b1-f58b-229bf40b5388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "f = a * b\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmKhYgsY0g_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9da88f-04b8-49f6-f12d-1875d9adc234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=14.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "c = a * b\n",
        "d = Var(9.0)\n",
        "e = a * d\n",
        "f = c + e\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3B6uEH140p"
      },
      "source": [
        "## Exercise a) What is being calculated?\n",
        "\n",
        "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?\n",
        "\n",
        "The output of the code is the representation of all the variables in the 2 expressions after the backward pass, with the value and the gradient.\n",
        "\n",
        "The first expression is $ f = a *b$ with $ a = 3, b = 5 $. Here we differentiate $a$ and $b$ with respect to $f$.\n",
        "\n",
        "The second expression is $ f = c + e = a*b + a*d $ with $ a=3, b=5, d=9 $. Here we differentiate $c$ and $e$ with respect to $f$ and subsequently we differentiate $b$ with respect to $c$, $a$ with respect to $c$ and $e$, $d$ with respect to $e$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_Q0t2I3Ruj"
      },
      "source": [
        "## Exercise b) How does the backward function work?\n",
        "\n",
        "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
        "\n",
        "Go through the following four steps and answer the questions on the way:\n",
        "\n",
        "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
        "\n",
        "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
        "\n",
        "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. \n",
        "\n",
        "4. Write down the sequence of calls to backprop.\n",
        "\n",
        "f.backward() ->\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f.backprop(1.0) [f.grad=1.0] -> \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a.backprop(5.0 * 1.0) [a.grad = 5.0]\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b.backprop(3.0 * 1.0) [b.grad = 3.0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idGr71jYXl26"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "import graphviz\n",
        "\n",
        "#logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
        "\n",
        "#graphviz.__version__, graphviz.version()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPe30Q2QXzeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "a2e8fb7b-f1ba-4224-a63b-507a81939013"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f5b9de6a750>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: first expression Pages: 1 -->\n<svg width=\"206pt\" height=\"98pt\"\n viewBox=\"0.00 0.00 205.99 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>first expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 201.9942,-94 201.9942,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-72\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- f=a*b -->\n<g id=\"node2\" class=\"node\">\n<title>f=a*b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"165.4971\" cy=\"-45\" rx=\"32.4942\" ry=\"32.4942\"/>\n<text text-anchor=\"middle\" x=\"165.4971\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f=a*b</text>\n</g>\n<!-- a&#45;&gt;f=a*b -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;f=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.7916,-68.7432C57.3815,-64.791 94.4929,-57.9976 123.5023,-52.6873\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.1803,-56.1215 133.3866,-50.878 122.9198,-49.2359 124.1803,-56.1215\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/da=b=5</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;f=a*b -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;f=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.8992,-21.0156C55.3802,-24.326 87.4369,-29.8561 115,-35 117.8004,-35.5226 120.6862,-36.0701 123.5929,-36.6282\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.9775,-40.074 133.4615,-38.5457 124.3127,-33.2025 122.9775,-40.074\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-38.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/db=a=3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
        "\n",
        "e1.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e1.attr('node', shape='circle')\n",
        "e1.edge('a', 'f=a*b', label='df/da=b=5')\n",
        "e1.edge('b', 'f=a*b', label='df/db=a=3')\n",
        "\n",
        "e1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nittR-mZFeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "955e7fdf-fe44-48e3-bdb2-2954b5483e74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f5b9de0b450>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: second expression Pages: 1 -->\n<svg width=\"281pt\" height=\"158pt\"\n viewBox=\"0.00 0.00 281.00 158.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 154)\">\n<title>second expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-154 277,-154 277,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- c -->\n<g id=\"node2\" class=\"node\">\n<title>c</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"137\" cy=\"-102\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">c</text>\n</g>\n<!-- a&#45;&gt;c -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.5589,-78.9839C55.0696,-83.4108 86.8223,-90.6151 109.6075,-95.7849\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.8425,-99.2002 119.3691,-97.9997 110.3914,-92.3737 108.8425,-99.2002\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/da=b</text>\n</g>\n<!-- e -->\n<g id=\"node4\" class=\"node\">\n<title>e</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"137\" cy=\"-48\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-44.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">e</text>\n</g>\n<!-- a&#45;&gt;e -->\n<g id=\"edge3\" class=\"edge\">\n<title>a&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.1093,-69.3663C41.0519,-67.518 47.7844,-65.5441 54,-64 72.2255,-59.4724 92.9698,-55.4733 109.1606,-52.6056\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.8139,-56.0446 119.0705,-50.8903 108.62,-49.1472 109.8139,-56.0446\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/da=d</text>\n</g>\n<!-- f -->\n<g id=\"node6\" class=\"node\">\n<title>f</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"255\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n</g>\n<!-- c&#45;&gt;f -->\n<g id=\"edge5\" class=\"edge\">\n<title>c&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M154.6658,-97.9578C173.8698,-93.5637 204.8151,-86.483 227.2661,-81.3459\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"228.2461,-84.7122 237.2134,-79.0698 226.6847,-77.8885 228.2461,-84.7122\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/dc=1</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-132\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-128.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;c -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.5589,-127.5734C55.0696,-122.6547 86.8223,-114.6498 109.6075,-108.9057\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.5281,-112.2832 119.3691,-106.4448 108.8169,-105.4955 110.5281,-112.2832\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-125.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/db=a</text>\n</g>\n<!-- e&#45;&gt;f -->\n<g id=\"edge6\" class=\"edge\">\n<title>e&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M155.0339,-50.9637C171.7278,-53.8381 197.1872,-58.555 219,-64 222.0098,-64.7513 225.1413,-65.6022 228.2457,-66.4899\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.3051,-69.8618 237.8887,-69.3744 229.3112,-63.1554 227.3051,-69.8618\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/de=1</text>\n</g>\n<!-- d -->\n<g id=\"node5\" class=\"node\">\n<title>d</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">d</text>\n</g>\n<!-- d&#45;&gt;e -->\n<g id=\"edge4\" class=\"edge\">\n<title>d&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.9889,-20.9104C52.9226,-23.8318 78.9119,-28.785 101,-35 104.0364,-35.8544 107.1845,-36.8389 110.2985,-37.8758\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.3565,-41.2547 119.9514,-41.2723 111.68,-34.6515 109.3565,-41.2547\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-38.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/dd=a</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
        "\n",
        "e2.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e2.attr('node', shape='circle')\n",
        "e2.edge('a', 'c', label='dc/da=b')\n",
        "e2.edge('b', 'c', label='dc/db=a')\n",
        "e2.edge('a', 'e', label='de/da=d')\n",
        "e2.edge('d', 'e', label='de/dd=a')\n",
        "e2.edge('c', 'f', label='df/dc=1')\n",
        "e2.edge('e', 'f', label='df/de=1')\n",
        "\n",
        "e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5oi21W4gpeM"
      },
      "source": [
        "## Exercise c) What happens if we run backward again?\n",
        "\n",
        "Try to execute the code below. Explain what happens.\n",
        "\n",
        "The gradient value increases each time by the first gradient value because for each backward run we are adding to the previous gradient (self.grad += bp). \n",
        "E.g. the gradient value of a goes like this: 14, 28, 42, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCtpJyr-gyX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ed28e0-dd72-43ad-e1a7-974d50845e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=28.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n"
          ]
        }
      ],
      "source": [
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bPVq2VhsP-"
      },
      "source": [
        "## Exercise d) Zero gradient\n",
        "\n",
        "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on.\n",
        "\n",
        "By backpropagating a -1.0 we are subtracting the same amount as the previous gradient value, so each Var's gradient becomes zero. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnyPDQx9lJe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a065c14-9f10-49da-fe9f-cb8059362fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n",
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(2.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)\n",
        "\n",
        "f.backprop(-1.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4057_ljNvWB"
      },
      "source": [
        "## Exercise e) Test correctness of derivatives with the finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
        "$$\n",
        "for $da \\ll 1$.\n",
        "\n",
        "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TGil92lSXDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eee37cb-4276-4b73-fde0-75880a4aa082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=2.0000, grad=1.0000)\n",
            "Var(v=17.0000, grad=1.0000)\n",
            "8.000000661922968\n"
          ]
        }
      ],
      "source": [
        "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
        "def f_function(a):\n",
        "  a = Var(a)\n",
        "  b = Var(5.0)\n",
        "  c = Var(2.0)\n",
        "  f = a * b + c\n",
        "  f.backward()\n",
        "  return a,b,c,f\n",
        "\n",
        "for v in f_function(3.0):\n",
        "  print(v)\n",
        "\n",
        "# Insert your finite difference code here\n",
        "def finite_difference(da=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    da:          The finite difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: numerical approximation to the derivative (float) \n",
        "    \"\"\"\n",
        "    \n",
        "    a = 4.0\n",
        "    fa_da = (a+da)**2         # <- Insert correct expression\n",
        "    fa = a**2               # <- Insert correct expression\n",
        "\n",
        "    finite_difference = (fa_da - fa) / da\n",
        "    \n",
        "    return finite_difference\n",
        "\n",
        "print(finite_difference())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZar5RKaUkg"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6yfMAQ8aduj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YabfD43ajNh"
      },
      "outputs": [],
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1oDngHLapIz"
      },
      "outputs": [],
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysfa3FsBavlm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "24aaf961-a7db-4e1f-91df-95879558f25e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zac2HHNlgbpm"
      },
      "outputs": [],
      "source": [
        "# convert from nparray to Var\n",
        "def nparray_to_Var(x):\n",
        "  if x.ndim==1:\n",
        "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
        "  else:\n",
        "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
        "  return y\n",
        "   \n",
        "x_train = nparray_to_Var(x_train)\n",
        "y_train = nparray_to_Var(y_train)\n",
        "x_validation = nparray_to_Var(x_validation)\n",
        "y_validation = nparray_to_Var(y_validation)\n",
        "x_test = nparray_to_Var(x_test)\n",
        "y_test = nparray_to_Var(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe"
      },
      "source": [
        "# Defining and initializing the network\n",
        "\n",
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
        "\n",
        "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij_ieRsAt7Xt"
      },
      "outputs": [],
      "source": [
        "class Initializer:\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb18N5phuIha"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class NormalInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, mean=0, std=0.1):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(0.0) for _ in range(n_out)]\n",
        "\n",
        "class ConstantInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, weight=1.0, bias=0.0):\n",
        "    self.weight = weight\n",
        "    self.bias = bias\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(self.bias) for _ in range(n_out)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOLYGnZKuM6W"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
        "        self.weights = initializer.init_weights(n_in, n_out)\n",
        "        self.bias = initializer.init_bias(n_out)\n",
        "        self.act_fn = act_fn\n",
        "    \n",
        "    def __repr__(self):    \n",
        "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
        "\n",
        "    def parameters(self) -> Sequence[Var]:\n",
        "      params = []\n",
        "      for r in self.weights:\n",
        "        params += r\n",
        "\n",
        "      return params + self.bias\n",
        "\n",
        "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
        "        # to the current layer matches the number of nodes in the current layer\n",
        "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "        weights = self.weights\n",
        "        out = []\n",
        "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "        # We therefore loop over the (number of) nodes in the current layer:\n",
        "        for j in range(len(weights[0])): \n",
        "            # Initialize the node value depending on its corresponding parameters.\n",
        "            node = self.bias[j] # <- Insert code\n",
        "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "            for i in range(len(single_input)):\n",
        "                node += weights[i][j] * single_input[i]  # <- Insert code\n",
        "            node = self.act_fn(node)\n",
        "            out.append(node)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIZPBpNI0pO"
      },
      "source": [
        "## Exercise f) Add more activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
        " \n",
        "Implement the following activation functions in the Var class:\n",
        "\n",
        "* Identity: $$\\mathrm{identity}(x) = x$$\n",
        "* Hyperbolic tangent: $$\\tanh(x)$$\n",
        "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
        "\n",
        "Hint: You can seek inspiration in the relu method in the Var class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8n_SKnIW2F"
      },
      "source": [
        "## Exercise g) Complete the forward pass\n",
        "\n",
        "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDEjtePxE7Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8aa02c0-d634-46e4-f4b0-a53863d75242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Var(v=-0.0200, grad=0.0000)], [Var(v=-0.0403, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0297, grad=0.0000)], [Var(v=-0.0274, grad=0.0000)], [Var(v=-0.0178, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0110, grad=0.0000)], [Var(v=-0.0309, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0248, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0133, grad=0.0000)], [Var(v=-0.0258, grad=0.0000)], [Var(v=-0.0476, grad=0.0000)], [Var(v=-0.0226, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0078, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0348, grad=0.0000)], [Var(v=-0.0351, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0451, grad=0.0000)], [Var(v=-0.0496, grad=0.0000)], [Var(v=-0.0053, grad=0.0000)], [Var(v=-0.0313, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0075, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0239, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0467, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0300, grad=0.0000)], [Var(v=-0.0284, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0290, grad=0.0000)], [Var(v=-0.0126, grad=0.0000)], [Var(v=-0.0011, grad=0.0000)], [Var(v=-0.0316, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0502, grad=0.0000)], [Var(v=-0.0441, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0165, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0293, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0142, grad=0.0000)], [Var(v=-0.0104, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0370, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0146, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0287, grad=0.0000)], [Var(v=-0.0168, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0454, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0473, grad=0.0000)], [Var(v=-0.0072, grad=0.0000)], [Var(v=-0.0043, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0171, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0268, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0123, grad=0.0000)], [Var(v=-0.0020, grad=0.0000)], [Var(v=-0.0428, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0281, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0136, grad=0.0000)], [Var(v=-0.0399, grad=0.0000)]]\n"
          ]
        }
      ],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "def forward(input, network):\n",
        "\n",
        "  def forward_single(x, network):\n",
        "    for layer in network:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  output = [ forward_single(input[n], network) for n in range(len(input))]\n",
        "  return output\n",
        "\n",
        "print(forward(x_train, NN))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm"
      },
      "source": [
        "## Exercise h) Print all network parameters\n",
        "\n",
        "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. \n",
        "\n",
        "Here we are using the __repr__ method in the DenseLayer class to print the weights and the biases for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iac-VwYGFtGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf37f74-50ee-4088-92db-1686a6384ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0:\n",
            "Weights: [[Var(v=0.0668, grad=0.0000), Var(v=0.0562, grad=0.0000), Var(v=0.0732, grad=0.0000), Var(v=-0.0030, grad=0.0000), Var(v=0.1414, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
            "Layer 1:\n",
            "Weights: [[Var(v=-0.0794, grad=0.0000)], [Var(v=-0.0218, grad=0.0000)], [Var(v=-0.2058, grad=0.0000)], [Var(v=-0.1522, grad=0.0000)], [Var(v=-0.0436, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
          ]
        }
      ],
      "source": [
        "# Insert code here and in the DenseLayer class\n",
        "for i in range(len(NN)):\n",
        "  print(f\"Layer {i}:\")\n",
        "  print(NN[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FcylHqLTl-Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fe49dd37-4233-4272-a919-ed0046175b21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5b9d8a52d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "\n",
        "# convert from Var to ndarray  \n",
        "def Var_to_nparray(x):\n",
        "  y = np.zeros((len(x),len(x[0])))\n",
        "  for i in range(len(x)):\n",
        "    for j in range(len(x[0])):\n",
        "      y[i,j] = x[i][j].v\n",
        "  return y\n",
        "\n",
        "# define 1-1 network with weight = 1 and relu activation \n",
        "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
        "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
        "\n",
        "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOL2UolJFtHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "5e78ba7d-76eb-4b90-aa94-2e6a290f279c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing all activation layers\n",
        "\n",
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"identity\": lambda x: x.identity(),\n",
        "    \"sigmoid\": lambda x: x.sigmoid(),\n",
        "    \"relu\": lambda x: x.relu(),\n",
        "    \"tanh\": lambda x: x.tanh()\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "The Glorot initialization has the form: \n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
        "\n",
        "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
        "\n",
        "The He initialization is very similar\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs"
      },
      "source": [
        "## Exercise i) Glorot and He initialization\n",
        " \n",
        "Using the Initializer class, implement functions that implement Glorot and He \n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
        "\n",
        "We could plot the weight gradients and see for example that with the Glorot initialization both the forward and backward variances remain constant.\n",
        "\n",
        "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyk01CgaFtGt"
      },
      "outputs": [],
      "source": [
        "## Glorot\n",
        "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
        "  std = (2 / (n_in + n_out)) ** (1 / 2) # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
        "\n",
        "## He\n",
        "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
        "  std = (2 / n_in) ** (1 / 2) # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk"
      },
      "source": [
        "## Exercise j) Forward pass unit test\n",
        "\n",
        "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
        "\n",
        "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0miqRUAFtHl"
      },
      "outputs": [],
      "source": [
        "# Insert code here\n",
        "x = np.linspace(-6, 6, 100)\n",
        "NN = [ DenseLayer(1, 1, lambda x: x.identity(), initializer = ConstantInitializer(1.0)) ] \n",
        "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
        "assert y.all() == x.all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp"
      },
      "source": [
        "# Loss functions\n",
        "\n",
        "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2eDYKvAFtHq"
      },
      "outputs": [],
      "source": [
        "def squared_loss(t, y):\n",
        "  \n",
        "  # add check that sizes agree\n",
        "  assert len(t) == len(y)\n",
        "  \n",
        "  def squared_loss_single(t, y):\n",
        "    Loss = Var(0.0)\n",
        "    for i in range(len(t)): # sum over outputs\n",
        "      Loss += (t[i]-y[i]) ** 2\n",
        "    return Loss\n",
        "\n",
        "  Loss = Var(0.0)\n",
        "  for n in range(len(t)): # sum over training data\n",
        "    Loss += squared_loss_single(t[n],y[n])\n",
        "  return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu"
      },
      "source": [
        "## Exercise k) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
        "$$\n",
        "with $p$ given by the the softmax function in terms of the logits $h$:\n",
        "$$\n",
        "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
        "$$\n",
        "Inserting $p$ in the expression for the loss gives\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
        "$$\n",
        "This is true for $t$ being a one-hot vector. \n",
        "\n",
        "Call the function to convince yourself it works. \n",
        "\n",
        "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
        "\n",
        "Help: You can add these methods in the Var class:\n",
        "\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "    \n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nMuxyfzFtHv"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(t, h):\n",
        "     \n",
        "    Loss = Var(0.0)\n",
        "    # Insert code here\n",
        "    def cross_entropy_loss_single(t, h):\n",
        "      Loss = Var(0.0)\n",
        "      sum_exp_h = Var(0.0)\n",
        "      for i in range(len(h)):\n",
        "        sum_exp_h += h[i].exp()\n",
        "      for i in range(len(h)):\n",
        "        Loss += -t[i]*h[i]\n",
        "      return Loss + sum_exp_h.log()\n",
        "\n",
        "    Loss = Var(0.0)\n",
        "    for n in range(len(t)): # sum over training data\n",
        "      Loss += cross_entropy_loss_single(t[n], h[n])\n",
        "    \n",
        "    return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy"
      },
      "source": [
        "# Backward pass\n",
        "\n",
        "Now the magic happens! We get the calculation of the gradients for free. Just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHyfPPI9Qqwu"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49biIAYKQ1oG"
      },
      "source": [
        "and the gradients will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGt1bq_Q7uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c40a8ef-4208-465e-a340-e422f656d694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 \n",
            " Weights: [[Var(v=-0.0077, grad=11.9970), Var(v=-0.0593, grad=3.2464), Var(v=-0.0091, grad=-8.5910), Var(v=0.1103, grad=11.4368), Var(v=-0.0039, grad=6.3598)]] Biases: [Var(v=0.0000, grad=-10.9042), Var(v=0.0000, grad=-2.9507), Var(v=0.0000, grad=7.8084), Var(v=0.0000, grad=9.8218), Var(v=0.0000, grad=-5.7805)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.1253, grad=0.7350)], [Var(v=-0.0339, grad=5.6819)], [Var(v=0.0897, grad=0.8720)], [Var(v=-0.1111, grad=-11.3538)], [Var(v=-0.0664, grad=0.3698)]] Biases: [Var(v=0.0000, grad=-1.3583)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9"
      },
      "source": [
        "# Backward pass unit test\n",
        "\n",
        "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks!\n",
        "\n",
        "We initialize again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ePmzBzRtdh"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 15, lambda x: x.relu()),\n",
        "    DenseLayer(15, 50, lambda x: x.relu()),\n",
        "    DenseLayer(50, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iRPiQ1ISHw"
      },
      "source": [
        "and make an update:\n",
        "\n",
        "We introduce a help function parameters to have a handle in all parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhAI7eyeznia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc62e45d-0ec2-484a-a8f9-0a939c3f9a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network before update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.2576, grad=-0.9637), Var(v=0.0182, grad=4.3745), Var(v=0.1423, grad=-6.3533), Var(v=-0.1186, grad=-3.9889), Var(v=-0.0184, grad=3.0956), Var(v=-0.0718, grad=2.0043), Var(v=-0.0897, grad=2.7483), Var(v=0.0356, grad=-8.3909), Var(v=-0.0041, grad=-4.6719), Var(v=0.1343, grad=-0.0225), Var(v=-0.0737, grad=-1.7055), Var(v=-0.0375, grad=3.3485), Var(v=-0.0441, grad=5.5311), Var(v=0.0312, grad=-4.3999), Var(v=0.1000, grad=-0.5867)]] Biases: [Var(v=0.0000, grad=0.8760), Var(v=0.0000, grad=3.7587), Var(v=0.0000, grad=-5.4590), Var(v=0.0000, grad=3.6257), Var(v=0.0000, grad=-2.8137), Var(v=0.0000, grad=-1.8218), Var(v=0.0000, grad=-2.4981), Var(v=0.0000, grad=-7.2098), Var(v=0.0000, grad=4.2466), Var(v=0.0000, grad=-0.0193), Var(v=0.0000, grad=1.5503), Var(v=0.0000, grad=-3.0437), Var(v=0.0000, grad=-5.0276), Var(v=0.0000, grad=-3.7805), Var(v=0.0000, grad=-0.5041)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0041, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=0.2036, grad=3.0602), Var(v=0.0188, grad=0.0000), Var(v=-0.0237, grad=-1.7465), Var(v=-0.1363, grad=0.0000), Var(v=-0.0500, grad=0.0000), Var(v=0.0834, grad=0.7736), Var(v=-0.1443, grad=0.0000), Var(v=-0.0363, grad=3.7686), Var(v=-0.0770, grad=0.0000), Var(v=-0.0590, grad=-3.0856), Var(v=-0.0732, grad=3.2791), Var(v=0.0478, grad=-1.1436), Var(v=-0.2103, grad=0.0000), Var(v=-0.0262, grad=0.0000), Var(v=0.0737, grad=2.5645), Var(v=-0.1888, grad=0.0000), Var(v=-0.0710, grad=3.2124), Var(v=-0.0008, grad=2.7965), Var(v=-0.0205, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.1150, grad=0.0000), Var(v=0.0373, grad=-0.9338), Var(v=0.0162, grad=0.0000), Var(v=0.0265, grad=3.3299), Var(v=-0.0723, grad=0.0000), Var(v=0.1901, grad=-4.9368), Var(v=0.0204, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=0.1319, grad=-0.1170), Var(v=0.0380, grad=0.0000), Var(v=0.1925, grad=3.4066), Var(v=-0.0387, grad=-2.3481), Var(v=0.0215, grad=-0.4792), Var(v=0.0715, grad=-1.2507), Var(v=-0.0084, grad=0.0000), Var(v=0.0877, grad=-1.7437), Var(v=0.0138, grad=-2.0088), Var(v=0.1772, grad=1.7304), Var(v=0.0021, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0099, grad=-1.8044), Var(v=0.0284, grad=-1.0651), Var(v=0.0235, grad=3.6483), Var(v=-0.1414, grad=0.0000), Var(v=0.1391, grad=-1.0609), Var(v=-0.2004, grad=0.0000), Var(v=-0.0194, grad=-2.4745)], [Var(v=-0.0285, grad=0.0000), Var(v=-0.0001, grad=-0.0790), Var(v=-0.0692, grad=0.0000), Var(v=0.0203, grad=-0.3424), Var(v=0.0958, grad=0.1289), Var(v=0.0375, grad=0.0000), Var(v=0.0597, grad=0.0000), Var(v=0.0423, grad=0.0000), Var(v=-0.0013, grad=0.1786), Var(v=-0.1504, grad=-0.2782), Var(v=-0.0088, grad=0.0000), Var(v=-0.0258, grad=0.2278), Var(v=-0.1054, grad=0.0000), Var(v=-0.0028, grad=0.0844), Var(v=0.0266, grad=0.0000), Var(v=-0.0780, grad=0.2754), Var(v=0.1450, grad=0.0000), Var(v=-0.3748, grad=0.0000), Var(v=0.1169, grad=-0.2372), Var(v=-0.0895, grad=-0.2065), Var(v=-0.0449, grad=-0.2693), Var(v=0.1507, grad=0.2114), Var(v=0.1083, grad=0.0549), Var(v=-0.0492, grad=0.0010), Var(v=-0.0905, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1409, grad=-0.2459), Var(v=-0.0489, grad=0.0000), Var(v=0.1042, grad=0.3645), Var(v=0.0281, grad=0.0000), Var(v=-0.0549, grad=0.1215), Var(v=-0.0283, grad=0.0000), Var(v=0.0235, grad=-0.2302), Var(v=-0.0823, grad=-0.2515), Var(v=0.0519, grad=0.1734), Var(v=-0.0447, grad=0.0354), Var(v=0.0527, grad=0.0000), Var(v=0.0564, grad=-0.0399), Var(v=0.1237, grad=0.1287), Var(v=-0.1543, grad=0.0000), Var(v=0.0654, grad=-0.1278), Var(v=0.3049, grad=0.0000), Var(v=0.1663, grad=0.0144), Var(v=-0.1218, grad=0.0000), Var(v=-0.0135, grad=0.0786), Var(v=0.0820, grad=-0.2694), Var(v=-0.1331, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=0.0691, grad=0.0000), Var(v=0.0869, grad=0.1827)], [Var(v=-0.0246, grad=0.0000), Var(v=0.0586, grad=-0.6166), Var(v=-0.0880, grad=0.0000), Var(v=0.1736, grad=-2.6709), Var(v=0.1178, grad=1.0059), Var(v=-0.1184, grad=0.0000), Var(v=-0.0834, grad=0.0000), Var(v=-0.0114, grad=0.0000), Var(v=-0.0928, grad=1.3931), Var(v=0.0828, grad=-2.1706), Var(v=-0.1222, grad=0.0000), Var(v=0.0076, grad=1.7773), Var(v=-0.0973, grad=0.0000), Var(v=0.0491, grad=0.6587), Var(v=-0.1636, grad=0.0000), Var(v=0.0101, grad=2.1486), Var(v=-0.1772, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.0237, grad=-1.8503), Var(v=0.1476, grad=-1.6107), Var(v=0.1351, grad=-2.1007), Var(v=0.0414, grad=1.6494), Var(v=0.1151, grad=0.4285), Var(v=0.1339, grad=0.0078), Var(v=0.0826, grad=0.0000), Var(v=-0.2547, grad=0.0000), Var(v=0.0956, grad=-1.9180), Var(v=-0.0667, grad=0.0000), Var(v=0.0369, grad=2.8435), Var(v=-0.2351, grad=0.0000), Var(v=-0.0148, grad=0.9480), Var(v=-0.0427, grad=0.0000), Var(v=0.0299, grad=-1.7960), Var(v=0.0209, grad=-1.9622), Var(v=0.0872, grad=1.3525), Var(v=0.0454, grad=0.2760), Var(v=0.0122, grad=0.0000), Var(v=0.1858, grad=-0.3111), Var(v=0.1092, grad=1.0044), Var(v=0.0919, grad=0.0000), Var(v=0.0678, grad=-0.9967), Var(v=-0.0673, grad=0.0000), Var(v=0.0683, grad=0.1124), Var(v=-0.1465, grad=0.0000), Var(v=0.1023, grad=0.6135), Var(v=0.0091, grad=-2.1014), Var(v=0.1312, grad=0.0000), Var(v=0.0307, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=0.1308, grad=1.4253)], [Var(v=-0.0325, grad=0.0000), Var(v=-0.1205, grad=0.0000), Var(v=0.0233, grad=1.4093), Var(v=-0.2196, grad=0.0000), Var(v=0.0912, grad=-0.8043), Var(v=0.1337, grad=0.0000), Var(v=-0.1130, grad=0.0000), Var(v=0.0073, grad=0.3563), Var(v=0.0406, grad=0.0000), Var(v=0.1805, grad=1.7355), Var(v=-0.0723, grad=0.0000), Var(v=0.0006, grad=-1.4210), Var(v=0.0987, grad=1.5101), Var(v=0.0602, grad=-0.5266), Var(v=0.0337, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=-0.0908, grad=1.1810), Var(v=-0.0521, grad=0.0000), Var(v=0.0275, grad=1.4793), Var(v=0.1359, grad=1.2878), Var(v=0.0269, grad=0.0000), Var(v=0.1218, grad=0.0000), Var(v=0.0245, grad=0.0000), Var(v=-0.1514, grad=0.0000), Var(v=-0.0114, grad=-0.4300), Var(v=-0.0355, grad=0.0000), Var(v=0.1612, grad=1.5335), Var(v=-0.1638, grad=0.0000), Var(v=0.0564, grad=-2.2734), Var(v=-0.1226, grad=0.0000), Var(v=-0.1305, grad=0.0000), Var(v=-0.0640, grad=-0.0539), Var(v=-0.0408, grad=0.0000), Var(v=0.0721, grad=1.5688), Var(v=0.0608, grad=-1.0813), Var(v=0.0927, grad=-0.2207), Var(v=-0.0554, grad=-0.5760), Var(v=0.0510, grad=0.0000), Var(v=0.0653, grad=-0.8030), Var(v=-0.0527, grad=-0.9251), Var(v=0.0440, grad=0.7969), Var(v=-0.0110, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=0.0135, grad=-0.8310), Var(v=0.0998, grad=-0.4905), Var(v=-0.0145, grad=1.6801), Var(v=0.0316, grad=0.0000), Var(v=-0.2746, grad=-0.4885), Var(v=-0.0208, grad=0.0000), Var(v=0.2582, grad=-1.1395)], [Var(v=-0.0485, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1626, grad=0.2189), Var(v=-0.0520, grad=0.0000), Var(v=-0.0355, grad=-0.1249), Var(v=0.0005, grad=0.0000), Var(v=0.0276, grad=0.0000), Var(v=0.0099, grad=0.0553), Var(v=0.1118, grad=0.0000), Var(v=-0.1060, grad=0.2695), Var(v=0.0315, grad=0.0000), Var(v=0.0554, grad=-0.2207), Var(v=-0.0139, grad=0.2345), Var(v=0.0548, grad=-0.0818), Var(v=-0.0301, grad=0.0000), Var(v=0.2480, grad=0.0000), Var(v=-0.0607, grad=0.1834), Var(v=0.0160, grad=0.0000), Var(v=0.1160, grad=0.2298), Var(v=-0.0078, grad=0.2000), Var(v=-0.1986, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=0.0513, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=0.0503, grad=-0.0668), Var(v=0.0447, grad=0.0000), Var(v=-0.0978, grad=0.2382), Var(v=-0.0628, grad=0.0000), Var(v=-0.1658, grad=-0.3531), Var(v=-0.1566, grad=0.0000), Var(v=0.0219, grad=0.0000), Var(v=-0.0344, grad=-0.0084), Var(v=0.1226, grad=0.0000), Var(v=0.0583, grad=0.2437), Var(v=0.0887, grad=-0.1679), Var(v=-0.1084, grad=-0.0343), Var(v=0.0141, grad=-0.0895), Var(v=-0.1120, grad=0.0000), Var(v=0.1310, grad=-0.1247), Var(v=-0.0108, grad=-0.1437), Var(v=-0.0879, grad=0.1238), Var(v=0.2348, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=0.1763, grad=-0.1291), Var(v=0.0977, grad=-0.0762), Var(v=0.1170, grad=0.2609), Var(v=0.0624, grad=0.0000), Var(v=0.1968, grad=-0.0759), Var(v=-0.0767, grad=0.0000), Var(v=-0.0820, grad=-0.1770)], [Var(v=-0.1277, grad=0.0000), Var(v=0.1632, grad=0.0000), Var(v=-0.0201, grad=0.8527), Var(v=-0.0175, grad=0.0000), Var(v=0.1703, grad=-0.4866), Var(v=0.0381, grad=0.0000), Var(v=-0.1618, grad=0.0000), Var(v=-0.0874, grad=0.2156), Var(v=-0.0123, grad=0.0000), Var(v=-0.0199, grad=1.0500), Var(v=-0.0821, grad=0.0000), Var(v=0.1501, grad=-0.8597), Var(v=0.0303, grad=0.9136), Var(v=-0.1220, grad=-0.3186), Var(v=0.0256, grad=0.0000), Var(v=0.0336, grad=0.0000), Var(v=0.0862, grad=0.7145), Var(v=0.1002, grad=0.0000), Var(v=0.1473, grad=0.8950), Var(v=-0.0640, grad=0.7792), Var(v=-0.0187, grad=0.0000), Var(v=0.0742, grad=0.0000), Var(v=-0.1355, grad=0.0000), Var(v=-0.1713, grad=0.0000), Var(v=0.0634, grad=-0.2602), Var(v=-0.0613, grad=0.0000), Var(v=-0.0910, grad=0.9278), Var(v=0.0369, grad=0.0000), Var(v=0.0569, grad=-1.3755), Var(v=0.0260, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0254, grad=-0.0326), Var(v=-0.1436, grad=0.0000), Var(v=-0.0148, grad=0.9492), Var(v=-0.0226, grad=-0.6542), Var(v=0.0504, grad=-0.1335), Var(v=-0.1716, grad=-0.3485), Var(v=-0.0673, grad=0.0000), Var(v=0.0371, grad=-0.4858), Var(v=-0.0468, grad=-0.5597), Var(v=0.0071, grad=0.4821), Var(v=-0.0273, grad=0.0000), Var(v=-0.1181, grad=0.0000), Var(v=0.0553, grad=-0.5028), Var(v=-0.1317, grad=-0.2968), Var(v=-0.0411, grad=1.0165), Var(v=0.0593, grad=0.0000), Var(v=0.0674, grad=-0.2956), Var(v=-0.2140, grad=0.0000), Var(v=-0.1131, grad=-0.6895)], [Var(v=0.0382, grad=0.0000), Var(v=-0.0167, grad=0.0000), Var(v=0.0148, grad=1.0660), Var(v=-0.1109, grad=0.0000), Var(v=0.0958, grad=-0.6084), Var(v=-0.1744, grad=0.0000), Var(v=-0.0791, grad=0.0000), Var(v=-0.0567, grad=0.2695), Var(v=-0.1071, grad=0.0000), Var(v=0.0218, grad=1.3128), Var(v=-0.0864, grad=0.0000), Var(v=0.1037, grad=-1.0749), Var(v=-0.0071, grad=1.1422), Var(v=0.0613, grad=-0.3984), Var(v=0.0069, grad=0.0000), Var(v=-0.2317, grad=0.0000), Var(v=0.1403, grad=0.8933), Var(v=-0.0045, grad=0.0000), Var(v=0.1302, grad=1.1190), Var(v=0.0150, grad=0.9741), Var(v=-0.0314, grad=0.0000), Var(v=0.1707, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=0.1495, grad=0.0000), Var(v=-0.0453, grad=-0.3253), Var(v=-0.1896, grad=0.0000), Var(v=-0.0924, grad=1.1600), Var(v=-0.0995, grad=0.0000), Var(v=0.0173, grad=-1.7197), Var(v=-0.0267, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0171, grad=-0.0408), Var(v=-0.0091, grad=0.0000), Var(v=-0.0430, grad=1.1867), Var(v=0.0788, grad=-0.8179), Var(v=0.2238, grad=-0.1669), Var(v=-0.0300, grad=-0.4357), Var(v=-0.1826, grad=0.0000), Var(v=-0.1139, grad=-0.6074), Var(v=0.1090, grad=-0.6998), Var(v=0.0441, grad=0.6028), Var(v=-0.1312, grad=0.0000), Var(v=-0.0721, grad=0.0000), Var(v=-0.0202, grad=-0.6286), Var(v=-0.0214, grad=-0.3710), Var(v=-0.0006, grad=1.2709), Var(v=-0.0772, grad=0.0000), Var(v=0.1003, grad=-0.3695), Var(v=-0.1435, grad=0.0000), Var(v=0.1161, grad=-0.8620)], [Var(v=-0.0678, grad=0.0000), Var(v=-0.2289, grad=-0.1544), Var(v=-0.0194, grad=0.0000), Var(v=0.1472, grad=-0.6688), Var(v=-0.0665, grad=0.2519), Var(v=0.0034, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=0.1695, grad=0.0000), Var(v=-0.1601, grad=0.3488), Var(v=-0.0351, grad=-0.5435), Var(v=-0.0574, grad=0.0000), Var(v=0.1626, grad=0.4450), Var(v=-0.0479, grad=0.0000), Var(v=0.1587, grad=0.1649), Var(v=-0.0512, grad=0.0000), Var(v=-0.0645, grad=0.5380), Var(v=-0.1293, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=0.1873, grad=-0.4633), Var(v=0.1758, grad=-0.4033), Var(v=0.0659, grad=-0.5260), Var(v=0.0801, grad=0.4130), Var(v=0.1120, grad=0.1073), Var(v=0.0066, grad=0.0020), Var(v=-0.0262, grad=0.0000), Var(v=0.0242, grad=0.0000), Var(v=-0.0624, grad=-0.4803), Var(v=0.0166, grad=0.0000), Var(v=0.0359, grad=0.7120), Var(v=-0.0433, grad=0.0000), Var(v=-0.0813, grad=0.2374), Var(v=-0.1046, grad=0.0000), Var(v=-0.0851, grad=-0.4497), Var(v=-0.0525, grad=-0.4913), Var(v=-0.1277, grad=0.3386), Var(v=0.0662, grad=0.0691), Var(v=-0.0703, grad=0.0000), Var(v=0.0921, grad=-0.0779), Var(v=-0.0477, grad=0.2515), Var(v=0.1512, grad=0.0000), Var(v=0.0341, grad=-0.2496), Var(v=-0.2140, grad=0.0000), Var(v=-0.0672, grad=0.0281), Var(v=0.0649, grad=0.0000), Var(v=-0.0713, grad=0.1536), Var(v=0.1535, grad=-0.5262), Var(v=-0.0122, grad=0.0000), Var(v=-0.1954, grad=0.0000), Var(v=0.0343, grad=0.0000), Var(v=-0.1101, grad=0.3569)], [Var(v=-0.0813, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=-0.0202, grad=0.0482), Var(v=0.0004, grad=0.0000), Var(v=0.0022, grad=-0.0275), Var(v=-0.1260, grad=0.0000), Var(v=0.1035, grad=0.0000), Var(v=0.1514, grad=0.0122), Var(v=-0.0923, grad=0.0000), Var(v=0.0126, grad=0.0594), Var(v=0.0353, grad=0.0000), Var(v=-0.0131, grad=-0.0486), Var(v=0.0312, grad=0.0517), Var(v=-0.1120, grad=-0.0180), Var(v=-0.0792, grad=0.0000), Var(v=-0.1545, grad=0.0000), Var(v=-0.0167, grad=0.0404), Var(v=0.0030, grad=0.0000), Var(v=0.0477, grad=0.0506), Var(v=-0.0626, grad=0.0441), Var(v=0.0139, grad=0.0000), Var(v=0.0116, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0989, grad=0.0000), Var(v=-0.1970, grad=-0.0147), Var(v=0.0626, grad=0.0000), Var(v=0.0072, grad=0.0525), Var(v=0.1502, grad=0.0000), Var(v=-0.0330, grad=-0.0778), Var(v=0.0590, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=0.0883, grad=-0.0018), Var(v=0.0493, grad=0.0000), Var(v=-0.0677, grad=0.0537), Var(v=-0.0455, grad=-0.0370), Var(v=0.1195, grad=-0.0076), Var(v=0.0741, grad=-0.0197), Var(v=0.1545, grad=0.0000), Var(v=0.1350, grad=-0.0275), Var(v=-0.0586, grad=-0.0317), Var(v=-0.0188, grad=0.0273), Var(v=0.0911, grad=0.0000), Var(v=-0.0256, grad=0.0000), Var(v=0.1966, grad=-0.0284), Var(v=-0.0560, grad=-0.0168), Var(v=0.2827, grad=0.0575), Var(v=0.0665, grad=0.0000), Var(v=0.0039, grad=-0.0167), Var(v=0.0646, grad=0.0000), Var(v=-0.0930, grad=-0.0390)], [Var(v=-0.1410, grad=0.0000), Var(v=0.1393, grad=-0.5820), Var(v=-0.0403, grad=0.0000), Var(v=-0.0521, grad=-2.5207), Var(v=-0.0616, grad=0.9494), Var(v=0.1510, grad=0.0000), Var(v=0.0460, grad=0.0000), Var(v=-0.1218, grad=0.0000), Var(v=0.1578, grad=1.3148), Var(v=0.0883, grad=-2.0486), Var(v=-0.0891, grad=0.0000), Var(v=0.0281, grad=1.6774), Var(v=-0.1346, grad=0.0000), Var(v=-0.1185, grad=0.6217), Var(v=0.0354, grad=0.0000), Var(v=0.2231, grad=2.0279), Var(v=0.1338, grad=0.0000), Var(v=0.0658, grad=0.0000), Var(v=-0.0187, grad=-1.7463), Var(v=-0.1616, grad=-1.5202), Var(v=0.0488, grad=-1.9826), Var(v=-0.1032, grad=1.5567), Var(v=-0.1003, grad=0.4044), Var(v=-0.0982, grad=0.0074), Var(v=-0.0980, grad=0.0000), Var(v=-0.0238, grad=0.0000), Var(v=0.0846, grad=-1.8102), Var(v=-0.0310, grad=0.0000), Var(v=-0.0011, grad=2.6837), Var(v=0.1276, grad=0.0000), Var(v=0.1385, grad=0.8947), Var(v=0.0793, grad=0.0000), Var(v=0.0677, grad=-1.6951), Var(v=0.1427, grad=-1.8519), Var(v=0.0777, grad=1.2764), Var(v=0.1135, grad=0.2605), Var(v=-0.1131, grad=0.0000), Var(v=0.0729, grad=-0.2936), Var(v=0.0748, grad=0.9479), Var(v=-0.1344, grad=0.0000), Var(v=-0.0634, grad=-0.9407), Var(v=-0.1124, grad=0.0000), Var(v=0.2109, grad=0.1061), Var(v=0.0492, grad=0.0000), Var(v=0.0382, grad=0.5790), Var(v=0.1207, grad=-1.9833), Var(v=-0.0450, grad=0.0000), Var(v=0.0310, grad=0.0000), Var(v=-0.0809, grad=0.0000), Var(v=-0.0435, grad=1.3452)], [Var(v=0.0180, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.0054, grad=0.8751), Var(v=-0.1866, grad=0.0000), Var(v=-0.1513, grad=-0.4994), Var(v=0.1177, grad=0.0000), Var(v=-0.0163, grad=0.0000), Var(v=-0.0503, grad=0.2212), Var(v=-0.1684, grad=0.0000), Var(v=-0.1960, grad=1.0777), Var(v=-0.2002, grad=0.0000), Var(v=0.1571, grad=-0.8824), Var(v=0.1135, grad=0.9377), Var(v=-0.0295, grad=-0.3270), Var(v=0.0284, grad=0.0000), Var(v=0.0294, grad=0.0000), Var(v=0.0389, grad=0.7334), Var(v=0.0557, grad=0.0000), Var(v=0.1802, grad=0.9186), Var(v=-0.0487, grad=0.7997), Var(v=0.0871, grad=0.0000), Var(v=-0.2544, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=0.0140, grad=0.0000), Var(v=-0.0797, grad=-0.2670), Var(v=0.1245, grad=0.0000), Var(v=0.0883, grad=0.9522), Var(v=-0.1244, grad=0.0000), Var(v=0.0201, grad=-1.4117), Var(v=0.0668, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=0.2530, grad=-0.0335), Var(v=-0.0452, grad=0.0000), Var(v=-0.0463, grad=0.9742), Var(v=0.0048, grad=-0.6715), Var(v=0.0183, grad=-0.1370), Var(v=0.0253, grad=-0.3577), Var(v=-0.1136, grad=0.0000), Var(v=-0.0094, grad=-0.4986), Var(v=-0.0446, grad=-0.5744), Var(v=-0.0561, grad=0.4948), Var(v=0.0954, grad=0.0000), Var(v=0.1029, grad=0.0000), Var(v=-0.0092, grad=-0.5160), Var(v=-0.0186, grad=-0.3046), Var(v=0.0686, grad=1.0433), Var(v=0.0447, grad=0.0000), Var(v=-0.0660, grad=-0.3034), Var(v=-0.0424, grad=0.0000), Var(v=-0.0444, grad=-0.7076)], [Var(v=0.1217, grad=0.0000), Var(v=-0.0701, grad=0.0000), Var(v=-0.1131, grad=0.4456), Var(v=-0.1082, grad=0.0000), Var(v=0.1065, grad=-0.2543), Var(v=-0.0275, grad=0.0000), Var(v=-0.0139, grad=0.0000), Var(v=0.0488, grad=0.1127), Var(v=0.0807, grad=0.0000), Var(v=0.0255, grad=0.5488), Var(v=-0.1079, grad=0.0000), Var(v=0.1314, grad=-0.4493), Var(v=-0.0022, grad=0.4775), Var(v=-0.1939, grad=-0.1665), Var(v=-0.0427, grad=0.0000), Var(v=-0.0259, grad=0.0000), Var(v=-0.1213, grad=0.3735), Var(v=-0.0778, grad=0.0000), Var(v=-0.0321, grad=0.4678), Var(v=0.1063, grad=0.4072), Var(v=-0.0315, grad=0.0000), Var(v=-0.0715, grad=0.0000), Var(v=-0.0731, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=-0.0349, grad=-0.1360), Var(v=-0.0685, grad=0.0000), Var(v=-0.3023, grad=0.4849), Var(v=-0.1867, grad=0.0000), Var(v=-0.0522, grad=-0.7189), Var(v=0.0433, grad=0.0000), Var(v=0.2106, grad=0.0000), Var(v=-0.1370, grad=-0.0170), Var(v=-0.0985, grad=0.0000), Var(v=0.1795, grad=0.4961), Var(v=0.0903, grad=-0.3419), Var(v=-0.0449, grad=-0.0698), Var(v=0.1645, grad=-0.1821), Var(v=-0.0453, grad=0.0000), Var(v=-0.0381, grad=-0.2539), Var(v=0.0148, grad=-0.2925), Var(v=-0.2603, grad=0.2520), Var(v=-0.0903, grad=0.0000), Var(v=0.1531, grad=0.0000), Var(v=-0.1227, grad=-0.2628), Var(v=-0.1589, grad=-0.1551), Var(v=0.0842, grad=0.5313), Var(v=0.0694, grad=0.0000), Var(v=0.0587, grad=-0.1545), Var(v=-0.0888, grad=0.0000), Var(v=-0.0432, grad=-0.3603)], [Var(v=-0.1800, grad=0.0000), Var(v=-0.0102, grad=0.0000), Var(v=-0.1794, grad=0.5242), Var(v=-0.0277, grad=0.0000), Var(v=0.1591, grad=-0.2992), Var(v=0.1367, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.0585, grad=0.1325), Var(v=-0.0222, grad=0.0000), Var(v=0.1893, grad=0.6455), Var(v=0.0440, grad=0.0000), Var(v=-0.0142, grad=-0.5286), Var(v=0.0158, grad=0.5617), Var(v=0.0362, grad=-0.1959), Var(v=-0.0652, grad=0.0000), Var(v=0.0770, grad=0.0000), Var(v=-0.0452, grad=0.4393), Var(v=0.1316, grad=0.0000), Var(v=-0.0595, grad=0.5503), Var(v=-0.1210, grad=0.4790), Var(v=0.0588, grad=0.0000), Var(v=-0.1811, grad=0.0000), Var(v=-0.0226, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.0018, grad=-0.1600), Var(v=-0.1500, grad=0.0000), Var(v=0.0039, grad=0.5704), Var(v=0.0849, grad=0.0000), Var(v=0.0301, grad=-0.8456), Var(v=0.0388, grad=0.0000), Var(v=0.1768, grad=0.0000), Var(v=0.0092, grad=-0.0200), Var(v=0.0213, grad=0.0000), Var(v=0.0330, grad=0.5835), Var(v=0.0981, grad=-0.4022), Var(v=0.0653, grad=-0.0821), Var(v=0.1238, grad=-0.2142), Var(v=-0.0395, grad=0.0000), Var(v=-0.0600, grad=-0.2987), Var(v=0.1110, grad=-0.3441), Var(v=0.0271, grad=0.2964), Var(v=-0.0257, grad=0.0000), Var(v=0.0134, grad=0.0000), Var(v=0.0763, grad=-0.3091), Var(v=0.1342, grad=-0.1824), Var(v=-0.0064, grad=0.6249), Var(v=-0.0938, grad=0.0000), Var(v=0.0544, grad=-0.1817), Var(v=0.0534, grad=0.0000), Var(v=-0.0812, grad=-0.4239)], [Var(v=-0.0702, grad=0.0000), Var(v=-0.0114, grad=-0.1354), Var(v=0.0272, grad=0.0000), Var(v=0.1497, grad=-0.5864), Var(v=-0.0119, grad=0.2208), Var(v=0.0615, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=-0.0404, grad=0.0000), Var(v=0.1052, grad=0.3058), Var(v=-0.0525, grad=-0.4765), Var(v=0.0190, grad=0.0000), Var(v=-0.1472, grad=0.3902), Var(v=0.2092, grad=0.0000), Var(v=0.1437, grad=0.1446), Var(v=-0.0012, grad=0.0000), Var(v=0.0956, grad=0.4717), Var(v=0.1141, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.1152, grad=-0.4062), Var(v=0.0204, grad=-0.3536), Var(v=0.1223, grad=-0.4612), Var(v=-0.0238, grad=0.3621), Var(v=-0.1339, grad=0.0941), Var(v=0.0788, grad=0.0017), Var(v=-0.1120, grad=0.0000), Var(v=0.1700, grad=0.0000), Var(v=0.1950, grad=-0.4211), Var(v=-0.1152, grad=0.0000), Var(v=-0.0238, grad=0.6243), Var(v=0.0619, grad=0.0000), Var(v=0.0522, grad=0.2081), Var(v=0.1708, grad=0.0000), Var(v=-0.0650, grad=-0.3943), Var(v=-0.0791, grad=-0.4308), Var(v=-0.0309, grad=0.2969), Var(v=-0.2467, grad=0.0606), Var(v=-0.2401, grad=0.0000), Var(v=0.0027, grad=-0.0683), Var(v=0.0008, grad=0.2205), Var(v=-0.1162, grad=0.0000), Var(v=0.0728, grad=-0.2188), Var(v=-0.1281, grad=0.0000), Var(v=-0.1130, grad=0.0247), Var(v=0.0595, grad=0.0000), Var(v=0.0347, grad=0.1347), Var(v=-0.0180, grad=-0.4613), Var(v=-0.0250, grad=0.0000), Var(v=-0.1540, grad=0.0000), Var(v=-0.0671, grad=0.0000), Var(v=-0.0645, grad=0.3129)], [Var(v=0.0142, grad=0.0000), Var(v=-0.0465, grad=-0.4332), Var(v=0.0621, grad=0.0000), Var(v=-0.1047, grad=-1.8762), Var(v=0.0702, grad=0.7067), Var(v=-0.2445, grad=0.0000), Var(v=-0.0193, grad=0.0000), Var(v=-0.1481, grad=0.0000), Var(v=0.0742, grad=0.9786), Var(v=0.0657, grad=-1.5248), Var(v=0.0759, grad=0.0000), Var(v=0.0207, grad=1.2485), Var(v=0.1192, grad=0.0000), Var(v=0.1562, grad=0.4627), Var(v=-0.1376, grad=0.0000), Var(v=0.0600, grad=1.5094), Var(v=0.0278, grad=0.0000), Var(v=-0.0991, grad=0.0000), Var(v=0.0301, grad=-1.2998), Var(v=0.0184, grad=-1.1315), Var(v=0.0141, grad=-1.4757), Var(v=0.1179, grad=1.1587), Var(v=-0.0403, grad=0.3010), Var(v=-0.0749, grad=0.0055), Var(v=-0.1141, grad=0.0000), Var(v=-0.0835, grad=0.0000), Var(v=-0.0553, grad=-1.3474), Var(v=-0.0684, grad=0.0000), Var(v=-0.0093, grad=1.9975), Var(v=0.0037, grad=0.0000), Var(v=0.0303, grad=0.6659), Var(v=-0.0668, grad=0.0000), Var(v=0.0399, grad=-1.2617), Var(v=0.0410, grad=-1.3784), Var(v=-0.1277, grad=0.9501), Var(v=0.0588, grad=0.1939), Var(v=0.1713, grad=0.0000), Var(v=0.0452, grad=-0.2185), Var(v=-0.0204, grad=0.7055), Var(v=-0.0858, grad=0.0000), Var(v=0.1110, grad=-0.7002), Var(v=0.0075, grad=0.0000), Var(v=-0.1074, grad=0.0789), Var(v=-0.0650, grad=0.0000), Var(v=0.1222, grad=0.4310), Var(v=0.1840, grad=-1.4762), Var(v=-0.1761, grad=0.0000), Var(v=-0.0966, grad=0.0000), Var(v=0.0367, grad=0.0000), Var(v=-0.0589, grad=1.0012)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-3.7223), Var(v=0.0000, grad=10.7974), Var(v=0.0000, grad=-16.1228), Var(v=0.0000, grad=-0.0897), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=2.7296), Var(v=0.0000, grad=8.4095), Var(v=0.0000, grad=0.1935), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-0.1584), Var(v=0.0000, grad=11.5696), Var(v=0.0000, grad=-0.0587), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=12.9703), Var(v=0.0000, grad=9.0484), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.1649), Var(v=0.0000, grad=0.1436), Var(v=0.0000, grad=-12.6810), Var(v=0.0000, grad=9.9568), Var(v=0.0000, grad=2.5867), Var(v=0.0000, grad=0.0471), Var(v=0.0000, grad=-3.2949), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.1709), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-0.2534), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=5.7225), Var(v=0.0000, grad=-0.4128), Var(v=0.0000, grad=-10.8418), Var(v=0.0000, grad=0.1749), Var(v=0.0000, grad=-0.1205), Var(v=0.0000, grad=-0.0246), Var(v=0.0000, grad=-4.4129), Var(v=0.0000, grad=-1.8777), Var(v=0.0000, grad=-0.0895), Var(v=0.0000, grad=-7.0877), Var(v=0.0000, grad=0.0888), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.6783), Var(v=0.0000, grad=-6.3665), Var(v=0.0000, grad=-0.0547), Var(v=0.0000, grad=0.1873), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-3.7430), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-0.1270)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0648, grad=0.0000)], [Var(v=0.0433, grad=-1.3909)], [Var(v=0.1237, grad=3.7911)], [Var(v=0.1874, grad=-1.7564)], [Var(v=-0.0706, grad=0.9242)], [Var(v=0.0653, grad=0.0000)], [Var(v=-0.1895, grad=0.0000)], [Var(v=0.0313, grad=0.7029)], [Var(v=-0.0977, grad=-1.2982)], [Var(v=0.1523, grad=-1.9289)], [Var(v=0.0050, grad=0.0000)], [Var(v=-0.1247, grad=1.3288)], [Var(v=0.1325, grad=0.3109)], [Var(v=-0.0462, grad=-0.8252)], [Var(v=0.0319, grad=0.0000)], [Var(v=-0.1507, grad=-3.6724)], [Var(v=0.1036, grad=2.1260)], [Var(v=0.0329, grad=0.0000)], [Var(v=0.1298, grad=0.9145)], [Var(v=0.1130, grad=0.0620)], [Var(v=0.1474, grad=-3.2583)], [Var(v=-0.1157, grad=-0.8693)], [Var(v=-0.0301, grad=-0.0663)], [Var(v=-0.0005, grad=-0.0180)], [Var(v=-0.0377, grad=0.1706)], [Var(v=-0.1125, grad=0.0000)], [Var(v=0.1346, grad=-2.1416)], [Var(v=-0.0008, grad=0.0000)], [Var(v=-0.1995, grad=5.0009)], [Var(v=-0.0794, grad=0.0000)], [Var(v=-0.0665, grad=-1.7280)], [Var(v=-0.0047, grad=3.5219)], [Var(v=0.1260, grad=-1.2731)], [Var(v=0.1377, grad=3.6065)], [Var(v=-0.0949, grad=0.6228)], [Var(v=-0.0194, grad=1.8196)], [Var(v=-0.0505, grad=1.0457)], [Var(v=0.0218, grad=-4.5225)], [Var(v=-0.0705, grad=-0.4017)], [Var(v=-0.0812, grad=0.5235)], [Var(v=0.0699, grad=2.2374)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0079, grad=-2.4445)], [Var(v=-0.0729, grad=0.8112)], [Var(v=-0.0430, grad=-2.2608)], [Var(v=0.1474, grad=-3.0325)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0429, grad=1.9672)], [Var(v=-0.0512, grad=0.0000)], [Var(v=-0.1000, grad=1.4347)]] Biases: [Var(v=0.0000, grad=1.2704)]\n",
            "\n",
            "Network after update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.2480, grad=-0.9637), Var(v=-0.0255, grad=4.3745), Var(v=0.2059, grad=-6.3533), Var(v=-0.0787, grad=-3.9889), Var(v=-0.0494, grad=3.0956), Var(v=-0.0918, grad=2.0043), Var(v=-0.1172, grad=2.7483), Var(v=0.1195, grad=-8.3909), Var(v=0.0427, grad=-4.6719), Var(v=0.1346, grad=-0.0225), Var(v=-0.0566, grad=-1.7055), Var(v=-0.0710, grad=3.3485), Var(v=-0.0994, grad=5.5311), Var(v=0.0752, grad=-4.3999), Var(v=0.1059, grad=-0.5867)]] Biases: [Var(v=-0.0088, grad=0.8760), Var(v=-0.0376, grad=3.7587), Var(v=0.0546, grad=-5.4590), Var(v=-0.0363, grad=3.6257), Var(v=0.0281, grad=-2.8137), Var(v=0.0182, grad=-1.8218), Var(v=0.0250, grad=-2.4981), Var(v=0.0721, grad=-7.2098), Var(v=-0.0425, grad=4.2466), Var(v=0.0002, grad=-0.0193), Var(v=-0.0155, grad=1.5503), Var(v=0.0304, grad=-3.0437), Var(v=0.0503, grad=-5.0276), Var(v=0.0378, grad=-3.7805), Var(v=0.0050, grad=-0.5041)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0041, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=0.1730, grad=3.0602), Var(v=0.0188, grad=0.0000), Var(v=-0.0063, grad=-1.7465), Var(v=-0.1363, grad=0.0000), Var(v=-0.0500, grad=0.0000), Var(v=0.0756, grad=0.7736), Var(v=-0.1443, grad=0.0000), Var(v=-0.0740, grad=3.7686), Var(v=-0.0770, grad=0.0000), Var(v=-0.0281, grad=-3.0856), Var(v=-0.1060, grad=3.2791), Var(v=0.0593, grad=-1.1436), Var(v=-0.2103, grad=0.0000), Var(v=-0.0262, grad=0.0000), Var(v=0.0481, grad=2.5645), Var(v=-0.1888, grad=0.0000), Var(v=-0.1032, grad=3.2124), Var(v=-0.0288, grad=2.7965), Var(v=-0.0205, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.1150, grad=0.0000), Var(v=0.0467, grad=-0.9338), Var(v=0.0162, grad=0.0000), Var(v=-0.0068, grad=3.3299), Var(v=-0.0723, grad=0.0000), Var(v=0.2395, grad=-4.9368), Var(v=0.0204, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=0.1331, grad=-0.1170), Var(v=0.0380, grad=0.0000), Var(v=0.1585, grad=3.4066), Var(v=-0.0152, grad=-2.3481), Var(v=0.0263, grad=-0.4792), Var(v=0.0840, grad=-1.2507), Var(v=-0.0084, grad=0.0000), Var(v=0.1052, grad=-1.7437), Var(v=0.0339, grad=-2.0088), Var(v=0.1599, grad=1.7304), Var(v=0.0021, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0279, grad=-1.8044), Var(v=0.0391, grad=-1.0651), Var(v=-0.0130, grad=3.6483), Var(v=-0.1414, grad=0.0000), Var(v=0.1497, grad=-1.0609), Var(v=-0.2004, grad=0.0000), Var(v=0.0053, grad=-2.4745)], [Var(v=-0.0285, grad=0.0000), Var(v=0.0007, grad=-0.0790), Var(v=-0.0692, grad=0.0000), Var(v=0.0237, grad=-0.3424), Var(v=0.0945, grad=0.1289), Var(v=0.0375, grad=0.0000), Var(v=0.0597, grad=0.0000), Var(v=0.0423, grad=0.0000), Var(v=-0.0031, grad=0.1786), Var(v=-0.1476, grad=-0.2782), Var(v=-0.0088, grad=0.0000), Var(v=-0.0281, grad=0.2278), Var(v=-0.1054, grad=0.0000), Var(v=-0.0036, grad=0.0844), Var(v=0.0266, grad=0.0000), Var(v=-0.0808, grad=0.2754), Var(v=0.1450, grad=0.0000), Var(v=-0.3748, grad=0.0000), Var(v=0.1193, grad=-0.2372), Var(v=-0.0874, grad=-0.2065), Var(v=-0.0422, grad=-0.2693), Var(v=0.1486, grad=0.2114), Var(v=0.1077, grad=0.0549), Var(v=-0.0492, grad=0.0010), Var(v=-0.0905, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1434, grad=-0.2459), Var(v=-0.0489, grad=0.0000), Var(v=0.1006, grad=0.3645), Var(v=0.0281, grad=0.0000), Var(v=-0.0561, grad=0.1215), Var(v=-0.0283, grad=0.0000), Var(v=0.0258, grad=-0.2302), Var(v=-0.0798, grad=-0.2515), Var(v=0.0502, grad=0.1734), Var(v=-0.0451, grad=0.0354), Var(v=0.0527, grad=0.0000), Var(v=0.0568, grad=-0.0399), Var(v=0.1224, grad=0.1287), Var(v=-0.1543, grad=0.0000), Var(v=0.0667, grad=-0.1278), Var(v=0.3049, grad=0.0000), Var(v=0.1661, grad=0.0144), Var(v=-0.1218, grad=0.0000), Var(v=-0.0143, grad=0.0786), Var(v=0.0847, grad=-0.2694), Var(v=-0.1331, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=0.0691, grad=0.0000), Var(v=0.0851, grad=0.1827)], [Var(v=-0.0246, grad=0.0000), Var(v=0.0648, grad=-0.6166), Var(v=-0.0880, grad=0.0000), Var(v=0.2003, grad=-2.6709), Var(v=0.1077, grad=1.0059), Var(v=-0.1184, grad=0.0000), Var(v=-0.0834, grad=0.0000), Var(v=-0.0114, grad=0.0000), Var(v=-0.1067, grad=1.3931), Var(v=0.1045, grad=-2.1706), Var(v=-0.1222, grad=0.0000), Var(v=-0.0102, grad=1.7773), Var(v=-0.0973, grad=0.0000), Var(v=0.0425, grad=0.6587), Var(v=-0.1636, grad=0.0000), Var(v=-0.0114, grad=2.1486), Var(v=-0.1772, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.0422, grad=-1.8503), Var(v=0.1638, grad=-1.6107), Var(v=0.1561, grad=-2.1007), Var(v=0.0249, grad=1.6494), Var(v=0.1108, grad=0.4285), Var(v=0.1339, grad=0.0078), Var(v=0.0826, grad=0.0000), Var(v=-0.2547, grad=0.0000), Var(v=0.1148, grad=-1.9180), Var(v=-0.0667, grad=0.0000), Var(v=0.0085, grad=2.8435), Var(v=-0.2351, grad=0.0000), Var(v=-0.0243, grad=0.9480), Var(v=-0.0427, grad=0.0000), Var(v=0.0479, grad=-1.7960), Var(v=0.0405, grad=-1.9622), Var(v=0.0736, grad=1.3525), Var(v=0.0426, grad=0.2760), Var(v=0.0122, grad=0.0000), Var(v=0.1889, grad=-0.3111), Var(v=0.0992, grad=1.0044), Var(v=0.0919, grad=0.0000), Var(v=0.0778, grad=-0.9967), Var(v=-0.0673, grad=0.0000), Var(v=0.0671, grad=0.1124), Var(v=-0.1465, grad=0.0000), Var(v=0.0962, grad=0.6135), Var(v=0.0301, grad=-2.1014), Var(v=0.1312, grad=0.0000), Var(v=0.0307, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=0.1165, grad=1.4253)], [Var(v=-0.0325, grad=0.0000), Var(v=-0.1205, grad=0.0000), Var(v=0.0092, grad=1.4093), Var(v=-0.2196, grad=0.0000), Var(v=0.0992, grad=-0.8043), Var(v=0.1337, grad=0.0000), Var(v=-0.1130, grad=0.0000), Var(v=0.0037, grad=0.3563), Var(v=0.0406, grad=0.0000), Var(v=0.1631, grad=1.7355), Var(v=-0.0723, grad=0.0000), Var(v=0.0148, grad=-1.4210), Var(v=0.0836, grad=1.5101), Var(v=0.0655, grad=-0.5266), Var(v=0.0337, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=-0.1026, grad=1.1810), Var(v=-0.0521, grad=0.0000), Var(v=0.0127, grad=1.4793), Var(v=0.1231, grad=1.2878), Var(v=0.0269, grad=0.0000), Var(v=0.1218, grad=0.0000), Var(v=0.0245, grad=0.0000), Var(v=-0.1514, grad=0.0000), Var(v=-0.0071, grad=-0.4300), Var(v=-0.0355, grad=0.0000), Var(v=0.1458, grad=1.5335), Var(v=-0.1638, grad=0.0000), Var(v=0.0791, grad=-2.2734), Var(v=-0.1226, grad=0.0000), Var(v=-0.1305, grad=0.0000), Var(v=-0.0635, grad=-0.0539), Var(v=-0.0408, grad=0.0000), Var(v=0.0565, grad=1.5688), Var(v=0.0716, grad=-1.0813), Var(v=0.0949, grad=-0.2207), Var(v=-0.0496, grad=-0.5760), Var(v=0.0510, grad=0.0000), Var(v=0.0733, grad=-0.8030), Var(v=-0.0435, grad=-0.9251), Var(v=0.0360, grad=0.7969), Var(v=-0.0110, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=0.0218, grad=-0.8310), Var(v=0.1047, grad=-0.4905), Var(v=-0.0313, grad=1.6801), Var(v=0.0316, grad=0.0000), Var(v=-0.2697, grad=-0.4885), Var(v=-0.0208, grad=0.0000), Var(v=0.2696, grad=-1.1395)], [Var(v=-0.0485, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1648, grad=0.2189), Var(v=-0.0520, grad=0.0000), Var(v=-0.0342, grad=-0.1249), Var(v=0.0005, grad=0.0000), Var(v=0.0276, grad=0.0000), Var(v=0.0093, grad=0.0553), Var(v=0.1118, grad=0.0000), Var(v=-0.1087, grad=0.2695), Var(v=0.0315, grad=0.0000), Var(v=0.0576, grad=-0.2207), Var(v=-0.0163, grad=0.2345), Var(v=0.0557, grad=-0.0818), Var(v=-0.0301, grad=0.0000), Var(v=0.2480, grad=0.0000), Var(v=-0.0625, grad=0.1834), Var(v=0.0160, grad=0.0000), Var(v=0.1137, grad=0.2298), Var(v=-0.0098, grad=0.2000), Var(v=-0.1986, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=0.0513, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=0.0510, grad=-0.0668), Var(v=0.0447, grad=0.0000), Var(v=-0.1002, grad=0.2382), Var(v=-0.0628, grad=0.0000), Var(v=-0.1623, grad=-0.3531), Var(v=-0.1566, grad=0.0000), Var(v=0.0219, grad=0.0000), Var(v=-0.0343, grad=-0.0084), Var(v=0.1226, grad=0.0000), Var(v=0.0558, grad=0.2437), Var(v=0.0904, grad=-0.1679), Var(v=-0.1081, grad=-0.0343), Var(v=0.0150, grad=-0.0895), Var(v=-0.1120, grad=0.0000), Var(v=0.1322, grad=-0.1247), Var(v=-0.0094, grad=-0.1437), Var(v=-0.0891, grad=0.1238), Var(v=0.2348, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=0.1776, grad=-0.1291), Var(v=0.0985, grad=-0.0762), Var(v=0.1144, grad=0.2609), Var(v=0.0624, grad=0.0000), Var(v=0.1976, grad=-0.0759), Var(v=-0.0767, grad=0.0000), Var(v=-0.0802, grad=-0.1770)], [Var(v=-0.1277, grad=0.0000), Var(v=0.1632, grad=0.0000), Var(v=-0.0286, grad=0.8527), Var(v=-0.0175, grad=0.0000), Var(v=0.1751, grad=-0.4866), Var(v=0.0381, grad=0.0000), Var(v=-0.1618, grad=0.0000), Var(v=-0.0895, grad=0.2156), Var(v=-0.0123, grad=0.0000), Var(v=-0.0304, grad=1.0500), Var(v=-0.0821, grad=0.0000), Var(v=0.1587, grad=-0.8597), Var(v=0.0212, grad=0.9136), Var(v=-0.1188, grad=-0.3186), Var(v=0.0256, grad=0.0000), Var(v=0.0336, grad=0.0000), Var(v=0.0790, grad=0.7145), Var(v=0.1002, grad=0.0000), Var(v=0.1384, grad=0.8950), Var(v=-0.0718, grad=0.7792), Var(v=-0.0187, grad=0.0000), Var(v=0.0742, grad=0.0000), Var(v=-0.1355, grad=0.0000), Var(v=-0.1713, grad=0.0000), Var(v=0.0660, grad=-0.2602), Var(v=-0.0613, grad=0.0000), Var(v=-0.1003, grad=0.9278), Var(v=0.0369, grad=0.0000), Var(v=0.0707, grad=-1.3755), Var(v=0.0260, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0251, grad=-0.0326), Var(v=-0.1436, grad=0.0000), Var(v=-0.0242, grad=0.9492), Var(v=-0.0160, grad=-0.6542), Var(v=0.0517, grad=-0.1335), Var(v=-0.1681, grad=-0.3485), Var(v=-0.0673, grad=0.0000), Var(v=0.0419, grad=-0.4858), Var(v=-0.0412, grad=-0.5597), Var(v=0.0023, grad=0.4821), Var(v=-0.0273, grad=0.0000), Var(v=-0.1181, grad=0.0000), Var(v=0.0604, grad=-0.5028), Var(v=-0.1287, grad=-0.2968), Var(v=-0.0512, grad=1.0165), Var(v=0.0593, grad=0.0000), Var(v=0.0703, grad=-0.2956), Var(v=-0.2140, grad=0.0000), Var(v=-0.1063, grad=-0.6895)], [Var(v=0.0382, grad=0.0000), Var(v=-0.0167, grad=0.0000), Var(v=0.0041, grad=1.0660), Var(v=-0.1109, grad=0.0000), Var(v=0.1019, grad=-0.6084), Var(v=-0.1744, grad=0.0000), Var(v=-0.0791, grad=0.0000), Var(v=-0.0594, grad=0.2695), Var(v=-0.1071, grad=0.0000), Var(v=0.0087, grad=1.3128), Var(v=-0.0864, grad=0.0000), Var(v=0.1144, grad=-1.0749), Var(v=-0.0186, grad=1.1422), Var(v=0.0653, grad=-0.3984), Var(v=0.0069, grad=0.0000), Var(v=-0.2317, grad=0.0000), Var(v=0.1314, grad=0.8933), Var(v=-0.0045, grad=0.0000), Var(v=0.1190, grad=1.1190), Var(v=0.0052, grad=0.9741), Var(v=-0.0314, grad=0.0000), Var(v=0.1707, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=0.1495, grad=0.0000), Var(v=-0.0420, grad=-0.3253), Var(v=-0.1896, grad=0.0000), Var(v=-0.1040, grad=1.1600), Var(v=-0.0995, grad=0.0000), Var(v=0.0345, grad=-1.7197), Var(v=-0.0267, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0167, grad=-0.0408), Var(v=-0.0091, grad=0.0000), Var(v=-0.0549, grad=1.1867), Var(v=0.0870, grad=-0.8179), Var(v=0.2254, grad=-0.1669), Var(v=-0.0257, grad=-0.4357), Var(v=-0.1826, grad=0.0000), Var(v=-0.1079, grad=-0.6074), Var(v=0.1160, grad=-0.6998), Var(v=0.0380, grad=0.6028), Var(v=-0.1312, grad=0.0000), Var(v=-0.0721, grad=0.0000), Var(v=-0.0139, grad=-0.6286), Var(v=-0.0177, grad=-0.3710), Var(v=-0.0133, grad=1.2709), Var(v=-0.0772, grad=0.0000), Var(v=0.1040, grad=-0.3695), Var(v=-0.1435, grad=0.0000), Var(v=0.1247, grad=-0.8620)], [Var(v=-0.0678, grad=0.0000), Var(v=-0.2273, grad=-0.1544), Var(v=-0.0194, grad=0.0000), Var(v=0.1539, grad=-0.6688), Var(v=-0.0690, grad=0.2519), Var(v=0.0034, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=0.1695, grad=0.0000), Var(v=-0.1636, grad=0.3488), Var(v=-0.0296, grad=-0.5435), Var(v=-0.0574, grad=0.0000), Var(v=0.1581, grad=0.4450), Var(v=-0.0479, grad=0.0000), Var(v=0.1570, grad=0.1649), Var(v=-0.0512, grad=0.0000), Var(v=-0.0699, grad=0.5380), Var(v=-0.1293, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=0.1919, grad=-0.4633), Var(v=0.1799, grad=-0.4033), Var(v=0.0711, grad=-0.5260), Var(v=0.0760, grad=0.4130), Var(v=0.1109, grad=0.1073), Var(v=0.0066, grad=0.0020), Var(v=-0.0262, grad=0.0000), Var(v=0.0242, grad=0.0000), Var(v=-0.0576, grad=-0.4803), Var(v=0.0166, grad=0.0000), Var(v=0.0288, grad=0.7120), Var(v=-0.0433, grad=0.0000), Var(v=-0.0837, grad=0.2374), Var(v=-0.1046, grad=0.0000), Var(v=-0.0806, grad=-0.4497), Var(v=-0.0476, grad=-0.4913), Var(v=-0.1311, grad=0.3386), Var(v=0.0655, grad=0.0691), Var(v=-0.0703, grad=0.0000), Var(v=0.0929, grad=-0.0779), Var(v=-0.0502, grad=0.2515), Var(v=0.1512, grad=0.0000), Var(v=0.0366, grad=-0.2496), Var(v=-0.2140, grad=0.0000), Var(v=-0.0674, grad=0.0281), Var(v=0.0649, grad=0.0000), Var(v=-0.0728, grad=0.1536), Var(v=0.1588, grad=-0.5262), Var(v=-0.0122, grad=0.0000), Var(v=-0.1954, grad=0.0000), Var(v=0.0343, grad=0.0000), Var(v=-0.1136, grad=0.3569)], [Var(v=-0.0813, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=-0.0207, grad=0.0482), Var(v=0.0004, grad=0.0000), Var(v=0.0025, grad=-0.0275), Var(v=-0.1260, grad=0.0000), Var(v=0.1035, grad=0.0000), Var(v=0.1513, grad=0.0122), Var(v=-0.0923, grad=0.0000), Var(v=0.0120, grad=0.0594), Var(v=0.0353, grad=0.0000), Var(v=-0.0126, grad=-0.0486), Var(v=0.0307, grad=0.0517), Var(v=-0.1118, grad=-0.0180), Var(v=-0.0792, grad=0.0000), Var(v=-0.1545, grad=0.0000), Var(v=-0.0172, grad=0.0404), Var(v=0.0030, grad=0.0000), Var(v=0.0472, grad=0.0506), Var(v=-0.0630, grad=0.0441), Var(v=0.0139, grad=0.0000), Var(v=0.0116, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0989, grad=0.0000), Var(v=-0.1968, grad=-0.0147), Var(v=0.0626, grad=0.0000), Var(v=0.0067, grad=0.0525), Var(v=0.1502, grad=0.0000), Var(v=-0.0322, grad=-0.0778), Var(v=0.0590, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=0.0883, grad=-0.0018), Var(v=0.0493, grad=0.0000), Var(v=-0.0683, grad=0.0537), Var(v=-0.0451, grad=-0.0370), Var(v=0.1196, grad=-0.0076), Var(v=0.0743, grad=-0.0197), Var(v=0.1545, grad=0.0000), Var(v=0.1353, grad=-0.0275), Var(v=-0.0582, grad=-0.0317), Var(v=-0.0191, grad=0.0273), Var(v=0.0911, grad=0.0000), Var(v=-0.0256, grad=0.0000), Var(v=0.1969, grad=-0.0284), Var(v=-0.0558, grad=-0.0168), Var(v=0.2821, grad=0.0575), Var(v=0.0665, grad=0.0000), Var(v=0.0041, grad=-0.0167), Var(v=0.0646, grad=0.0000), Var(v=-0.0926, grad=-0.0390)], [Var(v=-0.1410, grad=0.0000), Var(v=0.1451, grad=-0.5820), Var(v=-0.0403, grad=0.0000), Var(v=-0.0269, grad=-2.5207), Var(v=-0.0711, grad=0.9494), Var(v=0.1510, grad=0.0000), Var(v=0.0460, grad=0.0000), Var(v=-0.1218, grad=0.0000), Var(v=0.1447, grad=1.3148), Var(v=0.1088, grad=-2.0486), Var(v=-0.0891, grad=0.0000), Var(v=0.0113, grad=1.6774), Var(v=-0.1346, grad=0.0000), Var(v=-0.1247, grad=0.6217), Var(v=0.0354, grad=0.0000), Var(v=0.2028, grad=2.0279), Var(v=0.1338, grad=0.0000), Var(v=0.0658, grad=0.0000), Var(v=-0.0012, grad=-1.7463), Var(v=-0.1464, grad=-1.5202), Var(v=0.0686, grad=-1.9826), Var(v=-0.1187, grad=1.5567), Var(v=-0.1044, grad=0.4044), Var(v=-0.0983, grad=0.0074), Var(v=-0.0980, grad=0.0000), Var(v=-0.0238, grad=0.0000), Var(v=0.1027, grad=-1.8102), Var(v=-0.0310, grad=0.0000), Var(v=-0.0279, grad=2.6837), Var(v=0.1276, grad=0.0000), Var(v=0.1296, grad=0.8947), Var(v=0.0793, grad=0.0000), Var(v=0.0846, grad=-1.6951), Var(v=0.1612, grad=-1.8519), Var(v=0.0649, grad=1.2764), Var(v=0.1109, grad=0.2605), Var(v=-0.1131, grad=0.0000), Var(v=0.0759, grad=-0.2936), Var(v=0.0653, grad=0.9479), Var(v=-0.1344, grad=0.0000), Var(v=-0.0540, grad=-0.9407), Var(v=-0.1124, grad=0.0000), Var(v=0.2098, grad=0.1061), Var(v=0.0492, grad=0.0000), Var(v=0.0324, grad=0.5790), Var(v=0.1406, grad=-1.9833), Var(v=-0.0450, grad=0.0000), Var(v=0.0310, grad=0.0000), Var(v=-0.0809, grad=0.0000), Var(v=-0.0570, grad=1.3452)], [Var(v=0.0180, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.0142, grad=0.8751), Var(v=-0.1866, grad=0.0000), Var(v=-0.1463, grad=-0.4994), Var(v=0.1177, grad=0.0000), Var(v=-0.0163, grad=0.0000), Var(v=-0.0525, grad=0.2212), Var(v=-0.1684, grad=0.0000), Var(v=-0.2068, grad=1.0777), Var(v=-0.2002, grad=0.0000), Var(v=0.1659, grad=-0.8824), Var(v=0.1041, grad=0.9377), Var(v=-0.0262, grad=-0.3270), Var(v=0.0284, grad=0.0000), Var(v=0.0294, grad=0.0000), Var(v=0.0316, grad=0.7334), Var(v=0.0557, grad=0.0000), Var(v=0.1710, grad=0.9186), Var(v=-0.0567, grad=0.7997), Var(v=0.0871, grad=0.0000), Var(v=-0.2544, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=0.0140, grad=0.0000), Var(v=-0.0770, grad=-0.2670), Var(v=0.1245, grad=0.0000), Var(v=0.0788, grad=0.9522), Var(v=-0.1244, grad=0.0000), Var(v=0.0342, grad=-1.4117), Var(v=0.0668, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=0.2533, grad=-0.0335), Var(v=-0.0452, grad=0.0000), Var(v=-0.0561, grad=0.9742), Var(v=0.0115, grad=-0.6715), Var(v=0.0196, grad=-0.1370), Var(v=0.0289, grad=-0.3577), Var(v=-0.1136, grad=0.0000), Var(v=-0.0044, grad=-0.4986), Var(v=-0.0389, grad=-0.5744), Var(v=-0.0610, grad=0.4948), Var(v=0.0954, grad=0.0000), Var(v=0.1029, grad=0.0000), Var(v=-0.0040, grad=-0.5160), Var(v=-0.0155, grad=-0.3046), Var(v=0.0582, grad=1.0433), Var(v=0.0447, grad=0.0000), Var(v=-0.0630, grad=-0.3034), Var(v=-0.0424, grad=0.0000), Var(v=-0.0373, grad=-0.7076)], [Var(v=0.1217, grad=0.0000), Var(v=-0.0701, grad=0.0000), Var(v=-0.1175, grad=0.4456), Var(v=-0.1082, grad=0.0000), Var(v=0.1090, grad=-0.2543), Var(v=-0.0275, grad=0.0000), Var(v=-0.0139, grad=0.0000), Var(v=0.0476, grad=0.1127), Var(v=0.0807, grad=0.0000), Var(v=0.0200, grad=0.5488), Var(v=-0.1079, grad=0.0000), Var(v=0.1359, grad=-0.4493), Var(v=-0.0070, grad=0.4775), Var(v=-0.1922, grad=-0.1665), Var(v=-0.0427, grad=0.0000), Var(v=-0.0259, grad=0.0000), Var(v=-0.1250, grad=0.3735), Var(v=-0.0778, grad=0.0000), Var(v=-0.0368, grad=0.4678), Var(v=0.1022, grad=0.4072), Var(v=-0.0315, grad=0.0000), Var(v=-0.0715, grad=0.0000), Var(v=-0.0731, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=-0.0336, grad=-0.1360), Var(v=-0.0685, grad=0.0000), Var(v=-0.3071, grad=0.4849), Var(v=-0.1867, grad=0.0000), Var(v=-0.0450, grad=-0.7189), Var(v=0.0433, grad=0.0000), Var(v=0.2106, grad=0.0000), Var(v=-0.1368, grad=-0.0170), Var(v=-0.0985, grad=0.0000), Var(v=0.1745, grad=0.4961), Var(v=0.0937, grad=-0.3419), Var(v=-0.0443, grad=-0.0698), Var(v=0.1663, grad=-0.1821), Var(v=-0.0453, grad=0.0000), Var(v=-0.0356, grad=-0.2539), Var(v=0.0177, grad=-0.2925), Var(v=-0.2628, grad=0.2520), Var(v=-0.0903, grad=0.0000), Var(v=0.1531, grad=0.0000), Var(v=-0.1200, grad=-0.2628), Var(v=-0.1573, grad=-0.1551), Var(v=0.0789, grad=0.5313), Var(v=0.0694, grad=0.0000), Var(v=0.0603, grad=-0.1545), Var(v=-0.0888, grad=0.0000), Var(v=-0.0396, grad=-0.3603)], [Var(v=-0.1800, grad=0.0000), Var(v=-0.0102, grad=0.0000), Var(v=-0.1847, grad=0.5242), Var(v=-0.0277, grad=0.0000), Var(v=0.1621, grad=-0.2992), Var(v=0.1367, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.0598, grad=0.1325), Var(v=-0.0222, grad=0.0000), Var(v=0.1828, grad=0.6455), Var(v=0.0440, grad=0.0000), Var(v=-0.0089, grad=-0.5286), Var(v=0.0101, grad=0.5617), Var(v=0.0382, grad=-0.1959), Var(v=-0.0652, grad=0.0000), Var(v=0.0770, grad=0.0000), Var(v=-0.0495, grad=0.4393), Var(v=0.1316, grad=0.0000), Var(v=-0.0650, grad=0.5503), Var(v=-0.1258, grad=0.4790), Var(v=0.0588, grad=0.0000), Var(v=-0.1811, grad=0.0000), Var(v=-0.0226, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.0034, grad=-0.1600), Var(v=-0.1500, grad=0.0000), Var(v=-0.0018, grad=0.5704), Var(v=0.0849, grad=0.0000), Var(v=0.0386, grad=-0.8456), Var(v=0.0388, grad=0.0000), Var(v=0.1768, grad=0.0000), Var(v=0.0094, grad=-0.0200), Var(v=0.0213, grad=0.0000), Var(v=0.0271, grad=0.5835), Var(v=0.1021, grad=-0.4022), Var(v=0.0661, grad=-0.0821), Var(v=0.1259, grad=-0.2142), Var(v=-0.0395, grad=0.0000), Var(v=-0.0570, grad=-0.2987), Var(v=0.1144, grad=-0.3441), Var(v=0.0241, grad=0.2964), Var(v=-0.0257, grad=0.0000), Var(v=0.0134, grad=0.0000), Var(v=0.0794, grad=-0.3091), Var(v=0.1360, grad=-0.1824), Var(v=-0.0127, grad=0.6249), Var(v=-0.0938, grad=0.0000), Var(v=0.0563, grad=-0.1817), Var(v=0.0534, grad=0.0000), Var(v=-0.0770, grad=-0.4239)], [Var(v=-0.0702, grad=0.0000), Var(v=-0.0100, grad=-0.1354), Var(v=0.0272, grad=0.0000), Var(v=0.1555, grad=-0.5864), Var(v=-0.0142, grad=0.2208), Var(v=0.0615, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=-0.0404, grad=0.0000), Var(v=0.1021, grad=0.3058), Var(v=-0.0477, grad=-0.4765), Var(v=0.0190, grad=0.0000), Var(v=-0.1511, grad=0.3902), Var(v=0.2092, grad=0.0000), Var(v=0.1423, grad=0.1446), Var(v=-0.0012, grad=0.0000), Var(v=0.0909, grad=0.4717), Var(v=0.1141, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.1112, grad=-0.4062), Var(v=0.0239, grad=-0.3536), Var(v=0.1269, grad=-0.4612), Var(v=-0.0274, grad=0.3621), Var(v=-0.1348, grad=0.0941), Var(v=0.0788, grad=0.0017), Var(v=-0.1120, grad=0.0000), Var(v=0.1700, grad=0.0000), Var(v=0.1992, grad=-0.4211), Var(v=-0.1152, grad=0.0000), Var(v=-0.0300, grad=0.6243), Var(v=0.0619, grad=0.0000), Var(v=0.0501, grad=0.2081), Var(v=0.1708, grad=0.0000), Var(v=-0.0610, grad=-0.3943), Var(v=-0.0748, grad=-0.4308), Var(v=-0.0338, grad=0.2969), Var(v=-0.2473, grad=0.0606), Var(v=-0.2401, grad=0.0000), Var(v=0.0034, grad=-0.0683), Var(v=-0.0014, grad=0.2205), Var(v=-0.1162, grad=0.0000), Var(v=0.0750, grad=-0.2188), Var(v=-0.1281, grad=0.0000), Var(v=-0.1133, grad=0.0247), Var(v=0.0595, grad=0.0000), Var(v=0.0334, grad=0.1347), Var(v=-0.0134, grad=-0.4613), Var(v=-0.0250, grad=0.0000), Var(v=-0.1540, grad=0.0000), Var(v=-0.0671, grad=0.0000), Var(v=-0.0676, grad=0.3129)], [Var(v=0.0142, grad=0.0000), Var(v=-0.0422, grad=-0.4332), Var(v=0.0621, grad=0.0000), Var(v=-0.0859, grad=-1.8762), Var(v=0.0631, grad=0.7067), Var(v=-0.2445, grad=0.0000), Var(v=-0.0193, grad=0.0000), Var(v=-0.1481, grad=0.0000), Var(v=0.0644, grad=0.9786), Var(v=0.0809, grad=-1.5248), Var(v=0.0759, grad=0.0000), Var(v=0.0082, grad=1.2485), Var(v=0.1192, grad=0.0000), Var(v=0.1516, grad=0.4627), Var(v=-0.1376, grad=0.0000), Var(v=0.0449, grad=1.5094), Var(v=0.0278, grad=0.0000), Var(v=-0.0991, grad=0.0000), Var(v=0.0431, grad=-1.2998), Var(v=0.0297, grad=-1.1315), Var(v=0.0288, grad=-1.4757), Var(v=0.1063, grad=1.1587), Var(v=-0.0433, grad=0.3010), Var(v=-0.0750, grad=0.0055), Var(v=-0.1141, grad=0.0000), Var(v=-0.0835, grad=0.0000), Var(v=-0.0418, grad=-1.3474), Var(v=-0.0684, grad=0.0000), Var(v=-0.0293, grad=1.9975), Var(v=0.0037, grad=0.0000), Var(v=0.0236, grad=0.6659), Var(v=-0.0668, grad=0.0000), Var(v=0.0525, grad=-1.2617), Var(v=0.0548, grad=-1.3784), Var(v=-0.1372, grad=0.9501), Var(v=0.0569, grad=0.1939), Var(v=0.1713, grad=0.0000), Var(v=0.0474, grad=-0.2185), Var(v=-0.0275, grad=0.7055), Var(v=-0.0858, grad=0.0000), Var(v=0.1180, grad=-0.7002), Var(v=0.0075, grad=0.0000), Var(v=-0.1082, grad=0.0789), Var(v=-0.0650, grad=0.0000), Var(v=0.1179, grad=0.4310), Var(v=0.1988, grad=-1.4762), Var(v=-0.1761, grad=0.0000), Var(v=-0.0966, grad=0.0000), Var(v=0.0367, grad=0.0000), Var(v=-0.0689, grad=1.0012)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0372, grad=-3.7223), Var(v=-0.1080, grad=10.7974), Var(v=0.1612, grad=-16.1228), Var(v=0.0009, grad=-0.0897), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0273, grad=2.7296), Var(v=-0.0841, grad=8.4095), Var(v=-0.0019, grad=0.1935), Var(v=0.0000, grad=0.0000), Var(v=0.0016, grad=-0.1584), Var(v=-0.1157, grad=11.5696), Var(v=0.0006, grad=-0.0587), Var(v=0.0000, grad=0.0000), Var(v=-0.1297, grad=12.9703), Var(v=-0.0905, grad=9.0484), Var(v=0.0000, grad=0.0000), Var(v=-0.0016, grad=0.1649), Var(v=-0.0014, grad=0.1436), Var(v=0.1268, grad=-12.6810), Var(v=-0.0996, grad=9.9568), Var(v=-0.0259, grad=2.5867), Var(v=-0.0005, grad=0.0471), Var(v=0.0329, grad=-3.2949), Var(v=0.0000, grad=0.0000), Var(v=-0.0017, grad=0.1709), Var(v=0.0000, grad=0.0000), Var(v=0.0025, grad=-0.2534), Var(v=0.0000, grad=0.0000), Var(v=-0.0572, grad=5.7225), Var(v=0.0041, grad=-0.4128), Var(v=0.1084, grad=-10.8418), Var(v=-0.0017, grad=0.1749), Var(v=0.0012, grad=-0.1205), Var(v=0.0002, grad=-0.0246), Var(v=0.0441, grad=-4.4129), Var(v=0.0188, grad=-1.8777), Var(v=0.0009, grad=-0.0895), Var(v=0.0709, grad=-7.0877), Var(v=-0.0009, grad=0.0888), Var(v=0.0000, grad=0.0000), Var(v=-0.0068, grad=0.6783), Var(v=0.0637, grad=-6.3665), Var(v=0.0005, grad=-0.0547), Var(v=-0.0019, grad=0.1873), Var(v=0.0000, grad=0.0000), Var(v=0.0374, grad=-3.7430), Var(v=0.0000, grad=0.0000), Var(v=0.0013, grad=-0.1270)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0648, grad=0.0000)], [Var(v=0.0572, grad=-1.3909)], [Var(v=0.0857, grad=3.7911)], [Var(v=0.2049, grad=-1.7564)], [Var(v=-0.0798, grad=0.9242)], [Var(v=0.0653, grad=0.0000)], [Var(v=-0.1895, grad=0.0000)], [Var(v=0.0242, grad=0.7029)], [Var(v=-0.0848, grad=-1.2982)], [Var(v=0.1716, grad=-1.9289)], [Var(v=0.0050, grad=0.0000)], [Var(v=-0.1380, grad=1.3288)], [Var(v=0.1294, grad=0.3109)], [Var(v=-0.0380, grad=-0.8252)], [Var(v=0.0319, grad=0.0000)], [Var(v=-0.1140, grad=-3.6724)], [Var(v=0.0824, grad=2.1260)], [Var(v=0.0329, grad=0.0000)], [Var(v=0.1207, grad=0.9145)], [Var(v=0.1124, grad=0.0620)], [Var(v=0.1800, grad=-3.2583)], [Var(v=-0.1070, grad=-0.8693)], [Var(v=-0.0294, grad=-0.0663)], [Var(v=-0.0004, grad=-0.0180)], [Var(v=-0.0394, grad=0.1706)], [Var(v=-0.1125, grad=0.0000)], [Var(v=0.1560, grad=-2.1416)], [Var(v=-0.0008, grad=0.0000)], [Var(v=-0.2495, grad=5.0009)], [Var(v=-0.0794, grad=0.0000)], [Var(v=-0.0492, grad=-1.7280)], [Var(v=-0.0399, grad=3.5219)], [Var(v=0.1387, grad=-1.2731)], [Var(v=0.1016, grad=3.6065)], [Var(v=-0.1011, grad=0.6228)], [Var(v=-0.0376, grad=1.8196)], [Var(v=-0.0610, grad=1.0457)], [Var(v=0.0670, grad=-4.5225)], [Var(v=-0.0664, grad=-0.4017)], [Var(v=-0.0864, grad=0.5235)], [Var(v=0.0475, grad=2.2374)], [Var(v=-0.0004, grad=0.0000)], [Var(v=0.0166, grad=-2.4445)], [Var(v=-0.0810, grad=0.8112)], [Var(v=-0.0204, grad=-2.2608)], [Var(v=0.1777, grad=-3.0325)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0625, grad=1.9672)], [Var(v=-0.0512, grad=0.0000)], [Var(v=-0.1143, grad=1.4347)]] Biases: [Var(v=-0.0127, grad=1.2704)]\n",
            "\n",
            "Network after zeroing gradients:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.2480, grad=0.0000), Var(v=-0.0255, grad=0.0000), Var(v=0.2059, grad=0.0000), Var(v=-0.0787, grad=0.0000), Var(v=-0.0494, grad=0.0000), Var(v=-0.0918, grad=0.0000), Var(v=-0.1172, grad=0.0000), Var(v=0.1195, grad=0.0000), Var(v=0.0427, grad=0.0000), Var(v=0.1346, grad=0.0000), Var(v=-0.0566, grad=0.0000), Var(v=-0.0710, grad=0.0000), Var(v=-0.0994, grad=0.0000), Var(v=0.0752, grad=0.0000), Var(v=0.1059, grad=0.0000)]] Biases: [Var(v=-0.0088, grad=0.0000), Var(v=-0.0376, grad=0.0000), Var(v=0.0546, grad=0.0000), Var(v=-0.0363, grad=0.0000), Var(v=0.0281, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=0.0250, grad=0.0000), Var(v=0.0721, grad=0.0000), Var(v=-0.0425, grad=0.0000), Var(v=0.0002, grad=0.0000), Var(v=-0.0155, grad=0.0000), Var(v=0.0304, grad=0.0000), Var(v=0.0503, grad=0.0000), Var(v=0.0378, grad=0.0000), Var(v=0.0050, grad=0.0000)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0041, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=0.1730, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=-0.0063, grad=0.0000), Var(v=-0.1363, grad=0.0000), Var(v=-0.0500, grad=0.0000), Var(v=0.0756, grad=0.0000), Var(v=-0.1443, grad=0.0000), Var(v=-0.0740, grad=0.0000), Var(v=-0.0770, grad=0.0000), Var(v=-0.0281, grad=0.0000), Var(v=-0.1060, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=-0.2103, grad=0.0000), Var(v=-0.0262, grad=0.0000), Var(v=0.0481, grad=0.0000), Var(v=-0.1888, grad=0.0000), Var(v=-0.1032, grad=0.0000), Var(v=-0.0288, grad=0.0000), Var(v=-0.0205, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.1150, grad=0.0000), Var(v=0.0467, grad=0.0000), Var(v=0.0162, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=-0.0723, grad=0.0000), Var(v=0.2395, grad=0.0000), Var(v=0.0204, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=0.1331, grad=0.0000), Var(v=0.0380, grad=0.0000), Var(v=0.1585, grad=0.0000), Var(v=-0.0152, grad=0.0000), Var(v=0.0263, grad=0.0000), Var(v=0.0840, grad=0.0000), Var(v=-0.0084, grad=0.0000), Var(v=0.1052, grad=0.0000), Var(v=0.0339, grad=0.0000), Var(v=0.1599, grad=0.0000), Var(v=0.0021, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0279, grad=0.0000), Var(v=0.0391, grad=0.0000), Var(v=-0.0130, grad=0.0000), Var(v=-0.1414, grad=0.0000), Var(v=0.1497, grad=0.0000), Var(v=-0.2004, grad=0.0000), Var(v=0.0053, grad=0.0000)], [Var(v=-0.0285, grad=0.0000), Var(v=0.0007, grad=0.0000), Var(v=-0.0692, grad=0.0000), Var(v=0.0237, grad=0.0000), Var(v=0.0945, grad=0.0000), Var(v=0.0375, grad=0.0000), Var(v=0.0597, grad=0.0000), Var(v=0.0423, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=-0.1476, grad=0.0000), Var(v=-0.0088, grad=0.0000), Var(v=-0.0281, grad=0.0000), Var(v=-0.1054, grad=0.0000), Var(v=-0.0036, grad=0.0000), Var(v=0.0266, grad=0.0000), Var(v=-0.0808, grad=0.0000), Var(v=0.1450, grad=0.0000), Var(v=-0.3748, grad=0.0000), Var(v=0.1193, grad=0.0000), Var(v=-0.0874, grad=0.0000), Var(v=-0.0422, grad=0.0000), Var(v=0.1486, grad=0.0000), Var(v=0.1077, grad=0.0000), Var(v=-0.0492, grad=0.0000), Var(v=-0.0905, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1434, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=0.1006, grad=0.0000), Var(v=0.0281, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=0.0258, grad=0.0000), Var(v=-0.0798, grad=0.0000), Var(v=0.0502, grad=0.0000), Var(v=-0.0451, grad=0.0000), Var(v=0.0527, grad=0.0000), Var(v=0.0568, grad=0.0000), Var(v=0.1224, grad=0.0000), Var(v=-0.1543, grad=0.0000), Var(v=0.0667, grad=0.0000), Var(v=0.3049, grad=0.0000), Var(v=0.1661, grad=0.0000), Var(v=-0.1218, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=0.0847, grad=0.0000), Var(v=-0.1331, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=0.0691, grad=0.0000), Var(v=0.0851, grad=0.0000)], [Var(v=-0.0246, grad=0.0000), Var(v=0.0648, grad=0.0000), Var(v=-0.0880, grad=0.0000), Var(v=0.2003, grad=0.0000), Var(v=0.1077, grad=0.0000), Var(v=-0.1184, grad=0.0000), Var(v=-0.0834, grad=0.0000), Var(v=-0.0114, grad=0.0000), Var(v=-0.1067, grad=0.0000), Var(v=0.1045, grad=0.0000), Var(v=-0.1222, grad=0.0000), Var(v=-0.0102, grad=0.0000), Var(v=-0.0973, grad=0.0000), Var(v=0.0425, grad=0.0000), Var(v=-0.1636, grad=0.0000), Var(v=-0.0114, grad=0.0000), Var(v=-0.1772, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.0422, grad=0.0000), Var(v=0.1638, grad=0.0000), Var(v=0.1561, grad=0.0000), Var(v=0.0249, grad=0.0000), Var(v=0.1108, grad=0.0000), Var(v=0.1339, grad=0.0000), Var(v=0.0826, grad=0.0000), Var(v=-0.2547, grad=0.0000), Var(v=0.1148, grad=0.0000), Var(v=-0.0667, grad=0.0000), Var(v=0.0085, grad=0.0000), Var(v=-0.2351, grad=0.0000), Var(v=-0.0243, grad=0.0000), Var(v=-0.0427, grad=0.0000), Var(v=0.0479, grad=0.0000), Var(v=0.0405, grad=0.0000), Var(v=0.0736, grad=0.0000), Var(v=0.0426, grad=0.0000), Var(v=0.0122, grad=0.0000), Var(v=0.1889, grad=0.0000), Var(v=0.0992, grad=0.0000), Var(v=0.0919, grad=0.0000), Var(v=0.0778, grad=0.0000), Var(v=-0.0673, grad=0.0000), Var(v=0.0671, grad=0.0000), Var(v=-0.1465, grad=0.0000), Var(v=0.0962, grad=0.0000), Var(v=0.0301, grad=0.0000), Var(v=0.1312, grad=0.0000), Var(v=0.0307, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=0.1165, grad=0.0000)], [Var(v=-0.0325, grad=0.0000), Var(v=-0.1205, grad=0.0000), Var(v=0.0092, grad=0.0000), Var(v=-0.2196, grad=0.0000), Var(v=0.0992, grad=0.0000), Var(v=0.1337, grad=0.0000), Var(v=-0.1130, grad=0.0000), Var(v=0.0037, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=0.1631, grad=0.0000), Var(v=-0.0723, grad=0.0000), Var(v=0.0148, grad=0.0000), Var(v=0.0836, grad=0.0000), Var(v=0.0655, grad=0.0000), Var(v=0.0337, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=-0.1026, grad=0.0000), Var(v=-0.0521, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1231, grad=0.0000), Var(v=0.0269, grad=0.0000), Var(v=0.1218, grad=0.0000), Var(v=0.0245, grad=0.0000), Var(v=-0.1514, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=-0.0355, grad=0.0000), Var(v=0.1458, grad=0.0000), Var(v=-0.1638, grad=0.0000), Var(v=0.0791, grad=0.0000), Var(v=-0.1226, grad=0.0000), Var(v=-0.1305, grad=0.0000), Var(v=-0.0635, grad=0.0000), Var(v=-0.0408, grad=0.0000), Var(v=0.0565, grad=0.0000), Var(v=0.0716, grad=0.0000), Var(v=0.0949, grad=0.0000), Var(v=-0.0496, grad=0.0000), Var(v=0.0510, grad=0.0000), Var(v=0.0733, grad=0.0000), Var(v=-0.0435, grad=0.0000), Var(v=0.0360, grad=0.0000), Var(v=-0.0110, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=0.0218, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=-0.0313, grad=0.0000), Var(v=0.0316, grad=0.0000), Var(v=-0.2697, grad=0.0000), Var(v=-0.0208, grad=0.0000), Var(v=0.2696, grad=0.0000)], [Var(v=-0.0485, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1648, grad=0.0000), Var(v=-0.0520, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0276, grad=0.0000), Var(v=0.0093, grad=0.0000), Var(v=0.1118, grad=0.0000), Var(v=-0.1087, grad=0.0000), Var(v=0.0315, grad=0.0000), Var(v=0.0576, grad=0.0000), Var(v=-0.0163, grad=0.0000), Var(v=0.0557, grad=0.0000), Var(v=-0.0301, grad=0.0000), Var(v=0.2480, grad=0.0000), Var(v=-0.0625, grad=0.0000), Var(v=0.0160, grad=0.0000), Var(v=0.1137, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=-0.1986, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=0.0513, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=0.0510, grad=0.0000), Var(v=0.0447, grad=0.0000), Var(v=-0.1002, grad=0.0000), Var(v=-0.0628, grad=0.0000), Var(v=-0.1623, grad=0.0000), Var(v=-0.1566, grad=0.0000), Var(v=0.0219, grad=0.0000), Var(v=-0.0343, grad=0.0000), Var(v=0.1226, grad=0.0000), Var(v=0.0558, grad=0.0000), Var(v=0.0904, grad=0.0000), Var(v=-0.1081, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=0.1322, grad=0.0000), Var(v=-0.0094, grad=0.0000), Var(v=-0.0891, grad=0.0000), Var(v=0.2348, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=0.1776, grad=0.0000), Var(v=0.0985, grad=0.0000), Var(v=0.1144, grad=0.0000), Var(v=0.0624, grad=0.0000), Var(v=0.1976, grad=0.0000), Var(v=-0.0767, grad=0.0000), Var(v=-0.0802, grad=0.0000)], [Var(v=-0.1277, grad=0.0000), Var(v=0.1632, grad=0.0000), Var(v=-0.0286, grad=0.0000), Var(v=-0.0175, grad=0.0000), Var(v=0.1751, grad=0.0000), Var(v=0.0381, grad=0.0000), Var(v=-0.1618, grad=0.0000), Var(v=-0.0895, grad=0.0000), Var(v=-0.0123, grad=0.0000), Var(v=-0.0304, grad=0.0000), Var(v=-0.0821, grad=0.0000), Var(v=0.1587, grad=0.0000), Var(v=0.0212, grad=0.0000), Var(v=-0.1188, grad=0.0000), Var(v=0.0256, grad=0.0000), Var(v=0.0336, grad=0.0000), Var(v=0.0790, grad=0.0000), Var(v=0.1002, grad=0.0000), Var(v=0.1384, grad=0.0000), Var(v=-0.0718, grad=0.0000), Var(v=-0.0187, grad=0.0000), Var(v=0.0742, grad=0.0000), Var(v=-0.1355, grad=0.0000), Var(v=-0.1713, grad=0.0000), Var(v=0.0660, grad=0.0000), Var(v=-0.0613, grad=0.0000), Var(v=-0.1003, grad=0.0000), Var(v=0.0369, grad=0.0000), Var(v=0.0707, grad=0.0000), Var(v=0.0260, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0251, grad=0.0000), Var(v=-0.1436, grad=0.0000), Var(v=-0.0242, grad=0.0000), Var(v=-0.0160, grad=0.0000), Var(v=0.0517, grad=0.0000), Var(v=-0.1681, grad=0.0000), Var(v=-0.0673, grad=0.0000), Var(v=0.0419, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=0.0023, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=-0.1181, grad=0.0000), Var(v=0.0604, grad=0.0000), Var(v=-0.1287, grad=0.0000), Var(v=-0.0512, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=0.0703, grad=0.0000), Var(v=-0.2140, grad=0.0000), Var(v=-0.1063, grad=0.0000)], [Var(v=0.0382, grad=0.0000), Var(v=-0.0167, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=-0.1109, grad=0.0000), Var(v=0.1019, grad=0.0000), Var(v=-0.1744, grad=0.0000), Var(v=-0.0791, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.1071, grad=0.0000), Var(v=0.0087, grad=0.0000), Var(v=-0.0864, grad=0.0000), Var(v=0.1144, grad=0.0000), Var(v=-0.0186, grad=0.0000), Var(v=0.0653, grad=0.0000), Var(v=0.0069, grad=0.0000), Var(v=-0.2317, grad=0.0000), Var(v=0.1314, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=0.1190, grad=0.0000), Var(v=0.0052, grad=0.0000), Var(v=-0.0314, grad=0.0000), Var(v=0.1707, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=0.1495, grad=0.0000), Var(v=-0.0420, grad=0.0000), Var(v=-0.1896, grad=0.0000), Var(v=-0.1040, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=0.0345, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0167, grad=0.0000), Var(v=-0.0091, grad=0.0000), Var(v=-0.0549, grad=0.0000), Var(v=0.0870, grad=0.0000), Var(v=0.2254, grad=0.0000), Var(v=-0.0257, grad=0.0000), Var(v=-0.1826, grad=0.0000), Var(v=-0.1079, grad=0.0000), Var(v=0.1160, grad=0.0000), Var(v=0.0380, grad=0.0000), Var(v=-0.1312, grad=0.0000), Var(v=-0.0721, grad=0.0000), Var(v=-0.0139, grad=0.0000), Var(v=-0.0177, grad=0.0000), Var(v=-0.0133, grad=0.0000), Var(v=-0.0772, grad=0.0000), Var(v=0.1040, grad=0.0000), Var(v=-0.1435, grad=0.0000), Var(v=0.1247, grad=0.0000)], [Var(v=-0.0678, grad=0.0000), Var(v=-0.2273, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.1539, grad=0.0000), Var(v=-0.0690, grad=0.0000), Var(v=0.0034, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=0.1695, grad=0.0000), Var(v=-0.1636, grad=0.0000), Var(v=-0.0296, grad=0.0000), Var(v=-0.0574, grad=0.0000), Var(v=0.1581, grad=0.0000), Var(v=-0.0479, grad=0.0000), Var(v=0.1570, grad=0.0000), Var(v=-0.0512, grad=0.0000), Var(v=-0.0699, grad=0.0000), Var(v=-0.1293, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=0.1919, grad=0.0000), Var(v=0.1799, grad=0.0000), Var(v=0.0711, grad=0.0000), Var(v=0.0760, grad=0.0000), Var(v=0.1109, grad=0.0000), Var(v=0.0066, grad=0.0000), Var(v=-0.0262, grad=0.0000), Var(v=0.0242, grad=0.0000), Var(v=-0.0576, grad=0.0000), Var(v=0.0166, grad=0.0000), Var(v=0.0288, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0837, grad=0.0000), Var(v=-0.1046, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.0476, grad=0.0000), Var(v=-0.1311, grad=0.0000), Var(v=0.0655, grad=0.0000), Var(v=-0.0703, grad=0.0000), Var(v=0.0929, grad=0.0000), Var(v=-0.0502, grad=0.0000), Var(v=0.1512, grad=0.0000), Var(v=0.0366, grad=0.0000), Var(v=-0.2140, grad=0.0000), Var(v=-0.0674, grad=0.0000), Var(v=0.0649, grad=0.0000), Var(v=-0.0728, grad=0.0000), Var(v=0.1588, grad=0.0000), Var(v=-0.0122, grad=0.0000), Var(v=-0.1954, grad=0.0000), Var(v=0.0343, grad=0.0000), Var(v=-0.1136, grad=0.0000)], [Var(v=-0.0813, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=-0.0207, grad=0.0000), Var(v=0.0004, grad=0.0000), Var(v=0.0025, grad=0.0000), Var(v=-0.1260, grad=0.0000), Var(v=0.1035, grad=0.0000), Var(v=0.1513, grad=0.0000), Var(v=-0.0923, grad=0.0000), Var(v=0.0120, grad=0.0000), Var(v=0.0353, grad=0.0000), Var(v=-0.0126, grad=0.0000), Var(v=0.0307, grad=0.0000), Var(v=-0.1118, grad=0.0000), Var(v=-0.0792, grad=0.0000), Var(v=-0.1545, grad=0.0000), Var(v=-0.0172, grad=0.0000), Var(v=0.0030, grad=0.0000), Var(v=0.0472, grad=0.0000), Var(v=-0.0630, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=0.0116, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0989, grad=0.0000), Var(v=-0.1968, grad=0.0000), Var(v=0.0626, grad=0.0000), Var(v=0.0067, grad=0.0000), Var(v=0.1502, grad=0.0000), Var(v=-0.0322, grad=0.0000), Var(v=0.0590, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=0.0883, grad=0.0000), Var(v=0.0493, grad=0.0000), Var(v=-0.0683, grad=0.0000), Var(v=-0.0451, grad=0.0000), Var(v=0.1196, grad=0.0000), Var(v=0.0743, grad=0.0000), Var(v=0.1545, grad=0.0000), Var(v=0.1353, grad=0.0000), Var(v=-0.0582, grad=0.0000), Var(v=-0.0191, grad=0.0000), Var(v=0.0911, grad=0.0000), Var(v=-0.0256, grad=0.0000), Var(v=0.1969, grad=0.0000), Var(v=-0.0558, grad=0.0000), Var(v=0.2821, grad=0.0000), Var(v=0.0665, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=0.0646, grad=0.0000), Var(v=-0.0926, grad=0.0000)], [Var(v=-0.1410, grad=0.0000), Var(v=0.1451, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=-0.0269, grad=0.0000), Var(v=-0.0711, grad=0.0000), Var(v=0.1510, grad=0.0000), Var(v=0.0460, grad=0.0000), Var(v=-0.1218, grad=0.0000), Var(v=0.1447, grad=0.0000), Var(v=0.1088, grad=0.0000), Var(v=-0.0891, grad=0.0000), Var(v=0.0113, grad=0.0000), Var(v=-0.1346, grad=0.0000), Var(v=-0.1247, grad=0.0000), Var(v=0.0354, grad=0.0000), Var(v=0.2028, grad=0.0000), Var(v=0.1338, grad=0.0000), Var(v=0.0658, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=-0.1464, grad=0.0000), Var(v=0.0686, grad=0.0000), Var(v=-0.1187, grad=0.0000), Var(v=-0.1044, grad=0.0000), Var(v=-0.0983, grad=0.0000), Var(v=-0.0980, grad=0.0000), Var(v=-0.0238, grad=0.0000), Var(v=0.1027, grad=0.0000), Var(v=-0.0310, grad=0.0000), Var(v=-0.0279, grad=0.0000), Var(v=0.1276, grad=0.0000), Var(v=0.1296, grad=0.0000), Var(v=0.0793, grad=0.0000), Var(v=0.0846, grad=0.0000), Var(v=0.1612, grad=0.0000), Var(v=0.0649, grad=0.0000), Var(v=0.1109, grad=0.0000), Var(v=-0.1131, grad=0.0000), Var(v=0.0759, grad=0.0000), Var(v=0.0653, grad=0.0000), Var(v=-0.1344, grad=0.0000), Var(v=-0.0540, grad=0.0000), Var(v=-0.1124, grad=0.0000), Var(v=0.2098, grad=0.0000), Var(v=0.0492, grad=0.0000), Var(v=0.0324, grad=0.0000), Var(v=0.1406, grad=0.0000), Var(v=-0.0450, grad=0.0000), Var(v=0.0310, grad=0.0000), Var(v=-0.0809, grad=0.0000), Var(v=-0.0570, grad=0.0000)], [Var(v=0.0180, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.0142, grad=0.0000), Var(v=-0.1866, grad=0.0000), Var(v=-0.1463, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=-0.0163, grad=0.0000), Var(v=-0.0525, grad=0.0000), Var(v=-0.1684, grad=0.0000), Var(v=-0.2068, grad=0.0000), Var(v=-0.2002, grad=0.0000), Var(v=0.1659, grad=0.0000), Var(v=0.1041, grad=0.0000), Var(v=-0.0262, grad=0.0000), Var(v=0.0284, grad=0.0000), Var(v=0.0294, grad=0.0000), Var(v=0.0316, grad=0.0000), Var(v=0.0557, grad=0.0000), Var(v=0.1710, grad=0.0000), Var(v=-0.0567, grad=0.0000), Var(v=0.0871, grad=0.0000), Var(v=-0.2544, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=0.0140, grad=0.0000), Var(v=-0.0770, grad=0.0000), Var(v=0.1245, grad=0.0000), Var(v=0.0788, grad=0.0000), Var(v=-0.1244, grad=0.0000), Var(v=0.0342, grad=0.0000), Var(v=0.0668, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=0.2533, grad=0.0000), Var(v=-0.0452, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.0115, grad=0.0000), Var(v=0.0196, grad=0.0000), Var(v=0.0289, grad=0.0000), Var(v=-0.1136, grad=0.0000), Var(v=-0.0044, grad=0.0000), Var(v=-0.0389, grad=0.0000), Var(v=-0.0610, grad=0.0000), Var(v=0.0954, grad=0.0000), Var(v=0.1029, grad=0.0000), Var(v=-0.0040, grad=0.0000), Var(v=-0.0155, grad=0.0000), Var(v=0.0582, grad=0.0000), Var(v=0.0447, grad=0.0000), Var(v=-0.0630, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0373, grad=0.0000)], [Var(v=0.1217, grad=0.0000), Var(v=-0.0701, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=0.1090, grad=0.0000), Var(v=-0.0275, grad=0.0000), Var(v=-0.0139, grad=0.0000), Var(v=0.0476, grad=0.0000), Var(v=0.0807, grad=0.0000), Var(v=0.0200, grad=0.0000), Var(v=-0.1079, grad=0.0000), Var(v=0.1359, grad=0.0000), Var(v=-0.0070, grad=0.0000), Var(v=-0.1922, grad=0.0000), Var(v=-0.0427, grad=0.0000), Var(v=-0.0259, grad=0.0000), Var(v=-0.1250, grad=0.0000), Var(v=-0.0778, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=0.1022, grad=0.0000), Var(v=-0.0315, grad=0.0000), Var(v=-0.0715, grad=0.0000), Var(v=-0.0731, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.0685, grad=0.0000), Var(v=-0.3071, grad=0.0000), Var(v=-0.1867, grad=0.0000), Var(v=-0.0450, grad=0.0000), Var(v=0.0433, grad=0.0000), Var(v=0.2106, grad=0.0000), Var(v=-0.1368, grad=0.0000), Var(v=-0.0985, grad=0.0000), Var(v=0.1745, grad=0.0000), Var(v=0.0937, grad=0.0000), Var(v=-0.0443, grad=0.0000), Var(v=0.1663, grad=0.0000), Var(v=-0.0453, grad=0.0000), Var(v=-0.0356, grad=0.0000), Var(v=0.0177, grad=0.0000), Var(v=-0.2628, grad=0.0000), Var(v=-0.0903, grad=0.0000), Var(v=0.1531, grad=0.0000), Var(v=-0.1200, grad=0.0000), Var(v=-0.1573, grad=0.0000), Var(v=0.0789, grad=0.0000), Var(v=0.0694, grad=0.0000), Var(v=0.0603, grad=0.0000), Var(v=-0.0888, grad=0.0000), Var(v=-0.0396, grad=0.0000)], [Var(v=-0.1800, grad=0.0000), Var(v=-0.0102, grad=0.0000), Var(v=-0.1847, grad=0.0000), Var(v=-0.0277, grad=0.0000), Var(v=0.1621, grad=0.0000), Var(v=0.1367, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.0598, grad=0.0000), Var(v=-0.0222, grad=0.0000), Var(v=0.1828, grad=0.0000), Var(v=0.0440, grad=0.0000), Var(v=-0.0089, grad=0.0000), Var(v=0.0101, grad=0.0000), Var(v=0.0382, grad=0.0000), Var(v=-0.0652, grad=0.0000), Var(v=0.0770, grad=0.0000), Var(v=-0.0495, grad=0.0000), Var(v=0.1316, grad=0.0000), Var(v=-0.0650, grad=0.0000), Var(v=-0.1258, grad=0.0000), Var(v=0.0588, grad=0.0000), Var(v=-0.1811, grad=0.0000), Var(v=-0.0226, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.0034, grad=0.0000), Var(v=-0.1500, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=0.0849, grad=0.0000), Var(v=0.0386, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=0.1768, grad=0.0000), Var(v=0.0094, grad=0.0000), Var(v=0.0213, grad=0.0000), Var(v=0.0271, grad=0.0000), Var(v=0.1021, grad=0.0000), Var(v=0.0661, grad=0.0000), Var(v=0.1259, grad=0.0000), Var(v=-0.0395, grad=0.0000), Var(v=-0.0570, grad=0.0000), Var(v=0.1144, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.0257, grad=0.0000), Var(v=0.0134, grad=0.0000), Var(v=0.0794, grad=0.0000), Var(v=0.1360, grad=0.0000), Var(v=-0.0127, grad=0.0000), Var(v=-0.0938, grad=0.0000), Var(v=0.0563, grad=0.0000), Var(v=0.0534, grad=0.0000), Var(v=-0.0770, grad=0.0000)], [Var(v=-0.0702, grad=0.0000), Var(v=-0.0100, grad=0.0000), Var(v=0.0272, grad=0.0000), Var(v=0.1555, grad=0.0000), Var(v=-0.0142, grad=0.0000), Var(v=0.0615, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=-0.0404, grad=0.0000), Var(v=0.1021, grad=0.0000), Var(v=-0.0477, grad=0.0000), Var(v=0.0190, grad=0.0000), Var(v=-0.1511, grad=0.0000), Var(v=0.2092, grad=0.0000), Var(v=0.1423, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.0909, grad=0.0000), Var(v=0.1141, grad=0.0000), Var(v=0.0053, grad=0.0000), Var(v=-0.1112, grad=0.0000), Var(v=0.0239, grad=0.0000), Var(v=0.1269, grad=0.0000), Var(v=-0.0274, grad=0.0000), Var(v=-0.1348, grad=0.0000), Var(v=0.0788, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=0.1700, grad=0.0000), Var(v=0.1992, grad=0.0000), Var(v=-0.1152, grad=0.0000), Var(v=-0.0300, grad=0.0000), Var(v=0.0619, grad=0.0000), Var(v=0.0501, grad=0.0000), Var(v=0.1708, grad=0.0000), Var(v=-0.0610, grad=0.0000), Var(v=-0.0748, grad=0.0000), Var(v=-0.0338, grad=0.0000), Var(v=-0.2473, grad=0.0000), Var(v=-0.2401, grad=0.0000), Var(v=0.0034, grad=0.0000), Var(v=-0.0014, grad=0.0000), Var(v=-0.1162, grad=0.0000), Var(v=0.0750, grad=0.0000), Var(v=-0.1281, grad=0.0000), Var(v=-0.1133, grad=0.0000), Var(v=0.0595, grad=0.0000), Var(v=0.0334, grad=0.0000), Var(v=-0.0134, grad=0.0000), Var(v=-0.0250, grad=0.0000), Var(v=-0.1540, grad=0.0000), Var(v=-0.0671, grad=0.0000), Var(v=-0.0676, grad=0.0000)], [Var(v=0.0142, grad=0.0000), Var(v=-0.0422, grad=0.0000), Var(v=0.0621, grad=0.0000), Var(v=-0.0859, grad=0.0000), Var(v=0.0631, grad=0.0000), Var(v=-0.2445, grad=0.0000), Var(v=-0.0193, grad=0.0000), Var(v=-0.1481, grad=0.0000), Var(v=0.0644, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=0.0759, grad=0.0000), Var(v=0.0082, grad=0.0000), Var(v=0.1192, grad=0.0000), Var(v=0.1516, grad=0.0000), Var(v=-0.1376, grad=0.0000), Var(v=0.0449, grad=0.0000), Var(v=0.0278, grad=0.0000), Var(v=-0.0991, grad=0.0000), Var(v=0.0431, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.0288, grad=0.0000), Var(v=0.1063, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0750, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0835, grad=0.0000), Var(v=-0.0418, grad=0.0000), Var(v=-0.0684, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=0.0037, grad=0.0000), Var(v=0.0236, grad=0.0000), Var(v=-0.0668, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=0.0548, grad=0.0000), Var(v=-0.1372, grad=0.0000), Var(v=0.0569, grad=0.0000), Var(v=0.1713, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0275, grad=0.0000), Var(v=-0.0858, grad=0.0000), Var(v=0.1180, grad=0.0000), Var(v=0.0075, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0650, grad=0.0000), Var(v=0.1179, grad=0.0000), Var(v=0.1988, grad=0.0000), Var(v=-0.1761, grad=0.0000), Var(v=-0.0966, grad=0.0000), Var(v=0.0367, grad=0.0000), Var(v=-0.0689, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0372, grad=0.0000), Var(v=-0.1080, grad=0.0000), Var(v=0.1612, grad=0.0000), Var(v=0.0009, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=-0.0841, grad=0.0000), Var(v=-0.0019, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0016, grad=0.0000), Var(v=-0.1157, grad=0.0000), Var(v=0.0006, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.1297, grad=0.0000), Var(v=-0.0905, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0016, grad=0.0000), Var(v=-0.0014, grad=0.0000), Var(v=0.1268, grad=0.0000), Var(v=-0.0996, grad=0.0000), Var(v=-0.0259, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=0.0329, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0017, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0025, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0572, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=0.1084, grad=0.0000), Var(v=-0.0017, grad=0.0000), Var(v=0.0012, grad=0.0000), Var(v=0.0002, grad=0.0000), Var(v=0.0441, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=0.0009, grad=0.0000), Var(v=0.0709, grad=0.0000), Var(v=-0.0009, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=0.0637, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=-0.0019, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0374, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0013, grad=0.0000)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0648, grad=0.0000)], [Var(v=0.0572, grad=0.0000)], [Var(v=0.0857, grad=0.0000)], [Var(v=0.2049, grad=0.0000)], [Var(v=-0.0798, grad=0.0000)], [Var(v=0.0653, grad=0.0000)], [Var(v=-0.1895, grad=0.0000)], [Var(v=0.0242, grad=0.0000)], [Var(v=-0.0848, grad=0.0000)], [Var(v=0.1716, grad=0.0000)], [Var(v=0.0050, grad=0.0000)], [Var(v=-0.1380, grad=0.0000)], [Var(v=0.1294, grad=0.0000)], [Var(v=-0.0380, grad=0.0000)], [Var(v=0.0319, grad=0.0000)], [Var(v=-0.1140, grad=0.0000)], [Var(v=0.0824, grad=0.0000)], [Var(v=0.0329, grad=0.0000)], [Var(v=0.1207, grad=0.0000)], [Var(v=0.1124, grad=0.0000)], [Var(v=0.1800, grad=0.0000)], [Var(v=-0.1070, grad=0.0000)], [Var(v=-0.0294, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0394, grad=0.0000)], [Var(v=-0.1125, grad=0.0000)], [Var(v=0.1560, grad=0.0000)], [Var(v=-0.0008, grad=0.0000)], [Var(v=-0.2495, grad=0.0000)], [Var(v=-0.0794, grad=0.0000)], [Var(v=-0.0492, grad=0.0000)], [Var(v=-0.0399, grad=0.0000)], [Var(v=0.1387, grad=0.0000)], [Var(v=0.1016, grad=0.0000)], [Var(v=-0.1011, grad=0.0000)], [Var(v=-0.0376, grad=0.0000)], [Var(v=-0.0610, grad=0.0000)], [Var(v=0.0670, grad=0.0000)], [Var(v=-0.0664, grad=0.0000)], [Var(v=-0.0864, grad=0.0000)], [Var(v=0.0475, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=0.0166, grad=0.0000)], [Var(v=-0.0810, grad=0.0000)], [Var(v=-0.0204, grad=0.0000)], [Var(v=0.1777, grad=0.0000)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0625, grad=0.0000)], [Var(v=-0.0512, grad=0.0000)], [Var(v=-0.1143, grad=0.0000)]] Biases: [Var(v=-0.0127, grad=0.0000)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "print('Network before update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "def parameters(network):\n",
        "  params = []\n",
        "  for layer in range(len(network)):\n",
        "    params += network[layer].parameters()\n",
        "  return params\n",
        "\n",
        "def update_parameters(params, learning_rate=0.01):\n",
        "  for p in params:\n",
        "    p.v -= learning_rate*p.grad\n",
        "\n",
        "def zero_gradients(params):\n",
        "  for p in params:\n",
        "    p.grad = 0.0\n",
        "\n",
        "update_parameters(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "zero_gradients(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after zeroing gradients:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woWYpdw6FtIO"
      },
      "outputs": [],
      "source": [
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "#NN = [\n",
        "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
        "#    DenseLayer(16, 1, lambda x: x.identity())\n",
        "#]\n",
        "\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdqaqYBVFtIR"
      },
      "outputs": [],
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kfg76GMFtIW",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c9ce8e-50ee-4a6e-bbd0-3a9fd0cb5c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 104.525 \t Validation loss: 103.596\n",
            "  10 ( 5.00%) Train loss: 19.428 \t Validation loss: 14.111\n",
            "  20 (10.00%) Train loss: 13.812 \t Validation loss: 10.486\n",
            "  30 (15.00%) Train loss: 13.328 \t Validation loss: 10.081\n",
            "  40 (20.00%) Train loss: 12.920 \t Validation loss: 9.720\n",
            "  50 (25.00%) Train loss: 12.670 \t Validation loss: 9.454\n",
            "  60 (30.00%) Train loss: 12.472 \t Validation loss: 9.261\n",
            "  70 (35.00%) Train loss: 12.301 \t Validation loss: 9.078\n",
            "  80 (40.00%) Train loss: 12.150 \t Validation loss: 8.966\n",
            "  90 (45.00%) Train loss: 12.025 \t Validation loss: 8.878\n",
            " 100 (50.00%) Train loss: 11.967 \t Validation loss: 8.847\n",
            " 110 (55.00%) Train loss: 11.931 \t Validation loss: 8.846\n",
            " 120 (60.00%) Train loss: 11.909 \t Validation loss: 8.857\n",
            " 130 (65.00%) Train loss: 11.897 \t Validation loss: 8.877\n",
            " 140 (70.00%) Train loss: 11.890 \t Validation loss: 8.897\n",
            " 150 (75.00%) Train loss: 11.885 \t Validation loss: 8.913\n",
            " 160 (80.00%) Train loss: 11.881 \t Validation loss: 8.926\n",
            " 170 (85.00%) Train loss: 11.878 \t Validation loss: 8.935\n",
            " 180 (90.00%) Train loss: 11.876 \t Validation loss: 8.942\n",
            " 190 (95.00%) Train loss: 11.873 \t Validation loss: 8.947\n"
          ]
        }
      ],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VetyRWFwFtIY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0d387052-d38d-4a7b-ee40-9342ba613c19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmNi7S-vFtIc"
      },
      "outputs": [],
      "source": [
        "output_test = forward(x_test, NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mmJOTSEFtIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "4e1e1743-df13-480b-e5e3-f8fdf8c36b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  9.767\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "y_test_np = Var_to_nparray(y_test)\n",
        "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
        "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODi0WlmQFtIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0bdc3785-7e68-4220-965e-ac895422b1de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x_test_np = Var_to_nparray(x_test)\n",
        "x_train_np = Var_to_nparray(x_train)\n",
        "y_train_np = Var_to_nparray(y_train)\n",
        "if D1:\n",
        "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
        "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
        "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk"
      },
      "source": [
        "## Exercise l) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n",
        "\n",
        "I don't see any big difference between validation and test loss. However, it is important to keep the 2 losses separate to get an idea into how the model will perform with new unseen data.\n",
        "\n",
        "To reproduce underfitting we decreased the learning rate to 2e-6. The losses remained constant. (~100)\n",
        "\n",
        "To reproduce overfitting we need to train the network for a large number of epochs and increase the network capacity. However, the training phase becomes cumbersome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQZCn2dxFtIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "168bb468-9b7a-462b-ce65-8cb90a2b9261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 105.315 \t Validation loss: 107.638\n",
            "  10 ( 5.00%) Train loss: 105.291 \t Validation loss: 107.617\n",
            "  20 (10.00%) Train loss: 105.268 \t Validation loss: 107.595\n",
            "  30 (15.00%) Train loss: 105.244 \t Validation loss: 107.573\n",
            "  40 (20.00%) Train loss: 105.221 \t Validation loss: 107.552\n",
            "  50 (25.00%) Train loss: 105.197 \t Validation loss: 107.530\n",
            "  60 (30.00%) Train loss: 105.174 \t Validation loss: 107.508\n",
            "  70 (35.00%) Train loss: 105.150 \t Validation loss: 107.487\n",
            "  80 (40.00%) Train loss: 105.127 \t Validation loss: 107.465\n",
            "  90 (45.00%) Train loss: 105.104 \t Validation loss: 107.444\n",
            " 100 (50.00%) Train loss: 105.081 \t Validation loss: 107.422\n",
            " 110 (55.00%) Train loss: 105.057 \t Validation loss: 107.401\n",
            " 120 (60.00%) Train loss: 105.034 \t Validation loss: 107.379\n",
            " 130 (65.00%) Train loss: 105.011 \t Validation loss: 107.358\n",
            " 140 (70.00%) Train loss: 104.988 \t Validation loss: 107.336\n",
            " 150 (75.00%) Train loss: 104.965 \t Validation loss: 107.315\n",
            " 160 (80.00%) Train loss: 104.941 \t Validation loss: 107.293\n",
            " 170 (85.00%) Train loss: 104.918 \t Validation loss: 107.272\n",
            " 180 (90.00%) Train loss: 104.895 \t Validation loss: 107.250\n",
            " 190 (95.00%) Train loss: 104.872 \t Validation loss: 107.229\n",
            "Underfitting\n",
            "Train loss: 104.851 \t Validation loss: 107.209\n",
            "Test loss:  99.623\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Insert your code for getting overfitting, underfitting and just right fitting\n",
        "\n",
        "# Underfitting\n",
        "\n",
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-6\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "\n",
        "    if e%10==0:\n",
        "      print(\"{:4d}\".format(e),\n",
        "            \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "            \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);\n",
        "print(\"Underfitting\")\n",
        "print(\"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Overfitting\n",
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "EPOCHS = 3000\n",
        "LEARN_R = 2e-3\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "\n",
        "    if e%10==0:\n",
        "      print(\"{:4d}\".format(e),\n",
        "            \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "            \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);\n",
        "print(\"Overfitting\")\n",
        "print(\"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhNvjcy-aRH4",
        "outputId": "a4fcbd9a-adc4-47ce-e51b-d95f71439758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 105.853 \t Validation loss: 105.069\n",
            "  10 ( 0.33%) Train loss: 28.429 \t Validation loss: 19.581\n",
            "  20 ( 0.67%) Train loss: 14.521 \t Validation loss: 10.786\n",
            "  30 ( 1.00%) Train loss: 14.108 \t Validation loss: 10.495\n",
            "  40 ( 1.33%) Train loss: 13.779 \t Validation loss: 10.313\n",
            "  50 ( 1.67%) Train loss: 13.418 \t Validation loss: 10.086\n",
            "  60 ( 2.00%) Train loss: 12.973 \t Validation loss: 9.828\n",
            "  70 ( 2.33%) Train loss: 12.537 \t Validation loss: 9.536\n",
            "  80 ( 2.67%) Train loss: 12.285 \t Validation loss: 9.352\n",
            "  90 ( 3.00%) Train loss: 12.141 \t Validation loss: 9.293\n",
            " 100 ( 3.33%) Train loss: 12.038 \t Validation loss: 9.265\n",
            " 110 ( 3.67%) Train loss: 11.964 \t Validation loss: 9.215\n",
            " 120 ( 4.00%) Train loss: 11.931 \t Validation loss: 9.181\n",
            " 130 ( 4.33%) Train loss: 11.913 \t Validation loss: 9.149\n",
            " 140 ( 4.67%) Train loss: 11.903 \t Validation loss: 9.125\n",
            " 150 ( 5.00%) Train loss: 11.898 \t Validation loss: 9.105\n",
            " 160 ( 5.33%) Train loss: 11.895 \t Validation loss: 9.091\n",
            " 170 ( 5.67%) Train loss: 11.894 \t Validation loss: 9.080\n",
            " 180 ( 6.00%) Train loss: 11.893 \t Validation loss: 9.072\n",
            " 190 ( 6.33%) Train loss: 11.893 \t Validation loss: 9.066\n",
            " 200 ( 6.67%) Train loss: 11.893 \t Validation loss: 9.062\n",
            " 210 ( 7.00%) Train loss: 11.892 \t Validation loss: 9.059\n",
            " 220 ( 7.33%) Train loss: 11.892 \t Validation loss: 9.057\n",
            " 230 ( 7.67%) Train loss: 11.892 \t Validation loss: 9.055\n",
            " 240 ( 8.00%) Train loss: 11.892 \t Validation loss: 9.054\n",
            " 250 ( 8.33%) Train loss: 11.892 \t Validation loss: 9.053\n",
            " 260 ( 8.67%) Train loss: 11.892 \t Validation loss: 9.052\n",
            " 270 ( 9.00%) Train loss: 11.892 \t Validation loss: 9.052\n",
            " 280 ( 9.33%) Train loss: 11.892 \t Validation loss: 9.052\n",
            " 290 ( 9.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 300 (10.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 310 (10.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 320 (10.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 330 (11.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 340 (11.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 350 (11.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 360 (12.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 370 (12.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 380 (12.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 390 (13.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 400 (13.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 410 (13.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 420 (14.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 430 (14.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 440 (14.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 450 (15.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 460 (15.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 470 (15.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 480 (16.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 490 (16.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 500 (16.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 510 (17.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 520 (17.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 530 (17.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 540 (18.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 550 (18.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 560 (18.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 570 (19.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 580 (19.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 590 (19.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 600 (20.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 610 (20.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 620 (20.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 630 (21.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 640 (21.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 650 (21.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 660 (22.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 670 (22.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 680 (22.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 690 (23.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 700 (23.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 710 (23.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 720 (24.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 730 (24.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 740 (24.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 750 (25.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 760 (25.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 770 (25.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 780 (26.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 790 (26.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 800 (26.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 810 (27.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 820 (27.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 830 (27.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 840 (28.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 850 (28.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 860 (28.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 870 (29.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 880 (29.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 890 (29.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 900 (30.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 910 (30.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 920 (30.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 930 (31.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 940 (31.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 950 (31.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 960 (32.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 970 (32.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 980 (32.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            " 990 (33.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1000 (33.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1010 (33.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1020 (34.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1030 (34.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1040 (34.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1050 (35.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1060 (35.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1070 (35.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1080 (36.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1090 (36.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1100 (36.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1110 (37.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1120 (37.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1130 (37.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1140 (38.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1150 (38.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1160 (38.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1170 (39.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1180 (39.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1190 (39.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1200 (40.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1210 (40.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1220 (40.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1230 (41.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1240 (41.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1250 (41.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1260 (42.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1270 (42.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1280 (42.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1290 (43.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1300 (43.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1310 (43.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1320 (44.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1330 (44.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1340 (44.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1350 (45.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1360 (45.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1370 (45.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1380 (46.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1390 (46.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1400 (46.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1410 (47.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1420 (47.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1430 (47.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1440 (48.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1450 (48.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1460 (48.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1470 (49.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1480 (49.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1490 (49.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1500 (50.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1510 (50.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1520 (50.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1530 (51.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1540 (51.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1550 (51.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1560 (52.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1570 (52.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1580 (52.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1590 (53.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1600 (53.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1610 (53.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1620 (54.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1630 (54.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1640 (54.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1650 (55.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1660 (55.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1670 (55.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1680 (56.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1690 (56.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1700 (56.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1710 (57.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1720 (57.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1730 (57.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1740 (58.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1750 (58.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1760 (58.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1770 (59.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1780 (59.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1790 (59.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1800 (60.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1810 (60.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1820 (60.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1830 (61.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1840 (61.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1850 (61.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1860 (62.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1870 (62.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1880 (62.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1890 (63.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1900 (63.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1910 (63.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1920 (64.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1930 (64.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1940 (64.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1950 (65.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1960 (65.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1970 (65.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1980 (66.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "1990 (66.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2000 (66.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2010 (67.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2020 (67.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2030 (67.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2040 (68.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2050 (68.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2060 (68.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2070 (69.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2080 (69.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2090 (69.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2100 (70.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2110 (70.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2120 (70.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2130 (71.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2140 (71.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2150 (71.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2160 (72.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2170 (72.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2180 (72.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2190 (73.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2200 (73.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2210 (73.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2220 (74.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2230 (74.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2240 (74.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2250 (75.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2260 (75.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2270 (75.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2280 (76.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2290 (76.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2300 (76.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2310 (77.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2320 (77.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2330 (77.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2340 (78.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2350 (78.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2360 (78.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2370 (79.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2380 (79.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2390 (79.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2400 (80.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2410 (80.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2420 (80.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2430 (81.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2440 (81.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2450 (81.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2460 (82.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2470 (82.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2480 (82.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2490 (83.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2500 (83.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2510 (83.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2520 (84.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2530 (84.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2540 (84.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2550 (85.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2560 (85.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2570 (85.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2580 (86.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2590 (86.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2600 (86.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2610 (87.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2620 (87.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2630 (87.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2640 (88.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2650 (88.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2660 (88.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2670 (89.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2680 (89.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2690 (89.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2700 (90.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2710 (90.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2720 (90.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2730 (91.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2740 (91.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2750 (91.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2760 (92.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2770 (92.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2780 (92.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2790 (93.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2800 (93.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2810 (93.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2820 (94.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2830 (94.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2840 (94.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2850 (95.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2860 (95.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2870 (95.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2880 (96.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2890 (96.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2900 (96.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2910 (97.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2920 (97.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2930 (97.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2940 (98.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2950 (98.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2960 (98.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2970 (99.00%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2980 (99.33%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "2990 (99.67%) Train loss: 11.892 \t Validation loss: 9.051\n",
            "Overfitting\n",
            "Train loss: 11.892 \t Validation loss: 9.051\n",
            "Test loss:  9.864\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo"
      },
      "source": [
        "## Exercise m) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC8QrI2tFtIp"
      },
      "outputs": [],
      "source": [
        "# Just add code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APqhJv3tta1O"
      },
      "source": [
        "## Exercise n) optional - Introduce a NeuralNetwork class\n",
        "\n",
        "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqfnor1ouMLq"
      },
      "outputs": [],
      "source": [
        "# just add some code"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "U4057_ljNvWB",
        "jpIZPBpNI0pO",
        "p_8n_SKnIW2F",
        "oLrGJytZFtGm",
        "_79HOAXrFtHK",
        "mqeyab9qFtGs",
        "-XyXBD37FtHk",
        "zTBAmjsAFtIk",
        "qsVPul3QFtIo",
        "APqhJv3tta1O"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}