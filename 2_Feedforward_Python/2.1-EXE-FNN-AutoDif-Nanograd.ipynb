{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
        "\n",
        "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
        "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Nanograd automatic differentiation framework\n",
        "2. Finite difference method\n",
        "3. Data generation\n",
        "4. Defining and initializing the network\n",
        "5. Forward pass\n",
        "6. Training loop \n",
        "7. Testing your model\n",
        "8. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyXeAA-HuT7s"
      },
      "source": [
        "# Nanograd automatic differention framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6UWKCLKubgA"
      },
      "source": [
        "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Jd4CoEBNzNWS"
      },
      "outputs": [],
      "source": [
        "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
        "\n",
        "from math import exp, log\n",
        "\n",
        "class Var:\n",
        "    \"\"\"\n",
        "    A variable which holds a float and enables gradient computations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, val: float, grad_fn=lambda: []):\n",
        "        assert type(val) == float\n",
        "        self.v = val\n",
        "        self.grad_fn = grad_fn\n",
        "        self.grad = 0.0\n",
        "\n",
        "    def backprop(self, bp):\n",
        "        self.grad += bp\n",
        "        for input, grad in self.grad_fn():\n",
        "            input.backprop(grad * bp)\n",
        "\n",
        "    def backward(self):\n",
        "        self.backprop(1.0)\n",
        "\n",
        "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
        "\n",
        "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        assert type(power) in {float, int}, \"power must be float or int\"\n",
        "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
        "\n",
        "    def __neg__(self: 'Var') -> 'Var':\n",
        "        return Var(-1.0) * self\n",
        "\n",
        "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self + (-other)\n",
        "\n",
        "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self * other ** -1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
        "\n",
        "    def relu(self):\n",
        "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        return Var(1/(1+exp(-self.v)), lambda: [(self, exp(self.v)/((exp(self.v)+1)**2))])\n",
        "\n",
        "    def tanh(self):\n",
        "        return Var((exp(2*self.v)-1)/(exp(2*self.v)+1), lambda: [(self, 4/((exp(-self.v)+exp(self.v))**2))])\n",
        "\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])\n",
        "    \n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "\n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDX67D6jzcte"
      },
      "source": [
        "A few examples illustrate how we can use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6PeLc3zwPT",
        "outputId": "f310e91c-166c-408a-8eb8-a08d5548c559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "f = a * b\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmKhYgsY0g_o",
        "outputId": "bbec2bea-8a4b-4fab-de77-ca178e85b30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=14.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "c = a * b\n",
        "d = Var(9.0)\n",
        "e = a * d\n",
        "f = c + e\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3B6uEH140p"
      },
      "source": [
        "## Exercise a) What is being calculated?\n",
        "\n",
        "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (a)**\n",
        "\n",
        "The result Var($v$,grad) tells you the value $v$ of that defined object, and grad tells you how much the factor in which the value has changed. For example for $a$ in cell [5], $v=3.0$, and grad$=14$, as $14*a.v=42$, as $f=a*d+a*b$, therefore $a=d+b$. That is $∂f/∂a=d+b=14$. So $a$, $b$ and $d$ are the independent variables in this case, and $c$, $e$, and $f$ the dependent ones. \n",
        "\n",
        "And for completeness sake: $∂f/∂b=∂f/∂d=a=3$."
      ],
      "metadata": {
        "id": "rfg1rpHGAATh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_Q0t2I3Ruj"
      },
      "source": [
        "## Exercise b) How does the backward function work?\n",
        "\n",
        "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
        "\n",
        "Go through the following four steps and answer the questions on the way:\n",
        "\n",
        "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
        "\n",
        "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
        "\n",
        "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. \n",
        "\n",
        "4. Write down the sequence of calls to backprop."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (b)**\n",
        "\n",
        "The answer provided for exercise (a), assuming it is correct, pretty much describes what happens, and the procedure behind the code, so I refer to there and to the graphviz diagrams of the data schematics for section (b1), (b2), and (b3).\n",
        "\n",
        "**(b4)** The sequence of calls to backprop is:\n",
        "\n",
        "1. f.backward() is called\n",
        "\n",
        "2. f.grad is set to $∂f/∂f=1$\n",
        "\n",
        "3. Since $f$ is defined as $c+e$, which is stored in f.grad_fn, the backprop moves on to these objects. $c$ is here calculated/shown first in step (4), but $c$ and $e$ could be interchanged, meaning the following steps (4) and (5) could be interchanged regardless.\n",
        "\n",
        "4. c.grad is set to 1, since $∂f/∂c=1$. $c$ is defined as $c=a*b$, and backprop then moves onto these objects.\n",
        "\n",
        "  4.1 Starting with $a$, since $∂c/∂a=b=5$, a.grad is set to 5.\n",
        "\n",
        "  4.2 Now $b$, since $∂c/∂b=a=3$, b.grad is set to 3.\n",
        "\n",
        "5. e.grad is set to 1, since $∂f/∂e=1$. $e$ is defined as $e=a*d$, and, same as (4), backprop then moves onto these objects.\n",
        "\n",
        "  5.1 Since $∂e/∂a=d=9$, a.grad would be set to 9, but since a.grad is already set to 5, it therefore becomes a.grad$=d+b=9+5=14$\n",
        "\n",
        "  5.2 $∂e/∂d=a=3$, so d.grad is set to 3.\n",
        "\n",
        "Which concludes the f.backprop()\n"
      ],
      "metadata": {
        "id": "iWfEpItIFZqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "idGr71jYXl26"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "import graphviz\n",
        "\n",
        "#logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
        "\n",
        "#graphviz.__version__, graphviz.version()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "KPe30Q2QXzeG",
        "outputId": "6251c8c1-86ad-4083-f80c-3d3f149d01ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fc31560ba90>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: first expression Pages: 1 -->\n<svg width=\"206pt\" height=\"98pt\"\n viewBox=\"0.00 0.00 205.99 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>first expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 201.9942,-94 201.9942,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-72\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- f=a*b -->\n<g id=\"node2\" class=\"node\">\n<title>f=a*b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"165.4971\" cy=\"-45\" rx=\"32.4942\" ry=\"32.4942\"/>\n<text text-anchor=\"middle\" x=\"165.4971\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f=a*b</text>\n</g>\n<!-- a&#45;&gt;f=a*b -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;f=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.7916,-68.7432C57.3815,-64.791 94.4929,-57.9976 123.5023,-52.6873\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.1803,-56.1215 133.3866,-50.878 122.9198,-49.2359 124.1803,-56.1215\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/da=b=5</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;f=a*b -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;f=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.8992,-21.0156C55.3802,-24.326 87.4369,-29.8561 115,-35 117.8004,-35.5226 120.6862,-36.0701 123.5929,-36.6282\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.9775,-40.074 133.4615,-38.5457 124.3127,-33.2025 122.9775,-40.074\"/>\n<text text-anchor=\"middle\" x=\"84.5\" y=\"-38.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/db=a=3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
        "\n",
        "e1.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e1.attr('node', shape='circle')\n",
        "e1.edge('a', 'f=a*b', label='df/da=b=5')\n",
        "e1.edge('b', 'f=a*b', label='df/db=a=3')\n",
        "\n",
        "e1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "0nittR-mZFeX",
        "outputId": "ec84aebd-518b-4568-e52b-536fb91e7dd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fc315626f10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: second expression Pages: 1 -->\n<svg width=\"358pt\" height=\"162pt\"\n viewBox=\"0.00 0.00 357.59 161.59\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 157.594)\">\n<title>second expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-157.594 353.5882,-157.594 353.5882,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-76.797\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-73.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- c=a*b -->\n<g id=\"node2\" class=\"node\">\n<title>c=a*b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"168.797\" cy=\"-119.797\" rx=\"33.5952\" ry=\"33.5952\"/>\n<text text-anchor=\"middle\" x=\"168.797\" y=\"-116.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">c=a*b</text>\n</g>\n<!-- a&#45;&gt;c=a*b -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;c=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.535,-81.7207C55.3302,-87.2873 88.4941,-96.6384 117,-104.797 120.1052,-105.6857 123.3142,-106.6074 126.5426,-107.5369\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.6233,-110.9143 136.2016,-110.324 127.564,-104.1887 125.6233,-110.9143\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-108.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/da=b=5</text>\n</g>\n<!-- e=a*d -->\n<g id=\"node4\" class=\"node\">\n<title>e=a*d</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"168.797\" cy=\"-33.797\" rx=\"33.5952\" ry=\"33.5952\"/>\n<text text-anchor=\"middle\" x=\"168.797\" y=\"-30.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">e=a*d</text>\n</g>\n<!-- a&#45;&gt;e=a*d -->\n<g id=\"edge3\" class=\"edge\">\n<title>a&#45;&gt;e=a*d</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.5935,-71.7802C57.7207,-65.4706 96.4474,-54.4276 126.4596,-45.8696\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.6102,-49.1811 136.267,-43.073 125.6906,-42.4494 127.6102,-49.1811\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-70.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/da=d=9</text>\n</g>\n<!-- f=c+e -->\n<g id=\"node6\" class=\"node\">\n<title>f=c+e</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"317.0911\" cy=\"-76.797\" rx=\"32.4942\" ry=\"32.4942\"/>\n<text text-anchor=\"middle\" x=\"317.0911\" y=\"-73.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f=c+e</text>\n</g>\n<!-- c=a*b&#45;&gt;f=c+e -->\n<g id=\"edge5\" class=\"edge\">\n<title>c=a*b&#45;&gt;f=c+e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M201.3408,-110.3605C223.2738,-104.0007 252.3875,-95.5587 275.9293,-88.7325\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"277.0107,-92.0631 285.6403,-85.9166 275.0612,-85.3401 277.0107,-92.0631\"/>\n<text text-anchor=\"middle\" x=\"243.594\" y=\"-108.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/dc=1</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-133.797\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-130.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;c=a*b -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;c=a*b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M36.1896,-132.1083C58.0524,-130.0785 95.483,-126.6035 125.013,-123.8619\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.4565,-127.3359 135.0901,-122.9263 124.8094,-120.3658 125.4565,-127.3359\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-133.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/db=a=3</text>\n</g>\n<!-- e=a*d&#45;&gt;f=c+e -->\n<g id=\"edge6\" class=\"edge\">\n<title>e=a*d&#45;&gt;f=c+e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M201.3408,-43.2335C223.2738,-49.5933 252.3875,-58.0353 275.9293,-64.8615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"275.0612,-68.2539 285.6403,-67.6774 277.0107,-61.5309 275.0612,-68.2539\"/>\n<text text-anchor=\"middle\" x=\"243.594\" y=\"-65.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/de=1</text>\n</g>\n<!-- d -->\n<g id=\"node5\" class=\"node\">\n<title>d</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-19.797\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-16.097\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">d</text>\n</g>\n<!-- d&#45;&gt;e=a*d -->\n<g id=\"edge4\" class=\"edge\">\n<title>d&#45;&gt;e=a*d</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.972,-18.7139C55.8755,-17.8078 88.8712,-17.1767 117,-20.797 120.0288,-21.1868 123.1357,-21.6915 126.2503,-22.2751\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.6318,-25.7216 136.1392,-24.3635 127.0782,-18.8727 125.6318,-25.7216\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-24.597\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/dd=a=3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
        "\n",
        "e2.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e2.attr('node', shape='circle')\n",
        "e2.edge('a', 'c=a*b', label='dc/da=b=5')\n",
        "e2.edge('b', 'c=a*b', label='dc/db=a=3')\n",
        "e2.edge('a', 'e=a*d', label='de/da=d=9')\n",
        "e2.edge('d', 'e=a*d', label='de/dd=a=3')\n",
        "e2.edge('c=a*b', 'f=c+e', label='df/dc=1')\n",
        "e2.edge('e=a*d', 'f=c+e', label='df/de=1')\n",
        "\n",
        "e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5oi21W4gpeM"
      },
      "source": [
        "## Exercise c) What happens if we run backward again?\n",
        "\n",
        "Try to execute the code below. Explain what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCtpJyr-gyX1",
        "outputId": "90e15fba-8fec-4646-d2e2-3595f45536f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=28.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n"
          ]
        }
      ],
      "source": [
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (c)**\n",
        "\n",
        "The gradient is multiplied by 2, as the same gradients are calculated, and are added back to the variable gradient (from code: bp*grad, and bp=2, for the number of backpropagations done)."
      ],
      "metadata": {
        "id": "LArElv7DO3x5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bPVq2VhsP-"
      },
      "source": [
        "## Exercise d) Zero gradient\n",
        "\n",
        "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnyPDQx9lJe0",
        "outputId": "7c024864-45be-49eb-9711-8363bb23b2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n",
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(2.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)\n",
        "\n",
        "f.backprop(-1.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (d)**\n",
        "\n",
        "Even though a is redefined, the old a is still used/saved in f, and can be accessed by grad_fn() calls. What backprop(-1) then does is go back a step. So running the previous code in tandem, it would do two backpropagations, and then undo one/do a forward propagation."
      ],
      "metadata": {
        "id": "B7NMCByXWVen"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4057_ljNvWB"
      },
      "source": [
        "## Exercise e) Test correctness of derivatives with the finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
        "$$\n",
        "for $da \\ll 1$.\n",
        "\n",
        "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (e)**\n",
        "\n",
        "Below is the finite difference code snippet, calculating a numerical partial derivative at a point."
      ],
      "metadata": {
        "id": "eIPL2EwAM9kL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "9TGil92lSXDN",
        "outputId": "475d96a2-54a3-43b3-8b14-c1b152667263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "5.000000413701855\n"
          ]
        }
      ],
      "source": [
        "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
        "def f_function(a):\n",
        "  a = Var(a)\n",
        "  b = Var(5.0)\n",
        "  f = a * b\n",
        "  f.backward()\n",
        "  return a,b,f\n",
        "\n",
        "for v in f_function(3.0):\n",
        "  print(v)\n",
        "\n",
        "# Insert your finite difference code here\n",
        "def finite_difference(a, da=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    a:           Point of gradient\n",
        "    da:          The finite difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: numerical approximation to the derivative (float) \n",
        "    \"\"\"\n",
        "    a = float(a) if isinstance(a,int) else a\n",
        "    fa_da = f_function(a+da)[2].v\n",
        "    fa = f_function(a)[2].v\n",
        "\n",
        "    finite_difference = (fa_da - fa) / da\n",
        "    \n",
        "    return finite_difference\n",
        "\n",
        "print(finite_difference(a=3.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZar5RKaUkg"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Y6yfMAQ8aduj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "4YabfD43ajNh"
      },
      "outputs": [],
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "u1oDngHLapIz"
      },
      "outputs": [],
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Ysfa3FsBavlm",
        "outputId": "759ab00f-eae7-4872-fbff-1039ec87cafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "zac2HHNlgbpm"
      },
      "outputs": [],
      "source": [
        "# convert from nparray to Var\n",
        "def nparray_to_Var(x):\n",
        "  if x.ndim==1:\n",
        "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
        "  else:\n",
        "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
        "  return y\n",
        "   \n",
        "x_train = nparray_to_Var(x_train)\n",
        "y_train = nparray_to_Var(y_train)\n",
        "x_validation = nparray_to_Var(x_validation)\n",
        "y_validation = nparray_to_Var(y_validation)\n",
        "x_test = nparray_to_Var(x_test)\n",
        "y_test = nparray_to_Var(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe"
      },
      "source": [
        "# Defining and initializing the network\n",
        "\n",
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
        "\n",
        "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "ij_ieRsAt7Xt"
      },
      "outputs": [],
      "source": [
        "class Initializer:\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  ## Glorot\n",
        "  def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
        "    std = 2/(n_in+n_out) # <- replace with proper initialization\n",
        "    return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
        "\n",
        "  ## He\n",
        "  def DenseLayer_He_relu(n_in: int, n_out: int):\n",
        "    std = 2/n_in # <- replace with proper initialization\n",
        "    return DenseLayer(n_in, n_out, lambda x: x.relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "eb18N5phuIha"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class NormalInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, mean=0, std=0.1):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(0.0) for _ in range(n_out)]\n",
        "\n",
        "class ConstantInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, weight=1.0, bias=0.0):\n",
        "    self.weight = weight\n",
        "    self.bias = bias\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(self.bias) for _ in range(n_out)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "jOLYGnZKuM6W"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
        "        self.weights = initializer.init_weights(n_in, n_out)\n",
        "        self.bias = initializer.init_bias(n_out)\n",
        "        self.act_fn = act_fn\n",
        "    \n",
        "    def __repr__(self):    \n",
        "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
        "\n",
        "    def parameters(self) -> Sequence[Var]:\n",
        "      params = []\n",
        "      for r in self.weights:\n",
        "        params += r\n",
        "\n",
        "      return params + self.bias\n",
        "\n",
        "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input\n",
        "        # to the current layer matches the number of nodes in the current layer\n",
        "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "        weights = self.weights\n",
        "        out = []\n",
        "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "        # We therefore loop over the (number of) nodes in the current layer:\n",
        "        for j in range(len(weights[0])):\n",
        "            # Initialize the node value depending on its corresponding parameters.\n",
        "            node = self.bias[j]  # <- Insert code\n",
        "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "            for i in range(len(single_input)):\n",
        "                node += weights[i][j] * single_input[i]  # <- Insert code\n",
        "            node = self.act_fn(node)\n",
        "            out.append(node)\n",
        "\n",
        "        return out\n",
        "      \n",
        "    def describe(self):\n",
        "        # return f'Input nodes: {self.n_in}, output nodes: {self.n_out} ||' +  f'Weights of input{i}: ' for i in range(len(weights[0]))\n",
        "        n_out = len(self.weights[0])\n",
        "        n_in = len(self.weights)\n",
        "        params = self.parameters()\n",
        "        description = [[f'Weight from input {j} to node {i}: {params[n_out * j + i]} and bias {params[-(n_out-i)]}' for i\n",
        "          in range(n_out)] for j in range(n_in)]\n",
        "        return sum(description, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIZPBpNI0pO"
      },
      "source": [
        "## Exercise f) Add more activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
        " \n",
        "Implement the following activation functions in the Var class:\n",
        "\n",
        "* Identity: $$\\mathrm{identity}(x) = x$$\n",
        "* Hyperbolic tangent: $$\\tanh(x)$$\n",
        "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
        "\n",
        "Hint: You can seek inspiration in the relu method in the Var class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (f)**\n",
        "\n",
        "The following code snippet is implemented in the previous Var class."
      ],
      "metadata": {
        "id": "_AabEBeucF4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def sigmoid(self):\n",
        "        return Var(1/(1+exp(-self.v)), lambda: [(self, exp(self.v)/((exp(self.v)+1)**2))])\n",
        "\n",
        "    def tanh(self):\n",
        "        return Var((exp(2*self.v)-1)/(exp(2*self.v)+1), lambda: [(self, 4/((exp(-self.v)+exp(self.v))**2))])\n",
        "\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])"
      ],
      "metadata": {
        "id": "hSKslIHFcBuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8n_SKnIW2F"
      },
      "source": [
        "## Exercise g) Complete the forward pass\n",
        "\n",
        "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "xDEjtePxE7Mv",
        "outputId": "8cd1de46-5bb6-45a7-afba-11e2f755dc8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Var(v=-0.0023, grad=0.0000)], [Var(v=-0.0047, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=-0.0142, grad=0.0000)], [Var(v=-0.0034, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0034, grad=0.0000)], [Var(v=-0.0146, grad=0.0000)], [Var(v=-0.0013, grad=0.0000)], [Var(v=-0.0036, grad=0.0000)], [Var(v=-0.0090, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=-0.0103, grad=0.0000)], [Var(v=-0.0015, grad=0.0000)], [Var(v=-0.0030, grad=0.0000)], [Var(v=-0.0055, grad=0.0000)], [Var(v=-0.0026, grad=0.0000)], [Var(v=-0.0003, grad=0.0000)], [Var(v=-0.0009, grad=0.0000)], [Var(v=-0.0068, grad=0.0000)], [Var(v=-0.0030, grad=0.0000)], [Var(v=-0.0040, grad=0.0000)], [Var(v=-0.0041, grad=0.0000)], [Var(v=-0.0040, grad=0.0000)], [Var(v=-0.0106, grad=0.0000)], [Var(v=-0.0052, grad=0.0000)], [Var(v=-0.0057, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0036, grad=0.0000)], [Var(v=-0.0133, grad=0.0000)], [Var(v=-0.0009, grad=0.0000)], [Var(v=-0.0116, grad=0.0000)], [Var(v=-0.0126, grad=0.0000)], [Var(v=-0.0028, grad=0.0000)], [Var(v=-0.0023, grad=0.0000)], [Var(v=-0.0144, grad=0.0000)], [Var(v=-0.0055, grad=0.0000)], [Var(v=-0.0102, grad=0.0000)], [Var(v=-0.0073, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.0070, grad=0.0000)], [Var(v=-0.0054, grad=0.0000)], [Var(v=-0.0087, grad=0.0000)], [Var(v=-0.0069, grad=0.0000)], [Var(v=-0.0084, grad=0.0000)], [Var(v=-0.0035, grad=0.0000)], [Var(v=-0.0033, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=-0.0034, grad=0.0000)], [Var(v=-0.0015, grad=0.0000)], [Var(v=-0.0001, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=-0.0071, grad=0.0000)], [Var(v=-0.0058, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.0047, grad=0.0000)], [Var(v=-0.0052, grad=0.0000)], [Var(v=-0.0019, grad=0.0000)], [Var(v=-0.0125, grad=0.0000)], [Var(v=-0.0034, grad=0.0000)], [Var(v=-0.0088, grad=0.0000)], [Var(v=-0.0145, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=-0.0012, grad=0.0000)], [Var(v=-0.0119, grad=0.0000)], [Var(v=-0.0128, grad=0.0000)], [Var(v=-0.0043, grad=0.0000)], [Var(v=-0.0091, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0035, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=-0.0033, grad=0.0000)], [Var(v=-0.0019, grad=0.0000)], [Var(v=-0.0013, grad=0.0000)], [Var(v=-0.0052, grad=0.0000)], [Var(v=-0.0063, grad=0.0000)], [Var(v=-0.0061, grad=0.0000)], [Var(v=-0.0081, grad=0.0000)], [Var(v=-0.0055, grad=0.0000)], [Var(v=-0.0008, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0131, grad=0.0000)], [Var(v=-0.0018, grad=0.0000)], [Var(v=-0.0067, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0074, grad=0.0000)], [Var(v=-0.0136, grad=0.0000)], [Var(v=-0.0141, grad=0.0000)], [Var(v=-0.0020, grad=0.0000)], [Var(v=-0.0043, grad=0.0000)], [Var(v=-0.0078, grad=0.0000)], [Var(v=-0.0031, grad=0.0000)], [Var(v=-0.0112, grad=0.0000)], [Var(v=-0.0135, grad=0.0000)], [Var(v=-0.0014, grad=0.0000)], [Var(v=-0.0002, grad=0.0000)], [Var(v=-0.0050, grad=0.0000)], [Var(v=-0.0079, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=-0.0077, grad=0.0000)], [Var(v=-0.0118, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)]]\n"
          ]
        }
      ],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "def forward(input, network):\n",
        "\n",
        "  def forward_single(x, network):\n",
        "    for layer in network:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  output = [ forward_single(input[n], network) for n in range(len(input))]\n",
        "  return output\n",
        "\n",
        "print(forward(x_train, NN))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (g)**\n",
        "\n",
        "The following code snippet has been implemented ind the previous DenseLayer Class, as the function forward."
      ],
      "metadata": {
        "id": "nNWWvxQCeNf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "    # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input\n",
        "    # to the current layer matches the number of nodes in the current layer\n",
        "    assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "    weights = self.weights\n",
        "    out = []\n",
        "    # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "    # We therefore loop over the (number of) nodes in the current layer:\n",
        "    for j in range(len(weights[0])):\n",
        "        # Initialize the node value depending on its corresponding parameters.\n",
        "        node = self.bias[j]  # <- Insert code\n",
        "        # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "        for i in range(len(single_input)):\n",
        "            node += weights[i][j] * single_input[i]  # <- Insert code\n",
        "        node = self.act_fn(node)\n",
        "        out.append(node)"
      ],
      "metadata": {
        "id": "Eg210D_gectN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm"
      },
      "source": [
        "## Exercise h) Print all network parameters\n",
        "\n",
        "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (h)**\n",
        "\n",
        "The following code snippet has been implemented in the DenseLayer class. The intended behaviour, is that a .describe() call would show a string describing the weight from an input to the connected node, and the bias."
      ],
      "metadata": {
        "id": "vc2akTHMeqFD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iac-VwYGFtGm"
      },
      "outputs": [],
      "source": [
        "    def describe(self):\n",
        "        # return f'Input nodes: {self.n_in}, output nodes: {self.n_out} ||' +  f'Weights of input{i}: ' for i in range(len(weights[0]))\n",
        "        n_out = len(self.weights[0])\n",
        "        n_in = len(self.weights)\n",
        "        params = self.parameters()\n",
        "        description = [[f'Weight from input {j} to node {i}: {params[n_out * j + i]} and bias {params[-(n_out-i)]}' for i\n",
        "          in range(n_out)] for j in range(n_in)]\n",
        "        return sum(description, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for this following code snippet, the neural network is described in full, assuming the input is of the format of the previously used definition of NN, which is a list of DenseLayer class calls."
      ],
      "metadata": {
        "id": "u9-p1YMzm1SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_NN(NN):\n",
        "  for i, j in enumerate(NN):\n",
        "    if i==0:\n",
        "      print(f'Layer {i} (input) to {i+1}:')\n",
        "    elif i==len(NN)-1:\n",
        "      print(f'Layer {i} to {i+1} (output):')\n",
        "    else:\n",
        "      print(f'Layer {i} to {i+1}:')\n",
        "    for x in j.describe():\n",
        "        print(x)\n",
        "\n",
        "describe_NN(NN=NN)"
      ],
      "metadata": {
        "id": "AfDvTME8nJ8x",
        "outputId": "eb53d8eb-fedb-4243-866c-ef1d0c371ae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (input) to 1:\n",
            "Weight from input 0 to node 0: Var(v=0.1201, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 0 to node 1: Var(v=0.0575, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 0 to node 2: Var(v=-0.0978, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 0 to node 3: Var(v=0.1245, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 0 to node 4: Var(v=0.0589, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Layer 1 to 2 (output):\n",
            "Weight from input 0 to node 0: Var(v=-0.0520, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 1 to node 0: Var(v=0.0771, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 2 to node 0: Var(v=-0.0954, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 3 to node 0: Var(v=0.0339, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n",
            "Weight from input 4 to node 0: Var(v=-0.0954, grad=0.0000) and bias Var(v=0.0000, grad=0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "1FcylHqLTl-Z",
        "outputId": "b6256c98-ed42-4051-d3c0-40867b341dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc3088c3d50>]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "\n",
        "# convert from Var to ndarray  \n",
        "def Var_to_nparray(x):\n",
        "  y = np.zeros((len(x),len(x[0])))\n",
        "  for i in range(len(x)):\n",
        "    for j in range(len(x[0])):\n",
        "      y[i,j] = x[i][j].v\n",
        "  return y\n",
        "\n",
        "# define 1-1 network with weight = 1 and relu activation \n",
        "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
        "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
        "\n",
        "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "oOL2UolJFtHL",
        "outputId": "d9f2d938-a1c3-4db8-fa22-8af7fbe15ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing all activation layers\n",
        "\n",
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"identity\": lambda x: x.identity(),\n",
        "    #\"sigmoid\": lambda x: x.sigmoid(),  <- uncomment before sharing\n",
        "    \"relu\": lambda x: x.relu(),\n",
        "    #\"tanh\": lambda x: x.tanh() <- uncomment before sharing\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "The Glorot initialization has the form: \n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
        "\n",
        "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
        "\n",
        "The He initialization is very similar\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs"
      },
      "source": [
        "## Exercise i) Glorot and He initialization\n",
        " \n",
        "Using the Initializer class, implement functions that implement Glorot and He \n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
        "\n",
        "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (i)**\n",
        "\n",
        "Below is the code snippet of the attempt at implementing the Glorot and He initialization schemes. These are also found implemented in the Initializer class from before.\n",
        "\n",
        "A numerical test would simply be to plot the activation values of each layer, which should result in very alike distributions around 0 mean (see figure 6 of the Glorot paper). Additionally, if it holds, the ratio between ${\\mathbf z^i}$ to ${\\mathbf z^{i+1}}$ should be around 0.8, and 0.5 for the more common method ($1/\\sqrt(n)$) (see the Glorot paper section 4.2.2)."
      ],
      "metadata": {
        "id": "zfJAtlNrr9H2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Qyk01CgaFtGt"
      },
      "outputs": [],
      "source": [
        "## Glorot\n",
        "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
        "  std = 2/(n_in+n_out) # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
        "\n",
        "## He\n",
        "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
        "  std = 2/n_in # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))\n",
        "\n",
        "def DenseLayer_init(n_in: int, n_out: int, act_fn, init_type: str=None):\n",
        "  assert init_type in ['Glorot', 'He'], f\"Choose initialization type. Must be either 'Glorot' or 'He'\"\n",
        "  if init_type == 'Glorot':\n",
        "    alpha = 1\n",
        "    std = 2*alpha/(n_in+n_out)\n",
        "  elif init_type == 'He':\n",
        "    alpha = 2\n",
        "    std = alpha/n_in\n",
        "  return DenseLayer(n_in, n_out, act_fn, initializer = NormalInitializer(std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk"
      },
      "source": [
        "## Exercise j) Forward pass unit test\n",
        "\n",
        "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
        "\n",
        "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (j)**\n",
        "\n",
        "For the unit test a simple 1-2-1 layer network is made, with the ConstantInitializer initializer, and the identity activation function, for both in and out of the hidden layer of 2 nodes. This means the weights will remain 1, and the activation will be the input itself. Therefore the output layer will output half the value of the input, which then will be tested for, seen below."
      ],
      "metadata": {
        "id": "knpMcVWYxpEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "k0miqRUAFtHl"
      },
      "outputs": [],
      "source": [
        "UT_NN = [\n",
        "    DenseLayer(1, 2, lambda x: x.identity(), initializer=ConstantInitializer()),\n",
        "    DenseLayer(2, 1, lambda x: x.identity(), initializer=ConstantInitializer())\n",
        "]\n",
        "\n",
        "input_values = Var_to_nparray(x_train[:5])\n",
        "test_nn_results = forward(x_train[:5], UT_NN)\n",
        "adjusted_test_nn_results = Var_to_nparray([[i[0]*Var(0.5)] for i in test_nn_results])\n",
        "assert all(input_values == adjusted_test_nn_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp"
      },
      "source": [
        "# Loss functions\n",
        "\n",
        "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "I2eDYKvAFtHq"
      },
      "outputs": [],
      "source": [
        "def squared_loss(t, y):\n",
        "  \n",
        "  # add check that sizes agree\n",
        "  \n",
        "  def squared_loss_single(t, y):\n",
        "    Loss = Var(0.0)\n",
        "    for i in range(len(t)): # sum over outputs\n",
        "      Loss += (t[i]-y[i]) ** 2\n",
        "    return Loss\n",
        "\n",
        "  Loss = Var(0.0)\n",
        "  for n in range(len(t)): # sum over training data\n",
        "    Loss += squared_loss_single(t[n],y[n])\n",
        "  return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu"
      },
      "source": [
        "## Exercise k) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
        "$$\n",
        "with $p$ given by the the softmax function in terms of the logits $h$:\n",
        "$$\n",
        "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
        "$$\n",
        "Inserting $p$ in the expression for the loss gives\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
        "$$\n",
        "This is true for $t$ being a one-hot vector. \n",
        "\n",
        "Call the function to convince yourself it works. \n",
        "\n",
        "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
        "\n",
        "Help: You can add these methods in the Var class:\n",
        "\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "    \n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer (k)**\n",
        "\n",
        "Below is the implemented cross_entropy_loss function. With the aforementioned hint, where an expression for log and exp has been implemented in the Var class, these are used for easier calculation."
      ],
      "metadata": {
        "id": "KfTG1cK8GAj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "6nMuxyfzFtHv"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss_single(t,h):\n",
        "  Loss_1 = Var(0.0)\n",
        "  Loss_2 = Var(0,0)\n",
        "  for i in range(len(t)):\n",
        "    Loss_1 += -t[i]*h[i]\n",
        "    Loss_2 += h[i].exp()\n",
        "  Loss = Loss_1+Loss_2.log()\n",
        "  return Loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy"
      },
      "source": [
        "# Backward pass\n",
        "\n",
        "Now the magic happens! We get the calculation of the gradients for free. Just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "iHyfPPI9Qqwu"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49biIAYKQ1oG"
      },
      "source": [
        "and the gradients will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "_rGt1bq_Q7uk",
        "outputId": "08baa986-e564-4d0f-8f27-74e7a20c1db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 \n",
            " Weights: [[Var(v=0.0450, grad=-13.6451), Var(v=0.0904, grad=-9.4624), Var(v=-0.0743, grad=-8.2226), Var(v=0.0144, grad=2.5502), Var(v=0.1141, grad=-7.0948)]] Biases: [Var(v=0.0000, grad=-11.7266), Var(v=0.0000, grad=-8.1320), Var(v=0.0000, grad=7.4748), Var(v=0.0000, grad=2.1916), Var(v=0.0000, grad=-6.0973)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=0.1377, grad=-4.4575)], [Var(v=0.0955, grad=-8.9583)], [Var(v=0.0851, grad=7.1842)], [Var(v=-0.0257, grad=-1.4310)], [Var(v=0.0716, grad=-11.3097)]] Biases: [Var(v=0.0000, grad=2.6806)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9"
      },
      "source": [
        "# Backward pass unit test\n",
        "\n",
        "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks!\n",
        "\n",
        "We initialize again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "01ePmzBzRtdh"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 15, lambda x: x.relu()),\n",
        "    DenseLayer(15, 50, lambda x: x.relu()),\n",
        "    DenseLayer(50, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iRPiQ1ISHw"
      },
      "source": [
        "and make an update:\n",
        "\n",
        "We introduce a help function parameters to have a handle in all parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "dhAI7eyeznia",
        "outputId": "f9e50e6f-2dd4-48ec-ad4c-62f5bfef87c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network before update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=0.1520, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=-0.0644, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=-0.0217, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=-0.0913, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.1138, grad=0.0000)], [Var(v=0.0110, grad=0.0000)], [Var(v=0.0123, grad=0.0000)], [Var(v=0.1022, grad=0.0000)], [Var(v=0.0137, grad=0.0000)], [Var(v=0.2756, grad=0.0000)], [Var(v=0.1313, grad=0.0000)], [Var(v=-0.1195, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n",
            "\n",
            "Network after update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=0.1520, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=-0.0644, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=-0.0217, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=-0.0913, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.1138, grad=0.0000)], [Var(v=0.0110, grad=0.0000)], [Var(v=0.0123, grad=0.0000)], [Var(v=0.1022, grad=0.0000)], [Var(v=0.0137, grad=0.0000)], [Var(v=0.2756, grad=0.0000)], [Var(v=0.1313, grad=0.0000)], [Var(v=-0.1195, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n",
            "\n",
            "Network after zeroing gradients:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=0.1520, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=-0.0644, grad=0.0000), Var(v=-0.0390, grad=0.0000), Var(v=-0.0217, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=-0.0913, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.1138, grad=0.0000)], [Var(v=0.0110, grad=0.0000)], [Var(v=0.0123, grad=0.0000)], [Var(v=0.1022, grad=0.0000)], [Var(v=0.0137, grad=0.0000)], [Var(v=0.2756, grad=0.0000)], [Var(v=0.1313, grad=0.0000)], [Var(v=-0.1195, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "print('Network before update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "def parameters(network):\n",
        "  params = []\n",
        "  for layer in range(len(network)):\n",
        "    params += network[layer].parameters()\n",
        "  return params\n",
        "\n",
        "def update_parameters(params, learning_rate=0.01):\n",
        "  for p in params:\n",
        "    p.v -= learning_rate*p.grad\n",
        "\n",
        "def zero_gradients(params):\n",
        "  for p in params:\n",
        "    p.grad = 0.0\n",
        "\n",
        "update_parameters(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "zero_gradients(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after zeroing gradients:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "woWYpdw6FtIO"
      },
      "outputs": [],
      "source": [
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "#NN = [\n",
        "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
        "#    DenseLayer(16, 1, lambda x: x.identity())\n",
        "#]\n",
        "\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "mdqaqYBVFtIR"
      },
      "outputs": [],
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "5kfg76GMFtIW",
        "scrolled": true,
        "outputId": "7c10858f-7e00-442d-8064-d1a48327cfe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 104.162 \t Validation loss: 102.591\n",
            "  10 ( 5.00%) Train loss: 21.224 \t Validation loss: 17.020\n",
            "  20 (10.00%) Train loss: 13.803 \t Validation loss: 10.882\n",
            "  30 (15.00%) Train loss: 13.294 \t Validation loss: 10.246\n",
            "  40 (20.00%) Train loss: 12.978 \t Validation loss: 9.821\n",
            "  50 (25.00%) Train loss: 12.719 \t Validation loss: 9.463\n",
            "  60 (30.00%) Train loss: 12.508 \t Validation loss: 9.207\n",
            "  70 (35.00%) Train loss: 12.364 \t Validation loss: 9.103\n",
            "  80 (40.00%) Train loss: 12.258 \t Validation loss: 9.030\n",
            "  90 (45.00%) Train loss: 12.183 \t Validation loss: 8.979\n",
            " 100 (50.00%) Train loss: 12.119 \t Validation loss: 8.945\n",
            " 110 (55.00%) Train loss: 12.061 \t Validation loss: 8.943\n",
            " 120 (60.00%) Train loss: 11.982 \t Validation loss: 8.940\n",
            " 130 (65.00%) Train loss: 11.944 \t Validation loss: 8.950\n",
            " 140 (70.00%) Train loss: 11.928 \t Validation loss: 8.952\n",
            " 150 (75.00%) Train loss: 11.913 \t Validation loss: 8.964\n",
            " 160 (80.00%) Train loss: 11.906 \t Validation loss: 8.973\n",
            " 170 (85.00%) Train loss: 11.902 \t Validation loss: 8.982\n",
            " 180 (90.00%) Train loss: 11.901 \t Validation loss: 8.986\n",
            " 190 (95.00%) Train loss: 11.897 \t Validation loss: 9.010\n"
          ]
        }
      ],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "VetyRWFwFtIY",
        "outputId": "5880626d-2c8f-4d84-880f-ad665343fc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "HmNi7S-vFtIc"
      },
      "outputs": [],
      "source": [
        "output_test = forward(x_test, NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "7mmJOTSEFtIf",
        "outputId": "bcb47ceb-372b-47fb-85ea-e633a76def19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  9.793\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "y_test_np = Var_to_nparray(y_test)\n",
        "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
        "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "ODi0WlmQFtIh",
        "outputId": "fde2a4df-914a-4b66-991c-5f7b130ca315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x_test_np = Var_to_nparray(x_test)\n",
        "x_train_np = Var_to_nparray(x_train)\n",
        "y_train_np = Var_to_nparray(y_train)\n",
        "if D1:\n",
        "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
        "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
        "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk"
      },
      "source": [
        "## Exercise l) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "tQZCn2dxFtIl",
        "outputId": "baf157db-2e85-4e5c-f131-eebc7fb481c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 104.085 \t Validation loss: 101.119\n",
            "   0 ( 0.00%) Train loss: 105.987 \t Validation loss: 106.651\n",
            "  10 ( 5.00%) Train loss: 52.570 \t Validation loss: 38.953\n",
            "  20 (10.00%) Train loss: 15.139 \t Validation loss: 11.252\n",
            "  30 (15.00%) Train loss: 14.492 \t Validation loss: 10.701\n",
            "  40 (20.00%) Train loss: 13.847 \t Validation loss: 10.370\n",
            "  50 (25.00%) Train loss: 12.973 \t Validation loss: 9.837\n",
            "  60 (30.00%) Train loss: 12.554 \t Validation loss: 9.478\n",
            "  70 (35.00%) Train loss: 12.335 \t Validation loss: 9.274\n",
            "  80 (40.00%) Train loss: 12.173 \t Validation loss: 9.180\n",
            "  90 (45.00%) Train loss: 12.066 \t Validation loss: 9.064\n",
            " 100 (50.00%) Train loss: 12.010 \t Validation loss: 9.006\n",
            " 110 (55.00%) Train loss: 11.973 \t Validation loss: 8.962\n",
            " 120 (60.00%) Train loss: 11.949 \t Validation loss: 8.940\n",
            " 130 (65.00%) Train loss: 11.933 \t Validation loss: 8.921\n",
            " 140 (70.00%) Train loss: 11.922 \t Validation loss: 8.915\n",
            " 150 (75.00%) Train loss: 11.911 \t Validation loss: 8.936\n",
            " 160 (80.00%) Train loss: 11.904 \t Validation loss: 8.951\n",
            " 170 (85.00%) Train loss: 11.900 \t Validation loss: 8.966\n",
            " 180 (90.00%) Train loss: 11.897 \t Validation loss: 8.988\n",
            " 190 (95.00%) Train loss: 11.896 \t Validation loss: 9.005\n",
            "   0 ( 0.00%) Train loss: 107.237 \t Validation loss: 102.165\n",
            "  10 ( 0.05%) Train loss: 16.827 \t Validation loss: 12.473\n",
            "  20 ( 0.10%) Train loss: 13.922 \t Validation loss: 10.471\n",
            "  30 ( 0.15%) Train loss: 13.469 \t Validation loss: 10.128\n",
            "  40 ( 0.20%) Train loss: 13.066 \t Validation loss: 9.821\n",
            "  50 ( 0.25%) Train loss: 12.722 \t Validation loss: 9.546\n",
            "  60 ( 0.30%) Train loss: 12.467 \t Validation loss: 9.332\n",
            "  70 ( 0.35%) Train loss: 12.235 \t Validation loss: 9.189\n",
            "  80 ( 0.40%) Train loss: 12.095 \t Validation loss: 9.047\n",
            "  90 ( 0.45%) Train loss: 12.012 \t Validation loss: 8.966\n",
            " 100 ( 0.50%) Train loss: 11.958 \t Validation loss: 8.910\n",
            " 110 ( 0.55%) Train loss: 11.924 \t Validation loss: 8.879\n",
            " 120 ( 0.60%) Train loss: 11.900 \t Validation loss: 8.870\n",
            " 130 ( 0.65%) Train loss: 11.880 \t Validation loss: 8.881\n",
            " 140 ( 0.70%) Train loss: 11.868 \t Validation loss: 8.895\n",
            " 150 ( 0.75%) Train loss: 11.860 \t Validation loss: 8.910\n",
            " 160 ( 0.80%) Train loss: 11.854 \t Validation loss: 8.922\n",
            " 170 ( 0.85%) Train loss: 11.849 \t Validation loss: 8.931\n",
            " 180 ( 0.90%) Train loss: 11.845 \t Validation loss: 8.937\n",
            " 190 ( 0.95%) Train loss: 11.840 \t Validation loss: 8.937\n",
            " 200 ( 1.00%) Train loss: 11.835 \t Validation loss: 8.934\n",
            " 210 ( 1.05%) Train loss: 11.831 \t Validation loss: 8.931\n",
            " 220 ( 1.10%) Train loss: 11.826 \t Validation loss: 8.925\n",
            " 230 ( 1.15%) Train loss: 11.822 \t Validation loss: 8.919\n",
            " 240 ( 1.20%) Train loss: 11.814 \t Validation loss: 8.908\n",
            " 250 ( 1.25%) Train loss: 11.800 \t Validation loss: 8.887\n",
            " 260 ( 1.30%) Train loss: 11.768 \t Validation loss: 8.857\n",
            " 270 ( 1.35%) Train loss: 11.731 \t Validation loss: 8.824\n",
            " 280 ( 1.40%) Train loss: 11.695 \t Validation loss: 8.795\n",
            " 290 ( 1.45%) Train loss: 11.666 \t Validation loss: 8.776\n",
            " 300 ( 1.50%) Train loss: 11.649 \t Validation loss: 8.765\n",
            " 310 ( 1.55%) Train loss: 11.635 \t Validation loss: 8.756\n",
            " 320 ( 1.60%) Train loss: 11.621 \t Validation loss: 8.745\n",
            " 330 ( 1.65%) Train loss: 11.606 \t Validation loss: 8.732\n",
            " 340 ( 1.70%) Train loss: 11.591 \t Validation loss: 8.717\n",
            " 350 ( 1.75%) Train loss: 11.575 \t Validation loss: 8.700\n",
            " 360 ( 1.80%) Train loss: 11.559 \t Validation loss: 8.683\n",
            " 370 ( 1.85%) Train loss: 11.542 \t Validation loss: 8.664\n",
            " 380 ( 1.90%) Train loss: 11.524 \t Validation loss: 8.644\n",
            " 390 ( 1.95%) Train loss: 11.506 \t Validation loss: 8.624\n",
            " 400 ( 2.00%) Train loss: 11.487 \t Validation loss: 8.604\n",
            " 410 ( 2.05%) Train loss: 11.468 \t Validation loss: 8.582\n",
            " 420 ( 2.10%) Train loss: 11.448 \t Validation loss: 8.560\n",
            " 430 ( 2.15%) Train loss: 11.427 \t Validation loss: 8.538\n",
            " 440 ( 2.20%) Train loss: 11.407 \t Validation loss: 8.515\n",
            " 450 ( 2.25%) Train loss: 11.385 \t Validation loss: 8.492\n",
            " 460 ( 2.30%) Train loss: 11.364 \t Validation loss: 8.469\n",
            " 470 ( 2.35%) Train loss: 11.341 \t Validation loss: 8.444\n",
            " 480 ( 2.40%) Train loss: 11.318 \t Validation loss: 8.413\n",
            " 490 ( 2.45%) Train loss: 11.292 \t Validation loss: 8.379\n",
            " 500 ( 2.50%) Train loss: 11.267 \t Validation loss: 8.348\n",
            " 510 ( 2.55%) Train loss: 11.241 \t Validation loss: 8.319\n",
            " 520 ( 2.60%) Train loss: 11.215 \t Validation loss: 8.292\n",
            " 530 ( 2.65%) Train loss: 11.189 \t Validation loss: 8.268\n",
            " 540 ( 2.70%) Train loss: 11.165 \t Validation loss: 8.246\n",
            " 550 ( 2.75%) Train loss: 11.140 \t Validation loss: 8.224\n",
            " 560 ( 2.80%) Train loss: 11.116 \t Validation loss: 8.203\n",
            " 570 ( 2.85%) Train loss: 11.093 \t Validation loss: 8.183\n",
            " 580 ( 2.90%) Train loss: 11.070 \t Validation loss: 8.162\n",
            " 590 ( 2.95%) Train loss: 11.047 \t Validation loss: 8.142\n",
            " 600 ( 3.00%) Train loss: 11.025 \t Validation loss: 8.121\n",
            " 610 ( 3.05%) Train loss: 11.003 \t Validation loss: 8.102\n",
            " 620 ( 3.10%) Train loss: 10.981 \t Validation loss: 8.083\n",
            " 630 ( 3.15%) Train loss: 10.959 \t Validation loss: 8.065\n",
            " 640 ( 3.20%) Train loss: 10.938 \t Validation loss: 8.048\n",
            " 650 ( 3.25%) Train loss: 10.918 \t Validation loss: 8.032\n",
            " 660 ( 3.30%) Train loss: 10.898 \t Validation loss: 8.017\n",
            " 670 ( 3.35%) Train loss: 10.878 \t Validation loss: 8.002\n",
            " 680 ( 3.40%) Train loss: 10.859 \t Validation loss: 7.990\n",
            " 690 ( 3.45%) Train loss: 10.841 \t Validation loss: 7.983\n",
            " 700 ( 3.50%) Train loss: 10.823 \t Validation loss: 7.979\n",
            " 710 ( 3.55%) Train loss: 10.807 \t Validation loss: 7.969\n",
            " 720 ( 3.60%) Train loss: 10.790 \t Validation loss: 7.963\n",
            " 730 ( 3.65%) Train loss: 10.774 \t Validation loss: 7.955\n",
            " 740 ( 3.70%) Train loss: 10.759 \t Validation loss: 7.957\n",
            " 750 ( 3.75%) Train loss: 10.744 \t Validation loss: 7.946\n",
            " 760 ( 3.80%) Train loss: 10.730 \t Validation loss: 7.948\n",
            " 770 ( 3.85%) Train loss: 10.716 \t Validation loss: 7.940\n",
            " 780 ( 3.90%) Train loss: 10.703 \t Validation loss: 7.937\n",
            " 790 ( 3.95%) Train loss: 10.689 \t Validation loss: 7.936\n",
            " 800 ( 4.00%) Train loss: 10.676 \t Validation loss: 7.936\n",
            " 810 ( 4.05%) Train loss: 10.665 \t Validation loss: 7.933\n",
            " 820 ( 4.10%) Train loss: 10.654 \t Validation loss: 7.929\n",
            " 830 ( 4.15%) Train loss: 10.644 \t Validation loss: 7.928\n",
            " 840 ( 4.20%) Train loss: 10.633 \t Validation loss: 7.927\n",
            " 850 ( 4.25%) Train loss: 10.623 \t Validation loss: 7.924\n",
            " 860 ( 4.30%) Train loss: 10.613 \t Validation loss: 7.926\n",
            " 870 ( 4.35%) Train loss: 10.603 \t Validation loss: 7.925\n",
            " 880 ( 4.40%) Train loss: 10.594 \t Validation loss: 7.927\n",
            " 890 ( 4.45%) Train loss: 10.587 \t Validation loss: 7.923\n",
            " 900 ( 4.50%) Train loss: 10.581 \t Validation loss: 7.916\n",
            " 910 ( 4.55%) Train loss: 10.575 \t Validation loss: 7.910\n",
            " 920 ( 4.60%) Train loss: 10.570 \t Validation loss: 7.909\n",
            " 930 ( 4.65%) Train loss: 10.566 \t Validation loss: 7.909\n",
            " 940 ( 4.70%) Train loss: 10.561 \t Validation loss: 7.904\n",
            " 950 ( 4.75%) Train loss: 10.557 \t Validation loss: 7.907\n",
            " 960 ( 4.80%) Train loss: 10.552 \t Validation loss: 7.907\n",
            " 970 ( 4.85%) Train loss: 10.549 \t Validation loss: 7.904\n",
            " 980 ( 4.90%) Train loss: 10.545 \t Validation loss: 7.904\n",
            " 990 ( 4.95%) Train loss: 10.541 \t Validation loss: 7.899\n",
            "1000 ( 5.00%) Train loss: 10.538 \t Validation loss: 7.907\n",
            "1010 ( 5.05%) Train loss: 10.532 \t Validation loss: 7.903\n",
            "1020 ( 5.10%) Train loss: 10.523 \t Validation loss: 7.914\n",
            "1030 ( 5.15%) Train loss: 10.514 \t Validation loss: 7.928\n",
            "1040 ( 5.20%) Train loss: 10.507 \t Validation loss: 7.928\n",
            "1050 ( 5.25%) Train loss: 10.500 \t Validation loss: 7.933\n",
            "1060 ( 5.30%) Train loss: 10.493 \t Validation loss: 7.941\n",
            "1070 ( 5.35%) Train loss: 10.486 \t Validation loss: 7.947\n",
            "1080 ( 5.40%) Train loss: 10.479 \t Validation loss: 7.953\n",
            "1090 ( 5.45%) Train loss: 10.474 \t Validation loss: 7.952\n",
            "1100 ( 5.50%) Train loss: 10.468 \t Validation loss: 7.953\n",
            "1110 ( 5.55%) Train loss: 10.463 \t Validation loss: 7.957\n",
            "1120 ( 5.60%) Train loss: 10.460 \t Validation loss: 7.948\n",
            "1130 ( 5.65%) Train loss: 10.457 \t Validation loss: 7.946\n",
            "1140 ( 5.70%) Train loss: 10.454 \t Validation loss: 7.942\n",
            "1150 ( 5.75%) Train loss: 10.451 \t Validation loss: 7.941\n",
            "1160 ( 5.80%) Train loss: 10.448 \t Validation loss: 7.940\n",
            "1170 ( 5.85%) Train loss: 10.446 \t Validation loss: 7.936\n",
            "1180 ( 5.90%) Train loss: 10.445 \t Validation loss: 7.932\n",
            "1190 ( 5.95%) Train loss: 10.443 \t Validation loss: 7.931\n",
            "1200 ( 6.00%) Train loss: 10.441 \t Validation loss: 7.930\n",
            "1210 ( 6.05%) Train loss: 10.440 \t Validation loss: 7.927\n",
            "1220 ( 6.10%) Train loss: 10.438 \t Validation loss: 7.924\n",
            "1230 ( 6.15%) Train loss: 10.437 \t Validation loss: 7.923\n",
            "1240 ( 6.20%) Train loss: 10.436 \t Validation loss: 7.924\n",
            "1250 ( 6.25%) Train loss: 10.435 \t Validation loss: 7.921\n",
            "1260 ( 6.30%) Train loss: 10.434 \t Validation loss: 7.921\n",
            "1270 ( 6.35%) Train loss: 10.434 \t Validation loss: 7.920\n",
            "1280 ( 6.40%) Train loss: 10.433 \t Validation loss: 7.920\n",
            "1290 ( 6.45%) Train loss: 10.432 \t Validation loss: 7.920\n",
            "1300 ( 6.50%) Train loss: 10.432 \t Validation loss: 7.919\n",
            "1310 ( 6.55%) Train loss: 10.431 \t Validation loss: 7.918\n",
            "1320 ( 6.60%) Train loss: 10.431 \t Validation loss: 7.917\n",
            "1330 ( 6.65%) Train loss: 10.430 \t Validation loss: 7.916\n",
            "1340 ( 6.70%) Train loss: 10.430 \t Validation loss: 7.914\n",
            "1350 ( 6.75%) Train loss: 10.429 \t Validation loss: 7.916\n",
            "1360 ( 6.80%) Train loss: 10.429 \t Validation loss: 7.915\n",
            "1370 ( 6.85%) Train loss: 10.429 \t Validation loss: 7.913\n",
            "1380 ( 6.90%) Train loss: 10.428 \t Validation loss: 7.912\n",
            "1390 ( 6.95%) Train loss: 10.428 \t Validation loss: 7.913\n",
            "1400 ( 7.00%) Train loss: 10.428 \t Validation loss: 7.911\n",
            "1410 ( 7.05%) Train loss: 10.428 \t Validation loss: 7.913\n",
            "1420 ( 7.10%) Train loss: 10.428 \t Validation loss: 7.911\n",
            "1430 ( 7.15%) Train loss: 10.428 \t Validation loss: 7.913\n",
            "1440 ( 7.20%) Train loss: 10.427 \t Validation loss: 7.912\n",
            "1450 ( 7.25%) Train loss: 10.427 \t Validation loss: 7.914\n",
            "1460 ( 7.30%) Train loss: 10.427 \t Validation loss: 7.912\n",
            "1470 ( 7.35%) Train loss: 10.427 \t Validation loss: 7.914\n",
            "1480 ( 7.40%) Train loss: 10.427 \t Validation loss: 7.916\n",
            "1490 ( 7.45%) Train loss: 10.427 \t Validation loss: 7.914\n",
            "1500 ( 7.50%) Train loss: 10.427 \t Validation loss: 7.916\n",
            "1510 ( 7.55%) Train loss: 10.426 \t Validation loss: 7.914\n",
            "1520 ( 7.60%) Train loss: 10.426 \t Validation loss: 7.915\n",
            "1530 ( 7.65%) Train loss: 10.426 \t Validation loss: 7.917\n",
            "1540 ( 7.70%) Train loss: 10.426 \t Validation loss: 7.915\n",
            "1550 ( 7.75%) Train loss: 10.426 \t Validation loss: 7.916\n",
            "1560 ( 7.80%) Train loss: 10.426 \t Validation loss: 7.918\n",
            "1570 ( 7.85%) Train loss: 10.426 \t Validation loss: 7.917\n",
            "1580 ( 7.90%) Train loss: 10.426 \t Validation loss: 7.918\n",
            "1590 ( 7.95%) Train loss: 10.426 \t Validation loss: 7.919\n",
            "1600 ( 8.00%) Train loss: 10.425 \t Validation loss: 7.921\n",
            "1610 ( 8.05%) Train loss: 10.425 \t Validation loss: 7.918\n",
            "1620 ( 8.10%) Train loss: 10.425 \t Validation loss: 7.920\n",
            "1630 ( 8.15%) Train loss: 10.425 \t Validation loss: 7.921\n",
            "1640 ( 8.20%) Train loss: 10.425 \t Validation loss: 7.919\n",
            "1650 ( 8.25%) Train loss: 10.425 \t Validation loss: 7.920\n",
            "1660 ( 8.30%) Train loss: 10.425 \t Validation loss: 7.922\n",
            "1670 ( 8.35%) Train loss: 10.425 \t Validation loss: 7.923\n",
            "1680 ( 8.40%) Train loss: 10.425 \t Validation loss: 7.921\n",
            "1690 ( 8.45%) Train loss: 10.425 \t Validation loss: 7.922\n",
            "1700 ( 8.50%) Train loss: 10.424 \t Validation loss: 7.923\n",
            "1710 ( 8.55%) Train loss: 10.424 \t Validation loss: 7.924\n",
            "1720 ( 8.60%) Train loss: 10.424 \t Validation loss: 7.922\n",
            "1730 ( 8.65%) Train loss: 10.424 \t Validation loss: 7.923\n",
            "1740 ( 8.70%) Train loss: 10.424 \t Validation loss: 7.924\n",
            "1750 ( 8.75%) Train loss: 10.424 \t Validation loss: 7.925\n",
            "1760 ( 8.80%) Train loss: 10.424 \t Validation loss: 7.926\n",
            "1770 ( 8.85%) Train loss: 10.424 \t Validation loss: 7.924\n",
            "1780 ( 8.90%) Train loss: 10.424 \t Validation loss: 7.924\n",
            "1790 ( 8.95%) Train loss: 10.424 \t Validation loss: 7.926\n",
            "1800 ( 9.00%) Train loss: 10.424 \t Validation loss: 7.926\n",
            "1810 ( 9.05%) Train loss: 10.424 \t Validation loss: 7.927\n",
            "1820 ( 9.10%) Train loss: 10.423 \t Validation loss: 7.925\n",
            "1830 ( 9.15%) Train loss: 10.424 \t Validation loss: 7.926\n",
            "1840 ( 9.20%) Train loss: 10.424 \t Validation loss: 7.927\n",
            "1850 ( 9.25%) Train loss: 10.423 \t Validation loss: 7.928\n",
            "1860 ( 9.30%) Train loss: 10.423 \t Validation loss: 7.928\n",
            "1870 ( 9.35%) Train loss: 10.423 \t Validation loss: 7.929\n",
            "1880 ( 9.40%) Train loss: 10.423 \t Validation loss: 7.927\n",
            "1890 ( 9.45%) Train loss: 10.423 \t Validation loss: 7.928\n",
            "1900 ( 9.50%) Train loss: 10.423 \t Validation loss: 7.928\n",
            "1910 ( 9.55%) Train loss: 10.423 \t Validation loss: 7.930\n",
            "1920 ( 9.60%) Train loss: 10.423 \t Validation loss: 7.930\n",
            "1930 ( 9.65%) Train loss: 10.423 \t Validation loss: 7.931\n",
            "1940 ( 9.70%) Train loss: 10.423 \t Validation loss: 7.932\n",
            "1950 ( 9.75%) Train loss: 10.423 \t Validation loss: 7.933\n",
            "1960 ( 9.80%) Train loss: 10.423 \t Validation loss: 7.930\n",
            "1970 ( 9.85%) Train loss: 10.423 \t Validation loss: 7.931\n",
            "1980 ( 9.90%) Train loss: 10.423 \t Validation loss: 7.932\n",
            "1990 ( 9.95%) Train loss: 10.423 \t Validation loss: 7.932\n",
            "2000 (10.00%) Train loss: 10.423 \t Validation loss: 7.934\n",
            "2010 (10.05%) Train loss: 10.423 \t Validation loss: 7.934\n",
            "2020 (10.10%) Train loss: 10.423 \t Validation loss: 7.935\n",
            "2030 (10.15%) Train loss: 10.422 \t Validation loss: 7.935\n",
            "2040 (10.20%) Train loss: 10.422 \t Validation loss: 7.936\n",
            "2050 (10.25%) Train loss: 10.422 \t Validation loss: 7.937\n",
            "2060 (10.30%) Train loss: 10.422 \t Validation loss: 7.937\n",
            "2070 (10.35%) Train loss: 10.422 \t Validation loss: 7.935\n",
            "2080 (10.40%) Train loss: 10.422 \t Validation loss: 7.936\n",
            "2090 (10.45%) Train loss: 10.422 \t Validation loss: 7.936\n",
            "2100 (10.50%) Train loss: 10.422 \t Validation loss: 7.937\n",
            "2110 (10.55%) Train loss: 10.422 \t Validation loss: 7.937\n",
            "2120 (10.60%) Train loss: 10.422 \t Validation loss: 7.938\n",
            "2130 (10.65%) Train loss: 10.422 \t Validation loss: 7.939\n",
            "2140 (10.70%) Train loss: 10.422 \t Validation loss: 7.939\n",
            "2150 (10.75%) Train loss: 10.422 \t Validation loss: 7.940\n",
            "2160 (10.80%) Train loss: 10.422 \t Validation loss: 7.940\n",
            "2170 (10.85%) Train loss: 10.422 \t Validation loss: 7.941\n",
            "2180 (10.90%) Train loss: 10.422 \t Validation loss: 7.941\n",
            "2190 (10.95%) Train loss: 10.422 \t Validation loss: 7.941\n",
            "2200 (11.00%) Train loss: 10.422 \t Validation loss: 7.942\n",
            "2210 (11.05%) Train loss: 10.422 \t Validation loss: 7.942\n",
            "2220 (11.10%) Train loss: 10.422 \t Validation loss: 7.943\n",
            "2230 (11.15%) Train loss: 10.422 \t Validation loss: 7.943\n",
            "2240 (11.20%) Train loss: 10.422 \t Validation loss: 7.943\n",
            "2250 (11.25%) Train loss: 10.422 \t Validation loss: 7.944\n",
            "2260 (11.30%) Train loss: 10.422 \t Validation loss: 7.944\n",
            "2270 (11.35%) Train loss: 10.422 \t Validation loss: 7.944\n",
            "2280 (11.40%) Train loss: 10.422 \t Validation loss: 7.944\n",
            "2290 (11.45%) Train loss: 10.422 \t Validation loss: 7.945\n",
            "2300 (11.50%) Train loss: 10.422 \t Validation loss: 7.945\n",
            "2310 (11.55%) Train loss: 10.422 \t Validation loss: 7.945\n",
            "2320 (11.60%) Train loss: 10.422 \t Validation loss: 7.945\n",
            "2330 (11.65%) Train loss: 10.422 \t Validation loss: 7.945\n",
            "2340 (11.70%) Train loss: 10.422 \t Validation loss: 7.946\n",
            "2350 (11.75%) Train loss: 10.422 \t Validation loss: 7.946\n",
            "2360 (11.80%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2370 (11.85%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2380 (11.90%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2390 (11.95%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2400 (12.00%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2410 (12.05%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2420 (12.10%) Train loss: 10.421 \t Validation loss: 7.947\n",
            "2430 (12.15%) Train loss: 10.421 \t Validation loss: 7.947\n",
            "2440 (12.20%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2450 (12.25%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2460 (12.30%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2470 (12.35%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2480 (12.40%) Train loss: 10.421 \t Validation loss: 7.947\n",
            "2490 (12.45%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2500 (12.50%) Train loss: 10.421 \t Validation loss: 7.946\n",
            "2510 (12.55%) Train loss: 10.421 \t Validation loss: 7.947\n",
            "2520 (12.60%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2530 (12.65%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2540 (12.70%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2550 (12.75%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2560 (12.80%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2570 (12.85%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2580 (12.90%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2590 (12.95%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2600 (13.00%) Train loss: 10.421 \t Validation loss: 7.948\n",
            "2610 (13.05%) Train loss: 10.421 \t Validation loss: 7.948\n",
            "2620 (13.10%) Train loss: 10.421 \t Validation loss: 7.949\n",
            "2630 (13.15%) Train loss: 10.421 \t Validation loss: 7.948\n",
            "2640 (13.20%) Train loss: 10.421 \t Validation loss: 7.948\n",
            "2650 (13.25%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2660 (13.30%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2670 (13.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2680 (13.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2690 (13.45%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "2700 (13.50%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "2710 (13.55%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "2720 (13.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "2730 (13.65%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "2740 (13.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2750 (13.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2760 (13.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2770 (13.85%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2780 (13.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2790 (13.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2800 (14.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2810 (14.05%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2820 (14.10%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2830 (14.15%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "2840 (14.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2850 (14.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2860 (14.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2870 (14.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2880 (14.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2890 (14.45%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "2900 (14.50%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "2910 (14.55%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "2920 (14.60%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "2930 (14.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "2940 (14.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2950 (14.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2960 (14.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2970 (14.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "2980 (14.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "2990 (14.95%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3000 (15.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3010 (15.05%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3020 (15.10%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3030 (15.15%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "3040 (15.20%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3050 (15.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3060 (15.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3070 (15.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3080 (15.40%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3090 (15.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3100 (15.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3110 (15.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3120 (15.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3130 (15.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3140 (15.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3150 (15.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3160 (15.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3170 (15.85%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "3180 (15.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3190 (15.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3200 (16.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3210 (16.05%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3220 (16.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3230 (16.15%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3240 (16.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3250 (16.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3260 (16.30%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3270 (16.35%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3280 (16.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3290 (16.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3300 (16.50%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3310 (16.55%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3320 (16.60%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3330 (16.65%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3340 (16.70%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3350 (16.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3360 (16.80%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3370 (16.85%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3380 (16.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3390 (16.95%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "3400 (17.00%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3410 (17.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3420 (17.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3430 (17.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3440 (17.20%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3450 (17.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3460 (17.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3470 (17.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3480 (17.40%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3490 (17.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3500 (17.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3510 (17.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3520 (17.60%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3530 (17.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "3540 (17.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3550 (17.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3560 (17.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3570 (17.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3580 (17.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3590 (17.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3600 (18.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3610 (18.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "3620 (18.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3630 (18.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3640 (18.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3650 (18.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3660 (18.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3670 (18.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3680 (18.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3690 (18.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3700 (18.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3710 (18.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3720 (18.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3730 (18.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "3740 (18.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3750 (18.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3760 (18.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3770 (18.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "3780 (18.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3790 (18.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3800 (19.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3810 (19.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3820 (19.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3830 (19.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3840 (19.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "3850 (19.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3860 (19.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3870 (19.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3880 (19.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3890 (19.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3900 (19.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3910 (19.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3920 (19.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3930 (19.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3940 (19.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3950 (19.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "3960 (19.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "3970 (19.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3980 (19.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "3990 (19.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4000 (20.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4010 (20.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4020 (20.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4030 (20.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4040 (20.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "4050 (20.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4060 (20.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4070 (20.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4080 (20.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4090 (20.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4100 (20.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4110 (20.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4120 (20.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4130 (20.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4140 (20.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4150 (20.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4160 (20.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4170 (20.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4180 (20.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4190 (20.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4200 (21.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4210 (21.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4220 (21.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4230 (21.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4240 (21.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4250 (21.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4260 (21.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4270 (21.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4280 (21.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4290 (21.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4300 (21.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4310 (21.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4320 (21.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "4330 (21.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4340 (21.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4350 (21.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4360 (21.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4370 (21.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4380 (21.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4390 (21.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4400 (22.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4410 (22.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4420 (22.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4430 (22.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4440 (22.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4450 (22.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4460 (22.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4470 (22.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4480 (22.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4490 (22.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4500 (22.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4510 (22.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4520 (22.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "4530 (22.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4540 (22.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4550 (22.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4560 (22.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4570 (22.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4580 (22.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4590 (22.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4600 (23.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "4610 (23.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4620 (23.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4630 (23.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4640 (23.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4650 (23.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4660 (23.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4670 (23.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4680 (23.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4690 (23.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4700 (23.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4710 (23.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4720 (23.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4730 (23.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4740 (23.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4750 (23.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4760 (23.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4770 (23.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4780 (23.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4790 (23.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4800 (24.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4810 (24.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4820 (24.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4830 (24.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4840 (24.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4850 (24.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4860 (24.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4870 (24.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4880 (24.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "4890 (24.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4900 (24.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4910 (24.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4920 (24.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4930 (24.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "4940 (24.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4950 (24.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "4960 (24.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4970 (24.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "4980 (24.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "4990 (24.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5000 (25.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5010 (25.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5020 (25.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5030 (25.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5040 (25.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5050 (25.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5060 (25.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5070 (25.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5080 (25.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5090 (25.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5100 (25.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5110 (25.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5120 (25.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5130 (25.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5140 (25.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5150 (25.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5160 (25.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "5170 (25.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "5180 (25.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5190 (25.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5200 (26.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5210 (26.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5220 (26.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5230 (26.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5240 (26.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "5250 (26.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5260 (26.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5270 (26.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5280 (26.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5290 (26.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5300 (26.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5310 (26.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5320 (26.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5330 (26.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5340 (26.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5350 (26.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5360 (26.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5370 (26.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5380 (26.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5390 (26.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5400 (27.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5410 (27.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5420 (27.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5430 (27.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5440 (27.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "5450 (27.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "5460 (27.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5470 (27.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5480 (27.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5490 (27.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5500 (27.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5510 (27.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5520 (27.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "5530 (27.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5540 (27.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5550 (27.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5560 (27.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5570 (27.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5580 (27.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5590 (27.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5600 (28.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5610 (28.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "5620 (28.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5630 (28.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5640 (28.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5650 (28.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "5660 (28.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5670 (28.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5680 (28.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5690 (28.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5700 (28.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5710 (28.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5720 (28.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5730 (28.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5740 (28.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5750 (28.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5760 (28.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5770 (28.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5780 (28.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5790 (28.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5800 (29.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "5810 (29.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5820 (29.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5830 (29.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5840 (29.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5850 (29.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5860 (29.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5870 (29.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5880 (29.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5890 (29.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "5900 (29.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5910 (29.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5920 (29.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5930 (29.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5940 (29.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5950 (29.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5960 (29.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "5970 (29.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "5980 (29.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "5990 (29.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6000 (30.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6010 (30.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6020 (30.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6030 (30.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6040 (30.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6050 (30.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6060 (30.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6070 (30.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6080 (30.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "6090 (30.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "6100 (30.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6110 (30.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6120 (30.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6130 (30.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6140 (30.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6150 (30.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6160 (30.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "6170 (30.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6180 (30.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6190 (30.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6200 (31.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6210 (31.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6220 (31.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6230 (31.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6240 (31.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6250 (31.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6260 (31.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6270 (31.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6280 (31.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6290 (31.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6300 (31.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6310 (31.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6320 (31.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6330 (31.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6340 (31.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6350 (31.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6360 (31.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "6370 (31.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "6380 (31.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6390 (31.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6400 (32.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6410 (32.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6420 (32.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6430 (32.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6440 (32.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6450 (32.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6460 (32.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6470 (32.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6480 (32.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6490 (32.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6500 (32.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6510 (32.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6520 (32.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6530 (32.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6540 (32.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6550 (32.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6560 (32.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6570 (32.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "6580 (32.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6590 (32.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6600 (33.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6610 (33.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6620 (33.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6630 (33.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6640 (33.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "6650 (33.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6660 (33.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6670 (33.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6680 (33.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6690 (33.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6700 (33.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6710 (33.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6720 (33.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "6730 (33.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6740 (33.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6750 (33.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6760 (33.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6770 (33.85%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6780 (33.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6790 (33.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6800 (34.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6810 (34.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "6820 (34.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6830 (34.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6840 (34.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6850 (34.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6860 (34.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6870 (34.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6880 (34.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6890 (34.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6900 (34.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6910 (34.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6920 (34.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6930 (34.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6940 (34.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6950 (34.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6960 (34.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "6970 (34.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "6980 (34.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "6990 (34.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7000 (35.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "7010 (35.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7020 (35.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7030 (35.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7040 (35.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7050 (35.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7060 (35.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7070 (35.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7080 (35.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7090 (35.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7100 (35.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7110 (35.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7120 (35.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7130 (35.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7140 (35.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7150 (35.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7160 (35.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7170 (35.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7180 (35.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7190 (35.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7200 (36.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7210 (36.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7220 (36.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7230 (36.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7240 (36.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7250 (36.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7260 (36.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7270 (36.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7280 (36.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "7290 (36.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7300 (36.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7310 (36.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7320 (36.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7330 (36.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7340 (36.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7350 (36.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7360 (36.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7370 (36.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7380 (36.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7390 (36.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7400 (37.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7410 (37.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7420 (37.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7430 (37.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7440 (37.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7450 (37.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7460 (37.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7470 (37.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7480 (37.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7490 (37.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7500 (37.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7510 (37.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7520 (37.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7530 (37.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7540 (37.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7550 (37.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7560 (37.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7570 (37.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7580 (37.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7590 (37.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7600 (38.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7610 (38.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7620 (38.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7630 (38.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7640 (38.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "7650 (38.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7660 (38.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7670 (38.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7680 (38.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7690 (38.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7700 (38.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7710 (38.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7720 (38.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7730 (38.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7740 (38.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7750 (38.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7760 (38.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7770 (38.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7780 (38.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7790 (38.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7800 (39.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7810 (39.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7820 (39.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7830 (39.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7840 (39.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7850 (39.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7860 (39.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7870 (39.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7880 (39.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "7890 (39.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7900 (39.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7910 (39.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7920 (39.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "7930 (39.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "7940 (39.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7950 (39.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7960 (39.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7970 (39.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "7980 (39.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "7990 (39.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8000 (40.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "8010 (40.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8020 (40.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8030 (40.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8040 (40.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8050 (40.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8060 (40.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8070 (40.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8080 (40.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8090 (40.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8100 (40.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8110 (40.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8120 (40.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8130 (40.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "8140 (40.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8150 (40.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8160 (40.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8170 (40.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8180 (40.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8190 (40.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8200 (41.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "8210 (41.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8220 (41.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8230 (41.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8240 (41.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8250 (41.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8260 (41.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8270 (41.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8280 (41.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "8290 (41.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8300 (41.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8310 (41.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8320 (41.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8330 (41.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8340 (41.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8350 (41.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8360 (41.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8370 (41.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "8380 (41.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8390 (41.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8400 (42.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8410 (42.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "8420 (42.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8430 (42.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8440 (42.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8450 (42.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8460 (42.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8470 (42.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8480 (42.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8490 (42.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8500 (42.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8510 (42.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8520 (42.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8530 (42.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8540 (42.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8550 (42.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8560 (42.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "8570 (42.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "8580 (42.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8590 (42.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8600 (43.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8610 (43.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8620 (43.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8630 (43.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8640 (43.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8650 (43.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8660 (43.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8670 (43.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8680 (43.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8690 (43.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8700 (43.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8710 (43.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8720 (43.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8730 (43.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8740 (43.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8750 (43.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8760 (43.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8770 (43.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8780 (43.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8790 (43.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8800 (44.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8810 (44.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8820 (44.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8830 (44.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8840 (44.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "8850 (44.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "8860 (44.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8870 (44.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8880 (44.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8890 (44.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8900 (44.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8910 (44.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8920 (44.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8930 (44.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "8940 (44.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8950 (44.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8960 (44.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "8970 (44.85%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8980 (44.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "8990 (44.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9000 (45.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9010 (45.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9020 (45.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9030 (45.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9040 (45.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9050 (45.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9060 (45.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9070 (45.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9080 (45.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9090 (45.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9100 (45.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9110 (45.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9120 (45.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9130 (45.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9140 (45.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9150 (45.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9160 (45.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9170 (45.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9180 (45.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9190 (45.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9200 (46.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "9210 (46.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9220 (46.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9230 (46.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9240 (46.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9250 (46.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9260 (46.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9270 (46.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9280 (46.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9290 (46.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9300 (46.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9310 (46.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9320 (46.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9330 (46.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9340 (46.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9350 (46.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9360 (46.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9370 (46.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9380 (46.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9390 (46.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9400 (47.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9410 (47.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9420 (47.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9430 (47.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9440 (47.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9450 (47.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9460 (47.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9470 (47.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9480 (47.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "9490 (47.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9500 (47.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9510 (47.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9520 (47.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9530 (47.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9540 (47.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9550 (47.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9560 (47.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "9570 (47.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9580 (47.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9590 (47.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9600 (48.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9610 (48.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9620 (48.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9630 (48.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9640 (48.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9650 (48.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9660 (48.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9670 (48.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9680 (48.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9690 (48.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9700 (48.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9710 (48.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9720 (48.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9730 (48.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9740 (48.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9750 (48.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9760 (48.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "9770 (48.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9780 (48.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9790 (48.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9800 (49.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9810 (49.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9820 (49.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9830 (49.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9840 (49.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "9850 (49.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9860 (49.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9870 (49.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9880 (49.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9890 (49.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "9900 (49.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9910 (49.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9920 (49.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9930 (49.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9940 (49.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9950 (49.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9960 (49.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "9970 (49.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "9980 (49.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "9990 (49.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10000 (50.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10010 (50.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10020 (50.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10030 (50.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10040 (50.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10050 (50.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10060 (50.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10070 (50.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10080 (50.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10090 (50.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10100 (50.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10110 (50.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10120 (50.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "10130 (50.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10140 (50.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10150 (50.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10160 (50.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10170 (50.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10180 (50.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10190 (50.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10200 (51.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10210 (51.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10220 (51.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10230 (51.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10240 (51.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10250 (51.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10260 (51.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10270 (51.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10280 (51.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10290 (51.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10300 (51.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10310 (51.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10320 (51.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10330 (51.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10340 (51.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10350 (51.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10360 (51.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10370 (51.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10380 (51.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10390 (51.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10400 (52.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "10410 (52.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10420 (52.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10430 (52.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10440 (52.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10450 (52.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10460 (52.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10470 (52.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10480 (52.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10490 (52.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10500 (52.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10510 (52.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10520 (52.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10530 (52.65%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10540 (52.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10550 (52.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10560 (52.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10570 (52.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10580 (52.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10590 (52.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10600 (53.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10610 (53.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10620 (53.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10630 (53.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10640 (53.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10650 (53.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10660 (53.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10670 (53.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10680 (53.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10690 (53.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10700 (53.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10710 (53.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10720 (53.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10730 (53.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10740 (53.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10750 (53.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10760 (53.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "10770 (53.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10780 (53.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10790 (53.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10800 (54.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10810 (54.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10820 (54.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10830 (54.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10840 (54.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10850 (54.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10860 (54.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10870 (54.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10880 (54.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10890 (54.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "10900 (54.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10910 (54.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10920 (54.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10930 (54.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10940 (54.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10950 (54.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10960 (54.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "10970 (54.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "10980 (54.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "10990 (54.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11000 (55.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11010 (55.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11020 (55.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11030 (55.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11040 (55.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "11050 (55.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11060 (55.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11070 (55.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11080 (55.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11090 (55.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11100 (55.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11110 (55.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11120 (55.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11130 (55.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11140 (55.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11150 (55.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11160 (55.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11170 (55.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11180 (55.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11190 (55.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11200 (56.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11210 (56.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11220 (56.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11230 (56.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11240 (56.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11250 (56.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11260 (56.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11270 (56.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11280 (56.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11290 (56.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11300 (56.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11310 (56.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11320 (56.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "11330 (56.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11340 (56.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11350 (56.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11360 (56.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11370 (56.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11380 (56.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11390 (56.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11400 (57.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11410 (57.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11420 (57.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11430 (57.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11440 (57.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11450 (57.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11460 (57.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11470 (57.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11480 (57.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11490 (57.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11500 (57.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11510 (57.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11520 (57.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11530 (57.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11540 (57.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11550 (57.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11560 (57.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11570 (57.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11580 (57.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11590 (57.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11600 (58.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11610 (58.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11620 (58.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11630 (58.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11640 (58.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11650 (58.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11660 (58.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11670 (58.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11680 (58.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "11690 (58.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11700 (58.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11710 (58.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11720 (58.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11730 (58.65%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11740 (58.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11750 (58.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11760 (58.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11770 (58.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11780 (58.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11790 (58.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11800 (59.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11810 (59.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11820 (59.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11830 (59.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11840 (59.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11850 (59.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11860 (59.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11870 (59.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11880 (59.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11890 (59.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11900 (59.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11910 (59.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11920 (59.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "11930 (59.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "11940 (59.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11950 (59.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11960 (59.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "11970 (59.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "11980 (59.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "11990 (59.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12000 (60.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12010 (60.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12020 (60.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12030 (60.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12040 (60.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "12050 (60.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12060 (60.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12070 (60.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12080 (60.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12090 (60.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12100 (60.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12110 (60.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12120 (60.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12130 (60.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12140 (60.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12150 (60.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12160 (60.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12170 (60.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12180 (60.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12190 (60.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12200 (61.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12210 (61.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12220 (61.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12230 (61.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12240 (61.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "12250 (61.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "12260 (61.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12270 (61.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12280 (61.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12290 (61.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12300 (61.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12310 (61.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12320 (61.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12330 (61.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12340 (61.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12350 (61.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12360 (61.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12370 (61.85%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12380 (61.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12390 (61.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12400 (62.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12410 (62.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "12420 (62.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12430 (62.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12440 (62.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12450 (62.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "12460 (62.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12470 (62.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12480 (62.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12490 (62.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12500 (62.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12510 (62.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12520 (62.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12530 (62.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12540 (62.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12550 (62.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12560 (62.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12570 (62.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12580 (62.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12590 (62.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12600 (63.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "12610 (63.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12620 (63.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12630 (63.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12640 (63.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12650 (63.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12660 (63.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12670 (63.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12680 (63.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12690 (63.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "12700 (63.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12710 (63.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12720 (63.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12730 (63.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12740 (63.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12750 (63.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12760 (63.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12770 (63.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12780 (63.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12790 (63.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12800 (64.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12810 (64.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12820 (64.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12830 (64.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12840 (64.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12850 (64.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12860 (64.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12870 (64.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12880 (64.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "12890 (64.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "12900 (64.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12910 (64.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12920 (64.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12930 (64.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12940 (64.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12950 (64.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "12960 (64.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "12970 (64.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "12980 (64.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "12990 (64.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13000 (65.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13010 (65.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13020 (65.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13030 (65.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13040 (65.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13050 (65.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13060 (65.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13070 (65.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13080 (65.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13090 (65.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13100 (65.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13110 (65.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13120 (65.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13130 (65.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13140 (65.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13150 (65.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13160 (65.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "13170 (65.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13180 (65.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13190 (65.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13200 (66.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13210 (66.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13220 (66.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13230 (66.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13240 (66.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13250 (66.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13260 (66.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13270 (66.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13280 (66.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13290 (66.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13300 (66.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13310 (66.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13320 (66.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13330 (66.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "13340 (66.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13350 (66.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13360 (66.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13370 (66.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "13380 (66.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13390 (66.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13400 (67.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13410 (67.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13420 (67.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13430 (67.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13440 (67.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13450 (67.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13460 (67.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13470 (67.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13480 (67.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13490 (67.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13500 (67.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13510 (67.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13520 (67.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "13530 (67.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13540 (67.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13550 (67.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13560 (67.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13570 (67.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13580 (67.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13590 (67.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13600 (68.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13610 (68.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "13620 (68.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13630 (68.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13640 (68.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13650 (68.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13660 (68.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13670 (68.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13680 (68.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13690 (68.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13700 (68.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13710 (68.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13720 (68.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13730 (68.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13740 (68.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13750 (68.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13760 (68.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13770 (68.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13780 (68.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13790 (68.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13800 (69.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "13810 (69.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "13820 (69.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13830 (69.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13840 (69.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13850 (69.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13860 (69.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13870 (69.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13880 (69.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "13890 (69.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13900 (69.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13910 (69.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13920 (69.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13930 (69.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13940 (69.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13950 (69.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13960 (69.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "13970 (69.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "13980 (69.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "13990 (69.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14000 (70.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14010 (70.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14020 (70.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14030 (70.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14040 (70.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14050 (70.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14060 (70.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14070 (70.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14080 (70.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "14090 (70.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14100 (70.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14110 (70.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14120 (70.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14130 (70.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14140 (70.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14150 (70.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14160 (70.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14170 (70.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14180 (70.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14190 (70.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14200 (71.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14210 (71.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14220 (71.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14230 (71.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14240 (71.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14250 (71.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "14260 (71.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14270 (71.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14280 (71.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14290 (71.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "14300 (71.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14310 (71.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14320 (71.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14330 (71.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14340 (71.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14350 (71.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14360 (71.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14370 (71.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14380 (71.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14390 (71.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14400 (72.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14410 (72.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14420 (72.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14430 (72.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14440 (72.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "14450 (72.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14460 (72.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14470 (72.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14480 (72.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14490 (72.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14500 (72.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14510 (72.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14520 (72.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14530 (72.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "14540 (72.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14550 (72.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14560 (72.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14570 (72.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14580 (72.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14590 (72.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14600 (73.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14610 (73.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14620 (73.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14630 (73.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14640 (73.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14650 (73.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14660 (73.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14670 (73.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14680 (73.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14690 (73.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14700 (73.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14710 (73.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14720 (73.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "14730 (73.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "14740 (73.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14750 (73.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14760 (73.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14770 (73.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14780 (73.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14790 (73.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14800 (74.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "14810 (74.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14820 (74.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14830 (74.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14840 (74.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14850 (74.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14860 (74.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14870 (74.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14880 (74.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14890 (74.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14900 (74.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14910 (74.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14920 (74.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14930 (74.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14940 (74.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14950 (74.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "14960 (74.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "14970 (74.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14980 (74.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "14990 (74.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15000 (75.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "15010 (75.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15020 (75.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15030 (75.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15040 (75.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15050 (75.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15060 (75.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15070 (75.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15080 (75.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15090 (75.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15100 (75.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15110 (75.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15120 (75.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15130 (75.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15140 (75.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15150 (75.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15160 (75.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15170 (75.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "15180 (75.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15190 (75.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15200 (76.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15210 (76.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "15220 (76.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15230 (76.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15240 (76.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15250 (76.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15260 (76.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15270 (76.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15280 (76.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15290 (76.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15300 (76.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15310 (76.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15320 (76.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15330 (76.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15340 (76.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15350 (76.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15360 (76.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "15370 (76.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "15380 (76.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15390 (76.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15400 (77.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15410 (77.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15420 (77.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15430 (77.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15440 (77.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15450 (77.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "15460 (77.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15470 (77.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15480 (77.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15490 (77.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15500 (77.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15510 (77.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15520 (77.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15530 (77.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15540 (77.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15550 (77.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15560 (77.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15570 (77.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15580 (77.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15590 (77.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15600 (78.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15610 (78.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15620 (78.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15630 (78.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15640 (78.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "15650 (78.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "15660 (78.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15670 (78.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15680 (78.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15690 (78.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15700 (78.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15710 (78.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15720 (78.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "15730 (78.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15740 (78.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15750 (78.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15760 (78.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15770 (78.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15780 (78.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15790 (78.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15800 (79.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15810 (79.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15820 (79.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15830 (79.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15840 (79.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15850 (79.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15860 (79.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15870 (79.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15880 (79.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15890 (79.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15900 (79.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15910 (79.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15920 (79.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "15930 (79.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15940 (79.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15950 (79.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15960 (79.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "15970 (79.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "15980 (79.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "15990 (79.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16000 (80.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16010 (80.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16020 (80.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16030 (80.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16040 (80.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16050 (80.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16060 (80.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16070 (80.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16080 (80.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16090 (80.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "16100 (80.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16110 (80.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16120 (80.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16130 (80.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "16140 (80.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16150 (80.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16160 (80.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16170 (80.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16180 (80.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16190 (80.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16200 (81.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16210 (81.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16220 (81.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16230 (81.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16240 (81.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16250 (81.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16260 (81.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16270 (81.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16280 (81.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "16290 (81.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "16300 (81.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16310 (81.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16320 (81.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16330 (81.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16340 (81.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16350 (81.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16360 (81.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16370 (81.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16380 (81.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16390 (81.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16400 (82.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16410 (82.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16420 (82.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16430 (82.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16440 (82.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16450 (82.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16460 (82.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16470 (82.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16480 (82.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16490 (82.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16500 (82.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16510 (82.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16520 (82.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16530 (82.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16540 (82.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16550 (82.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16560 (82.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "16570 (82.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "16580 (82.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16590 (82.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16600 (83.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16610 (83.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16620 (83.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16630 (83.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16640 (83.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "16650 (83.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16660 (83.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16670 (83.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16680 (83.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16690 (83.45%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16700 (83.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16710 (83.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16720 (83.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16730 (83.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16740 (83.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16750 (83.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16760 (83.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16770 (83.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "16780 (83.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16790 (83.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16800 (84.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16810 (84.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16820 (84.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16830 (84.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16840 (84.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "16850 (84.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16860 (84.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16870 (84.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16880 (84.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16890 (84.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16900 (84.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16910 (84.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16920 (84.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "16930 (84.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16940 (84.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16950 (84.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16960 (84.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "16970 (84.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "16980 (84.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "16990 (84.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17000 (85.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17010 (85.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17020 (85.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17030 (85.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17040 (85.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17050 (85.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17060 (85.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17070 (85.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17080 (85.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17090 (85.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17100 (85.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17110 (85.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17120 (85.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17130 (85.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17140 (85.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17150 (85.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17160 (85.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17170 (85.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17180 (85.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17190 (85.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17200 (86.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "17210 (86.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17220 (86.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17230 (86.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17240 (86.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17250 (86.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17260 (86.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17270 (86.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17280 (86.40%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17290 (86.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17300 (86.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17310 (86.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17320 (86.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17330 (86.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17340 (86.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17350 (86.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17360 (86.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17370 (86.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17380 (86.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17390 (86.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17400 (87.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17410 (87.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17420 (87.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17430 (87.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17440 (87.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17450 (87.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17460 (87.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17470 (87.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17480 (87.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "17490 (87.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17500 (87.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17510 (87.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17520 (87.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17530 (87.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17540 (87.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17550 (87.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17560 (87.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17570 (87.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17580 (87.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17590 (87.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17600 (88.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17610 (88.05%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17620 (88.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17630 (88.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17640 (88.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17650 (88.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17660 (88.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17670 (88.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17680 (88.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17690 (88.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17700 (88.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17710 (88.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17720 (88.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17730 (88.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17740 (88.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17750 (88.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17760 (88.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "17770 (88.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17780 (88.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17790 (88.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17800 (89.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17810 (89.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17820 (89.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17830 (89.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17840 (89.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "17850 (89.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17860 (89.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17870 (89.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17880 (89.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17890 (89.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "17900 (89.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17910 (89.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17920 (89.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17930 (89.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17940 (89.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17950 (89.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17960 (89.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "17970 (89.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "17980 (89.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "17990 (89.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18000 (90.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18010 (90.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18020 (90.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18030 (90.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18040 (90.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18050 (90.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18060 (90.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18070 (90.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18080 (90.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18090 (90.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18100 (90.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18110 (90.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18120 (90.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "18130 (90.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "18140 (90.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18150 (90.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18160 (90.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18170 (90.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18180 (90.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18190 (90.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18200 (91.00%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18210 (91.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18220 (91.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18230 (91.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18240 (91.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18250 (91.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18260 (91.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18270 (91.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18280 (91.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18290 (91.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18300 (91.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18310 (91.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18320 (91.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18330 (91.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18340 (91.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18350 (91.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18360 (91.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18370 (91.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18380 (91.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18390 (91.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18400 (92.00%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "18410 (92.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "18420 (92.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18430 (92.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18440 (92.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18450 (92.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18460 (92.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18470 (92.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18480 (92.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18490 (92.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18500 (92.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18510 (92.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18520 (92.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18530 (92.65%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18540 (92.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18550 (92.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18560 (92.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18570 (92.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18580 (92.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18590 (92.95%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18600 (93.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18610 (93.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "18620 (93.10%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18630 (93.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18640 (93.20%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18650 (93.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18660 (93.30%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18670 (93.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18680 (93.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18690 (93.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18700 (93.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18710 (93.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18720 (93.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18730 (93.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18740 (93.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18750 (93.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18760 (93.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "18770 (93.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18780 (93.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18790 (93.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18800 (94.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18810 (94.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18820 (94.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18830 (94.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18840 (94.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18850 (94.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "18860 (94.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18870 (94.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18880 (94.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18890 (94.45%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "18900 (94.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18910 (94.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18920 (94.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18930 (94.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18940 (94.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18950 (94.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18960 (94.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "18970 (94.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "18980 (94.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "18990 (94.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19000 (95.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19010 (95.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19020 (95.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19030 (95.15%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19040 (95.20%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "19050 (95.25%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19060 (95.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19070 (95.35%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19080 (95.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19090 (95.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19100 (95.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19110 (95.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19120 (95.60%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19130 (95.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19140 (95.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19150 (95.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19160 (95.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19170 (95.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19180 (95.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19190 (95.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19200 (96.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19210 (96.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19220 (96.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19230 (96.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19240 (96.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19250 (96.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19260 (96.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19270 (96.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19280 (96.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19290 (96.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19300 (96.50%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19310 (96.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19320 (96.60%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "19330 (96.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19340 (96.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19350 (96.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19360 (96.80%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19370 (96.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19380 (96.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19390 (96.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19400 (97.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19410 (97.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19420 (97.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19430 (97.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19440 (97.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19450 (97.25%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19460 (97.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19470 (97.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19480 (97.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19490 (97.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19500 (97.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19510 (97.55%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19520 (97.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19530 (97.65%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19540 (97.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19550 (97.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19560 (97.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19570 (97.85%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19580 (97.90%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19590 (97.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19600 (98.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19610 (98.05%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19620 (98.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19630 (98.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19640 (98.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19650 (98.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19660 (98.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19670 (98.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19680 (98.40%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "19690 (98.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19700 (98.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19710 (98.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19720 (98.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19730 (98.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19740 (98.70%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19750 (98.75%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19760 (98.80%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19770 (98.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19780 (98.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19790 (98.95%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19800 (99.00%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19810 (99.05%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19820 (99.10%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19830 (99.15%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19840 (99.20%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19850 (99.25%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19860 (99.30%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19870 (99.35%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19880 (99.40%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19890 (99.45%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19900 (99.50%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19910 (99.55%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19920 (99.60%) Train loss: 10.421 \t Validation loss: 7.951\n",
            "19930 (99.65%) Train loss: 10.421 \t Validation loss: 7.953\n",
            "19940 (99.70%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19950 (99.75%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19960 (99.80%) Train loss: 10.421 \t Validation loss: 7.950\n",
            "19970 (99.85%) Train loss: 10.421 \t Validation loss: 7.954\n",
            "19980 (99.90%) Train loss: 10.421 \t Validation loss: 7.952\n",
            "19990 (99.95%) Train loss: 10.421 \t Validation loss: 7.952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc30d5f4cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 171
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVzUdf7A8dcHUMkTr7xNrVCRY0AEFW+3MjU1b/JCS9fc0u0w3Q4lN3etXG2tXV0rz1+rmKkd2laSpnZoqOAVqRklSaaUBwoq8Pn9MUczOMMxA8J3fD8fDx8M3/ke7/nM+OYzn+/n+/4qrTVCCCG8i095ByCEEKL0SXIXQggvJMldCCG8kCR3IYTwQpLchRDCC/mVdwAA9erV0y1atCjvMIQQwlD27t17Vmtd39lzFSK5t2jRgqSkpPIOQwghDEUp9YOr52RYRgghvJAkdyGE8EKS3IUQwgtViDF3IcrStWvXSE9PJycnp7xDEcIt/v7+NG3alEqVKhV7G0nuwuulp6dTo0YNWrRogVKqvMMRokS01mRmZpKenk7Lli2LvZ0Mywivl5OTQ926dSWxC0NSSlG3bt0Sf/OU5C5uCpLYhZG58/k1dHI/dfwcu987QV5ufnmHIoQQFYqhk/tPX39P0pY08q5cK+9QhHApLS2N4OBgh2Xx8fHMnz+/RPvp0aNHiS/2S01NxWQyER4eznfffUfnzp1tMf33v/+1rZecnMyWLVtsv7/33nvMmzevRMcqqejoaEwmE82bN6d+/fqYTCZMJhNpaWlFbnvq1CmGDh1aouO5035GZujknnP8OAD5V66UcyRCVDx5eXls2rSJoUOHsn//fm6//Xa++OILoOjkPmDAAGbOnFmm8e3evZvk5GTmzJnDiBEjSE5OJjk5GWspktzcXJfbNm7cmPXr15dpfEZn6OSenpUOQHZudjlHIoT7evTowYwZM4iKiiIwMJCdO3cCkJ2dzciRI2nbti33338/2dm/f84//vhjOnXqREREBMOGDSMrKwswl/KYMWMGERERJCQk8Morr7B48WJ69uwJQPXq1QGYOXMmO3fuxGQy8eKLLzJr1iwSEhIwmUwkJCSwYsUKHnnkEQDi4uKYOnUqnTt3plWrVrakmp+fz5QpU2jTpg133XUXffv29TjhxsfHM2bMGGJiYhgzZgxpaWl07dqViIgIIiIiHP44Wb8NrVixgsGDB9OnTx/uvPNOnnrqqSKPs2bNGkJCQggODmbGjBmA+Y9hXFwcwcHBhISEsHDhQgAWLVpEUFAQoaGhjBw50qPXdyMZeiqk7RSD3CpQFNPz7x/myKkLpbrPoMY1mX1fO4/2kZuby549e9iyZQvPP/88W7duZfHixVStWpVvvvmGAwcOEBERAcDZs2d54YUX2Lp1K9WqVePFF19kwYIFzJo1C4C6deuyb98+AI4ePUr16tV58sknHY43b9485s+fzwcffABAgwYNSEpK4rXXXgPMCdNeRkYGu3btIjU1lQEDBjB06FA2bNhAWloaR44c4ZdffqFt27ZMmDDBo3YAOHLkCLt27eKWW27h8uXLfPLJJ/j7+3Ps2DFiY2OdDq0kJyezf/9+qlSpQuvWrXn00Udp1qyZ0/2fOnWKGTNmsHfvXmrXrs3dd9/Npk2baNasGT/99BOHDh0C4Ny5c7a2+v7776lSpYptmREYuueOzIAQBuBqpoP98sGDBwPQvn1725jzjh07GD16NAChoaGEhoYC8NVXX3HkyBFiYmIwmUysXLmSH374vX7UiBEjSv01DBo0CB8fH4KCgjh9+jQAu3btYtiwYfj4+NCwYUPbtwNPDRgwgFtuuQUwX4A2ceJEQkJCGDZsGEeOHHG6Te/evalVqxb+/v4EBQU5tEdBX3/9NT169KB+/fr4+fkxatQoduzYQatWrThx4gSPPvoo//vf/6hZsyZgbvtRo0bxf//3f/j5Gac/bJxIncjNN8+Syc3LK+dIhFF42sN2R926dfntt98clv36668OF6RUqVIFAF9f30LHmsF8Uctdd93FmjVrnD5frVo1DyO+njU+6/GL6+TJk9x3330ATJ48mcmTJxe5jX38CxcupEGDBqSkpJCfn4+/v3+R8RWnDZ2pXbs2KSkpfPTRRyxZsoR169axbNkyNm/ezI4dO3j//feZO3cuBw8eNESSN3TP/WKO+Q3MuSbJXVRc1atXp1GjRnz66aeAObH/73//o0uXLoVu161bN9tJz0OHDnHgwAEAOnbsyOeff85xy4SCS5cucfTo0RLFVKNGDS5evOjy9+KIiYnhnXfeIT8/n9OnT7N9+/br1mnWrJntRGlxEntB58+fp1GjRvj4+LB69WrySqEjFxUVxWeffcbZs2fJy8tjzZo1dO/enbNnz5Kfn8+QIUN44YUX2LdvH/n5+Zw8eZKePXvy4osvcv78edv5jYqu4v/5KQ4t89xFxbZq1Sr+9Kc/8fjjjwMwe/Zsbr/99kK3efjhhxk/fjxt27albdu2tG/fHoD69euzYsUKYmNjuWKZKfbCCy8QGBhY7HhCQ0Px9fUlLCyMuLg4xo0bx7x58zCZTPzlL38p1j6GDBlCYmIiQUFBNGvWjIiICGrVqlXsGIpjypQpDBkyhFWrVtGnT59S+VbSqFEj5s2bR8+ePdFa069fPwYOHEhKSgrjx48n3zIi8Pe//528vDxGjx7N+fPn0VozdepUAgICPI7hRlAl+YpVViIjI7U780//Pe0l9JVI7pvRiuYtW5R6XMI7fPPNN7Rt27a8w/BKWVlZVK9enczMTKKiovj8889p2LBheYfllZx9jpVSe7XWkc7WN3TPXV/LIjcnmfxrzs+KCyHKVv/+/Tl37hxXr17lueeek8RegRg6uede/Y3c7H3k5d5b3qEIcVNyNs4uKoYiT6gqpZYppX5RSh2yW1ZHKfWJUuqY5Wdty3KllFqklDqulDqglIoo0+C1+eRKfl7Jz4wLIYQ3K85smRVAnwLLZgKJWus7gUTL7wD3Anda/k0CFpdOmM755ltqyuRJ+QEhhLBXZHLXWu8Afi2weCCw0vJ4JTDIbvkqbfYVEKCUalRawRYSY1kfQgghDMXdee4NtNYZlsc/Aw0sj5sAJ+3WS7csK1OlMfdVCCG8iccXMWlzt7nEXWel1CSlVJJSKunMmTOeBuHZ9kKUISn569z48eP5z3/+47Bs06ZN3Huv6wkScXFxtuJkDz30kNNyBPZFz1zZvn27rQgZwJIlS1i1alVJwnfK2XtdXtxN7qetwy2Wn79Ylv8E2M9LbGpZdh2t9VKtdaTWOrJ+/fpuhmHZF3IRkxAFVfSSv7Gxsaxdu9Zh2dq1a4mNjS3W9m+88QZBQUFuHbtgcp88eTJjx451a18VlbvJ/T1gnOXxOOBdu+VjLbNmOgLn7YZvSp+17pL03IWB3awlf3v37k1qaioZGeYUcenSJbZu3cqgQYOYM2cOHTp0IDg4mEmTJjk9r2b/TWb58uUEBgbaLqSyev/994mOjiY8PJw//OEPnD59mrS0NJYsWcLChQsxmUzs3LnT4ZtUcnIyHTt2JDQ0lPvvv99WF8jV++RKTk4O48ePJyQkhPDwcLZt2wbA4cOHiYqKwmQyERoayrFjx7h06RL9+vUjLCyM4OBgEhISit2OrhQ5z10ptQboAdRTSqUDs4F5wDql1IPAD8Bwy+pbgL7AceAyMN7jCAtlfsN1viR3UUwfzoSfD5buPhuGwL2eDWHcjCV/fX19GTJkCOvWrWPatGm8//779OjRg5o1a/LII4/YXs+YMWP44IMPbAXICsrIyGD27Nns3buXWrVq0bNnT8LDwwHo0qULX331FUop3njjDV566SX+8Y9/MHnyZId2SUxMtO1v7NixvPrqq3Tv3p1Zs2bx/PPP88orr7h8n1z517/+hVKKgwcPkpqayt13383Ro0dZsmQJ06ZNY9SoUVy9epW8vDy2bNlC48aN2bx5M2CuqeOpIpO71trVd6TeTtbVwJ88Daq4lLXrLj13UYF5UvJ36tSpgOuSvwBXr16lU6dOtn0ZqeRvbGwsTz75JNOmTWPt2rWMGTMGgG3btvHSSy9x+fJlfv31V9q1a+cyue/evdtWwhfMr99aSC09PZ0RI0aQkZHB1atXHSpxOnP+/HnOnTtH9+7dARg3bhzDhg2zPe/sfXJl165dPProowC0adOG2267jaNHj9KpUyfmzp1Leno6gwcP5s477yQkJIQnnniCGTNm0L9/f7p27VpEyxXN0FeoWkluF8XmYQ/bHVLy13XJ386dO5ORkUFKSgpffPEFa9euJScnhylTppCUlESzZs2Ij48nJyfHrbgfffRRHn/8cQYMGMD27duJj493az9WJXmfXHnggQeIjo5m8+bN9O3bl//85z/06tWLffv2sWXLFp599ll69+5t++biLkOX/LWSYRlRkUnJX9clf5VSjBgxgnHjxnHvvffi7+9vS+T16tUjKyuryHH86OhoPvvsMzIzM7l27Rpvv/227bnz58/TpIl5NvbKlStty1293lq1alG7dm3bePrq1attvfiS6tq1K2+99RZgHh778ccfad26NSdOnKBVq1ZMnTqVgQMHcuDAAU6dOkXVqlUZPXo006dPtw2recLYyV1OqAqDWLVqFX/9618xmUz06tWr2CV/s7KyaNu2LbNmzXJa8jc0NJROnTqRmppaonjsS/4uXLiQnj17cuTIEdsJ1eIYMmQITZs2JSgoiNGjR7td8jc2NpaUlBTbLJmAgAAmTpxIcHAw99xzDx06dCh0+0aNGhEfH0+nTp2IiYlxqJwYHx/PsGHDaN++PfXq1bMtv++++9i4caPthKq9lStXMn36dEJDQ0lOTna7Bz1lyhTy8/MJCQlhxIgRrFixgipVqrBu3TqCg4MxmUwcOnSIsWPHcvDgQdtJ1ueff55nn33WrWPaM3TJ38UPTuRyVgZ3TXmM0O7XnQIQApCSv2VJSv7eODdVyV9r1738/zwJcXOSkr8Vl8GTu5WkdyHKg5T8rbiMPeZukS+32RNCCAfGTu7Wae75ktyFEMKesZO7VZ4MywghhD2vSO75FWDGjxBCVCRekdzlhKqo6KwFu0pi06ZNTkvaOlOcEr3bt2+nf//+Tp975ZVXuHz5crFjy8zMxGQyYTKZaNiwIU2aNLH9fvXq1SK3T0pKspVWKK4WLVpw9uzZEm1zM/OK2TKS2oU32rRpE/379y+yrG1ubi4DBgxgwIABbh/rlVdeYfTo0VStWrVY69etW5fk5GTAfKGQs+Jkubm5+Pk5TzGRkZFERjqdni1KiVf03CvChVhCFKVgz/mRRx6xVV+cOXMmQUFBhIaG8uSTT/LFF1/w3nvvMX36dEwmE999953DvuLi4pg8eTLR0dE89dRTDiV6v/vuOzp27EhISAjPPvusw7eGrKwshg4dSps2bRg1ahRaaxYtWsSpU6fo2bOnW8W/Cotrz549dOrUifDwcDp37sy33357XVvEx8czYcIEevToQatWrVi0aFGRx1mwYAHBwcEEBwfbKja6KptbsG1vFgbvudvqD5RrFMI4XtzzIqm/luxS/aK0qdOGGVEz3N4+MzOTjRs3kpqailKKc+fOERAQwIABA+jfvz9Dhw51ul16ejpffPEFvr6+DiV6p02bxrRp04iNjWXJkiUO2+zfv5/Dhw/TuHFjYmJi+Pzzz5k6dSoLFixg27ZtDpfou8s+rgsXLrBz5078/PzYunUrTz/9NO+8885126SmprJt2zYuXrxI69atefjhh6lUqZLT/e/du5fly5eze/dutNZER0fTvXt3Tpw4cV3ZXGdte7MwdM/dVjBVZssIA6tVqxb+/v48+OCDbNiwodhDI8OGDcPX1/e65V9++aWtTO0DDzzg8FxUVBRNmzbFx8cHk8lUZNlad9jHdf78eYYNG0ZwcDCPPfYYhw8fdrpNv379qFKlCvXq1ePWW2+1lRV2ZteuXdx///1Uq1aN6tWrM3jwYHbu3ElISAiffPIJM2bMYOfOndSqVcvttvUGBu+5W27WIT13UUye9LA95efnR77dNRnW6od+fn7s2bOHxMRE1q9fz2uvvWarIFkYd0r72pfuLU7Z2t27d/PHP/4RgDlz5hRrXN8+rueee46ePXuyceNG0tLS6NGjR6nE5UxgYKDTsrnutK03MHZyt92rQy5iEhXfbbfdxpEjR7hy5QrZ2dkkJibSpUsXsrKyuHz5Mn379iUmJoZWrVoB7pXhBXNJ4HfeeYcRI0Zcd49SV6zHKjgsEx0dbTtx6g77krsF7+7krq5duxIXF8fMmTPRWrNx40ZWr17NqVOnqFOnDqNHjyYgIIA33njDZdveDIyd3K3khKqowHJzc6lSpQrNmjVj+PDhBAcH07JlS9ut4C5evMjAgQPJyclBa82CBQsAGDlyJBMnTmTRokWsX7++yBLBVtaZL3PnzqVPnz7FKsM7adIk+vTpQ+PGjW33+iwNTz31FOPGjeOFF16gX79+pbLPiIgI4uLiiIqKAuChhx4iPDycjz76iOnTp+Pj40OlSpVYvHixy7a9GRi65O+SiX/k0oWfiBkzgY79B5dBZMIblHfJ35SUFCZOnMiePXtuyPEuX77MLbfcglKKtWvXsmbNGt59992iNxQV2k1W8tcsX+7EJCqoJUuWsGjRItt0vRth7969PPLII2itCQgIYNmyZTfs2KLiMHZyt465ywlVUUE5u29oWevatSspKSk39Jii4jH0VMjfSXIXQgh7XpHctcxzF0IIBwZP7nKFqhBCOGPs5G6b516+YQghREVj6OSurFeoSnYXFZy3lfwF6NmzJx999NF1+3n44YddbtOjRw+s05779u3rtNZLfHw88+fPL/TYBdtm1qxZbN26tSThO1VYGxmNoZO7EN6suMndWvJ35syZbh/LneQeGxt73RWwa9euJTY2tljbb9myhYCAgBId06pg28yZM4c//OEPbu3LWxk8uZvHZaT8gDACbyv5O3ToUDZv3my7OUdaWhqnTp2ia9euPPzww0RGRtKuXTtmz57tdHv7m2/MnTuXwMBAunTpYisLDPD666/ToUMHwsLCGDJkCJcvX3baNnFxcaxfvx6AxMREwsPDCQkJYcKECVy5csV2vNmzZxMREUFISAipqYVXB/31118ZNGgQoaGhdOzYkQMHDgDw2Wef2W5MEh4ezsWLF8nIyKBbt26YTCaCg4PZuXNnsduxrBh7nrsMy4gS+vlvf+PKN6Vb8rdK2zY0fPppt7c3asnfOnXqEBUVxYcffsjAgQNZu3Ytw4cPRynF3LlzqVOnDnl5efTu3ZsDBw4QGhrqdD979+5l7dq1JCcnk5ubS0REBO3btwdg8ODBTJw4EYBnn32WN998k0cffdRl2+Tk5BAXF0diYiKBgYGMHTuWxYsX8+c//xmAevXqsW/fPv79738zf/583njjDZevb/bs2YSHh7Np0yY+/fRTxo4dS3JyMvPnz+df//oXMTExZGVl4e/vz9KlS7nnnnt45plnyMvLK/G3oLLgFT13IYzMyCV/7Ydm7Idk1q1bR0REBOHh4Rw+fLjQ4aWdO3dy//33U7VqVWrWrOlQefLQoUN07dqVkJAQ3nrrLZclg62+/fZbWrZsSWBgIADjxo1jx44dtucHDzaXKWnfvn2Rr33Xrl2MGTMGgF69epGZmcmFCxeIiYnh8ccfZ9GiRZw7dw4/Pz86dOjA8uXLiY+P5+DBg9SoUaPQfd8Ixu6522ZCSs9dFI8nPWxPeWPJ34EDB/LYY4+xb98+Ll++TPv27fn++++ZP38+X3/9NbVr1yYuLs72WksqLi6OTZs2ERYWxooVK9i+fbtb+7Gyvn53ywqDeQitX79+bNmyhZiYGD766CO6devGjh072Lx5M3FxcTz++OOMHTvWo1g95VHPXSn1mFLqsFLqkFJqjVLKXynVUim1Wyl1XCmVoJSqXFrBuibJXVR89iV/z507R2JiImAeBz9//jx9+/Zl4cKFttIBnpb8BUpc8rcga8nf5ORkp7Xcq1evTs+ePZkwYYKt137hwgWqVatGrVq1OH36NB9++GGhx+7WrRubNm0iOzubixcv8v7779ueu3jxIo0aNeLatWu89dZbRcbbunVr0tLSOH78OACrV6+me/fuxWqDgrp27Wo75vbt26lXrx41a9bku+++IyQkhBkzZtChQwdSU1P54YcfaNCgARMnTuShhx5i3759bh2zNLmd3JVSTYCpQKTWOhjwBUYCLwILtdZ3AL8BD5ZGoIXJlxOqogJzVvJ3+PDhDiV/+/fvT2hoKF26dHEo+fvyyy8THh5+3QnVwrzyyissWLCA0NBQjh8/XqKSv+7cQzU2NpaUlBRbcg8LCyM8PJw2bdrwwAMPEBMTU+j2ERERjBgxgrCwMO699146dOhge+6vf/0r0dHRxMTE0KZNG9tyV23j7+/P8uXLGTZsGCEhIfj4+Lhd2yc+Pp69e/cSGhrKzJkzWblyJWBu3+DgYEJDQ6lUqRL33nsv27dvt73uhIQEpk2b5tYxS5PbJX8tyf0rIAy4AGwCXgXeAhpqrXOVUp2AeK31PYXty92Sv/+ZPJms39IJHzyMXiPGlXh7cXOQkr9S8tcb3LCSv1rrn5RS84EfgWzgY2AvcE5rbR3MSgeaONteKTUJmATQvHlzd8OwBOPZ5kKUFSn5K8qL28ldKVUbGAi0BM4BbwN9iru91nopsBTMPXd34zDvy5OthSg7UvJXlBdPTqj+Afhea31Ga30N2ADEAAFKKesfjabATx7GWDTJ7kII4cCT5P4j0FEpVVUppYDewBFgG2C9smAcUGaDfUoqhwkhhFNuJ3et9W5gPbAPOGjZ11JgBvC4Uuo4UBd4sxTiLCKYMj+CEEIYikcXMWmtZwMFC0ecAKI82W+xyUVMQgjhlMHLDwhhDOnp6QwcOJA777yT22+/nWnTptkKbpWG6dOn065dO6ZPn86SJUtYtWoVACtWrODUqVO29QpWf3RVdre0LF++3FZkq3LlyoSEhGAymYpdwbKkpXy9qWSvp4xdfsBCbpAtKjKtNYMHD+bhhx/m3XffJS8vj0mTJvHMM8/w8ssve7Tv3Nxc/Pz8WLp0Kb/++ut1tWZWrFhBcHAwjRs3BszJffTo0bb6NVu2bPHo+EUZP34848ePB8xVGZ0VJ8vLy3NaIwfMJQ+Ee7yi5y4XqIqK7NNPP8Xf39+W5Hx9fVm4cCHLli3j8uXLdOzY0aEglvWGFpcuXWLChAlERUURHh5uuxBpxYoVDBgwgF69etG7d28GDBhAVlYW7du3JyEhwXazi/Xr15OUlMSoUaMwmUz885//vK60r7XsblpaGm3btmXixIm0a9eOu+++m+zsbAC+/vprQkNDMZlMTJ8+neDgYI/bpHr16jzxxBOEhYXx5ZdfMmfOHDp06EBwcDCTJk2yVXq1L+V7s5Xs9ZSX9NyFKJ6d645y9mRWqe6zXrPqdB0e6PL5w4cP20rYWtWsWZPmzZtz/PhxRowYwbp163j++efJyMggIyODyMhInn76aXr16sWyZcs4d+4cUVFRthtS7Nu3jwMHDlCnTh3AnCyTk5MB82XzYK63/tprrzF//nwiI80XMS5cuNBlad9jx46xZs0aXn/9dYYPH84777zD6NGjGT9+PK+//jqdOnXy6IYg9i5dukR0dDT/+Mc/AAgKCmLWrFkAjBkzhg8++ID77rvvuu1uppK9nvKKnjtI110Y1/Dhw22903Xr1tlqlH/88cfMmzcPk8lEjx49yMnJ4ccffwTgrrvusiX20tKyZUtMJhPwe0ncc+fOcfHiRTp16gRcX0LYXb6+vgwZMsT2+7Zt24iOjiYkJIRPP/3UZWnfm6lkr6eM3XO3TnMv3yiEgRTWwy4rQUFBtuRtdeHCBX788UfuuOMOqlatSt26dTlw4AAJCQm2G2xorXnnnXdo3bq1w7a7d+92q9xvUQqWA7YOyxTHM888w+bNmwFs3yAK4+/vbxtnz8nJYcqUKSQlJdGsWTPi4+Ndlgi+mUr2eso7eu4yFVJUYL179+by5cu2GSx5eXk88cQTxMXF2U5sjhgxgpdeeonz58/b7lh0zz338Oqrr9rGn/fv31/iYxcsjVvSMsIBAQHUqFGD3bt3A65LCM+dO9dWGrikrIm8Xr16ZGVlXfeH0F1GL9nrKe9I7kJUYEopNm7cyNtvv82dd95JYGAg/v7+/O1vf7OtM3ToUNtt6qyee+45rl27RmhoKO3ateO5554r8bGt91o1mUxkZ2e7Vdr3zTffZOLEiZhMJi5dulSsEsIlERAQwMSJEwkODuaee+5xKPnrCaOX7PWU2yV/S5O7JX+XTnmYi5knCe4zgHvGTyqDyIQ3KO+Sv0aXlZVlu8n2vHnzyMjI4J///Gc5R3XzuWElfysCa20ZLXMhhSgzmzdv5u9//zu5ubncdtttDjfjFhWXoZO7Tfl/+RDCa40YMYIRI0aUdxiihLxizF2uUBVCCEfGTu7WwmH5ktyFEMKesZO7haR2IYRw5BXJXQghhCNjJ3cZlhEGcbOW/E1LS6Np06bk5zvOaDOZTLYLo5xtYy1OlpSUxNSpU52uZy16Vhj7awkAOnfuXNzQC2Vf0KyiMnZyt5HkLioua8nfQYMGcezYMY4ePUpWVhbPPPOMx/u2XoK/dOlSDhw4wMsvv8zkyZNtl84Xldy3bNlCQECAx3G40qJFC5o3b+5QZTE1NZWLFy8SHR1d5PaRkZEsWrTI7eMXTO5ffPGF2/syGmMnd2WZ517OYQhRmJu95G9sbKxD2YK1a9cycuRI0tLS6Nq1KxEREURERDhNvPY338jMzOTuu++mXbt2PPTQQ9hfgDlo0CDat29Pu3btWLp0KWCuIZOdnY3JZGLUqFEAtouxtNa21xISEkJCQoLteD169GDo0KG0adOGUaNGUdSFnomJiYSHhxMSEsKECRO4cuWK7fhBQUGEhoby5JNPAvD2228THBxMWFgY3bp1K1E7lpSh57kra1qvAFfZCmPYtmIpv/xwolT3eettregZ5/oK6Zu95O/w4cMxmUy8+uqr+Pn5kdBrgMEAABOuSURBVJCQwNtvv82tt97KJ598gr+/P8eOHSM2NpbCrlR//vnn6dKlC7NmzWLz5s28+ebvt2detmwZderUITs7mw4dOjBkyBDmzZvHa6+95rTezYYNG0hOTiYlJYWzZ8/SoUMHW7Ldv38/hw8fpnHjxsTExPD555/TpUsXpzHl5OQQFxdHYmIigYGBjB07lsWLFzNmzBg2btxIamoqSinb0NecOXP46KOPaNKkSZkOh4HRe+62QXchjMvbS/42aNCA4OBgEhMTSU5Oxs/Pj+DgYK5du8bEiRMJCQlh2LBhHDlypND97Nixg9GjRwPQr18/ateubXtu0aJFhIWF0bFjR06ePMmxY8cK3deuXbuIjY3F19eXBg0a0L17d77++msAoqKiaNq0KT4+PphMpkJLC3/77be0bNmSwEBztdFx48axY8cOatWqhb+/Pw8++CAbNmywFYiLiYkhLi6O119/nby8vCLbzhOG7rlbVYT6OMIYCuthlxUp+fv70EyDBg2IjY0FzN8iGjRoQEpKCvn5+fj7+7sV9/bt29m6dStffvklVatWtf0hdFfBdnCntLCfnx979uwhMTGR9evX89prr/Hpp5+yZMkSdu/ezebNm2nfvj179+6lbt26bsdaGGP33KXjLgxASv6ab7KxZcsWEhISGDlyJADnz5+nUaNG+Pj4sHr16iJ7st26deO///0vAB9++CG//fabbT+1a9ematWqpKam8tVXX9m2qVSpEteuXbtuX127diUhIYG8vDzOnDnDjh07iIqKKqI1rte6dWvS0tI4fvw4AKtXr6Z79+5kZWVx/vx5+vbty8KFC0lJSQHgu+++Izo6mjlz5lC/fn1OnjxZ4mMWl7GTu4X020VFJiV/zX8kOnXqRIMGDWjVqhUAU6ZMYeXKlYSFhZGamlrkt5HZs2ezY8cO2rVrx4YNG2jevDkAffr0ITc3l7Zt2zJz5kw6duxo22bSpEmEhobaTqha3X///YSGhhIWFkavXr146aWXaNiwYYlfl7+/P8uXL2fYsGGEhITg4+PD5MmTuXjxIv379yc0NJQuXbqwYMECwDxlNSQkhODgYDp37kxYWFiJj1lchi75+8bUP3H+9A8Edr+L+6YYv/6yKBtS8tczUvK3YripSv7+rvz/QAnhraTkrzEZOrnb6rlLOXchyoyU/DUmrxhzF6IoFWH4UQh3ufP59Y7kLv9xRSH8/f3JzMyUBC8MSWtNZmZmiaeKGnpYRqZCiuJo2rQp6enpnDlzprxDEcIt/v7+NG3atETbGDu5W8mguyhEpUqVaNmyZXmHIcQN5RXDMvJlWwghHHmU3JVSAUqp9UqpVKXUN0qpTkqpOkqpT5RSxyw/axe9J09JehdCCHue9tz/CfxPa90GCAO+AWYCiVrrO4FEy+9CCCFuILeTu1KqFtANeBNAa31Va30OGAistKy2EhjkaZCugzD/kEkQQgjhyJOee0vgDLBcKbVfKfWGUqoa0EBrnWFZ52eggbONlVKTlFJJSqkkj2cxSHYXQggHniR3PyACWKy1DgcuUWAIRpsnFjvNvFrrpVrrSK11ZP369d0KQMlcSCGEcMqT5J4OpGutrXe5XY852Z9WSjUCsPz8xbMQiyYXpwghhCO3k7vW+mfgpFLKeieB3sAR4D1gnGXZOOBdjyIsjHTchRDCKU8vYnoUeEspVRk4AYzH/AdjnVLqQeAHYHgh23vI3GOXjrsQQjjyKLlrrZMBZ7WEe3uy3+KzTpe5MUcTQgij8IorVIUQQjjyjuQutWWEEMKBoZO7UnJGVQghnDF0cv+dDLoLIYQ9r0juktqFEMKRVyR3ye5CCOHI2MndOuQuE92FEMKBsZO7XKIqhBBOGTy5m0ltGSGEcGTo5C79diGEcM7Qyd1GOu5CCOHA2MldSssIIYRTxk7uMjAjhBBOGTy5W/rsUltGCCEcGDy5CyGEcMYrkrvWMjwjhBD2DJ3cpSikEEI4Z+jkbld/oFyjEEKIisbYyd3Wc5fkLoQQ9oyd3K2k/IAQQjgwdHJXlq67nFAVQghHhk7u0l8XQgjnDJ3cfydpXggh7Bk7uStJ6kII4Yyhk7uyVQ6TJC+EEPaMndwltwshhFOGTu6/k+wuhBD2DJ3ctZT8FUIIpwyd3JX02IUQwilDJ3cbGXQXQggHxk7uljOqMjwjhBCOPE7uSilfpdR+pdQHlt9bKqV2K6WOK6USlFKVPQ+zCNJzF0IIB6XRc58GfGP3+4vAQq31HcBvwIOlcAynlNwgWwghnPIouSulmgL9gDcsvyugF7DesspKYJAnxygigrLbtRBCGJinPfdXgKcA6x2q6wLntNa5lt/TgSbONlRKTVJKJSmlks6cOePWwW09dhmWEUIIB24nd6VUf+AXrfVed7bXWi/VWkdqrSPr16/vXgxubSWEEN7Pz4NtY4ABSqm+gD9QE/gnEKCU8rP03psCP3keZhGk4y6EEA7c7rlrrf+itW6qtW4BjAQ+1VqPArYBQy2rjQPe9ThKV2wnVCW7CyGEvbKY5z4DeFwpdRzzGPybZXAMwL4qZFkdQQghjMmTYRkbrfV2YLvl8QkgqjT2WyTroLskdyGEcGDsK1RtJLsLIYQ9L0nuQggh7Bk7udtqywghhLBn7ORuJRcxCSGEA4Mnd7mMSQghnDF4creQnrsQQjgwdnKXjrsQQjhl6OQuuV0IIZwzdHK30jIsI4QQDgyd3JWSvrsQQjhj6OQuhBDCOUMnd61kOEYIIZwxdHJXckpVCCGcMnRyt86XkROqQgjhyNDJXc6nCiGEc4ZO7jbScRdCCAfGTu62nrtkdyGEsGfs5C6EEMIp70ju0nEXQggHBk/uckZVCCGcMXZyt+R26bgLIYQjQyd3OZ8qhBDOGTq5/z7RXbK7EELYM3ZyF0II4ZShk7tcoSqEEM4ZOrn/XlumnMMQQogKxtDJ/feqkJLdhRDCnqGTu5ZhGSGEcMrQyV0mugshhHMGT+6S1YUQwhlDJ3cZlRFCCOfcTu5KqWZKqW1KqSNKqcNKqWmW5XWUUp8opY5ZftYuvXCvi8HySHrwQghhz5Oeey7whNY6COgI/EkpFQTMBBK11ncCiZbfy4SWvrsQQjjldnLXWmdorfdZHl8EvgGaAAOBlZbVVgKDPA3SFWvHXe6hKoQQjkplzF0p1QIIB3YDDbTWGZanfgYauNhmklIqSSmVdObMGbeOW6daZQAq+Rr61IEQQpQ6j7OiUqo68A7wZ631BfvntLlL7bRbrbVeqrWO1FpH1q9f361jV6lkDl9SuxBCOPIoLyqlKmFO7G9prTdYFp9WSjWyPN8I+MWzEAs9PiDlB4QQoiBPZsso4E3gG631Arun3gPGWR6PA951PzwhhBDu8PNg2xhgDHBQKZVsWfY0MA9Yp5R6EPgBGO5ZiIWRqZBCCOGM28lda70L19cR9XZ3vyUiMyGFEMIpQ5+LlNvsCSGEc4ZO7ihz+JLbhRDCkaGT+++jMpLehRDCnqGTuxBCCOcMndyrVqoCwLX83HKORAghKhZDJ/cqNZoAcCU/v5wjEUKIisXQyb1q4+YAXL1WuZwjEUKIisXQyb1SFXP4uddqlHMkQghRsRg6uVtr/vrkBXD5anY5ByOEEBWHJ+UHyp2yTIb0wZ8Fz2zgau0L3BveizuCG1OvSXWUj1zCKoS4ORk6uVsnuteq9TPXqtxK9dO12ffuT+x79yeqVPWj0R0BNG1dm6Zta1OnUTW72/IJIYR3M3Zy9zGPKgU3uYX2Tw4B4NK5K6R/+xunjp3jp29/I+3AWQCq1qxM0za1aRJYm1q33kK1gCr4V6tElVv8pIcvhPA6hk7ufgHme29fPXPWtqxaQBVaRzekdXRDAC5kZpOe+hvpqb9x8ptfObrntONOFLYEr3wUPgqUr0IpRbE6+gVWcrqJ/O0QQrgQdV9LAjs0LPX9Gjq512rYCF8Np9N/cLlOzbq3EBRzC0ExjdH5mvNns7lwNpvLF65y5VIuOZeucSU7F52nydcana/Refq6G4BoZyUOCq4jVRCEECV0S/Wymcpt6OTuV7kyDWrUIuO3TPKys9FZWfgVcss+5aMIuLUqAbdWvYFRCiHEjWfo5A5wZ6cufPbJZt4YOZB8pWgY3ZFmwWE0bh1EozsCUT7Gnu0phBDuMHxyj5gwiQuZmZw++g2Va9Qk81Q6J5L3AlC9dh1u79CJlqb2NLojkKq1Aso5WiGEuDGUrgADxZGRkTopKanU9nfp3G/8eCiFo1/tIu3AfnKvXAGgkv8tVKpSBb/KlfH180Pna67mZJOfl4dvpUr4+PiaT6w63Hhbo7X5H5Z/GtCWejbW5bbnLWxj9NYfDu2skbOsxeVNbWX9DJTV6/Gmtrp59Bj3ECE973ZrW6XUXq11pLPnDN9zd6ZaQG3adulB2y49yL16lYzj33L6xHEuZp4l9+oVcq9eJS83F6UUlf1vwcfPl7xr18jPy0frfFsiVgDKkuyVQlluDmL+1QesP8EynVJZnrf7D2Z5aF0PrX+fYWP/uCBX61kfO9vW+gfE+rz1cVHrFLaePfsYirvMfr8F13EVS8F4Cr4WV6+94DZFrVNYjMXdl6vYihOPs/1Y2e/D1U9X+ynuZ6okxyu4TsH2sn/eFVefOVfHK7iNu5+10jye/TrO9ucqpkL2UadR0+tjLQVemdzt+VWuTLOgEJoFhZR3KEIIccPI2UYhhPBCktyFEMILSXIXQggvJMldCCG8kCR3IYTwQpLchRDCC0lyF0IILyTJXQghvFCFKD+glDoDuK7bW7h6wNki17rxJK6SkbhKrqLGJnGVjCdx3aa1dloKt0Ikd08opZJc1VYoTxJXyUhcJVdRY5O4Sqas4pJhGSGE8EKS3IUQwgt5Q3JfWt4BuCBxlYzEVXIVNTaJq2TKJC7Dj7kLIYS4njf03IUQQhQgyV0IIbyQoZO7UqqPUupbpdRxpdTMMj5WM6XUNqXUEaXUYaXUNMvyeKXUT0qpZMu/vnbb/MUS27dKqXvKMm6lVJpS6qAlhiTLsjpKqU+UUscsP2tbliul1CLL8Q8opSLs9jPOsv4xpdQ4D2NqbdcuyUqpC0qpP5dHmymllimlflFKHbJbVmrto5Rqb2n/45Zti3W/OxdxvayUSrUce6NSKsCyvIVSKtuu3ZYUdXxXr9HNuErtfVNKtVRK7bYsT1BKVfYgrgS7mNKUUsnl0F6u8kP5fcas9wc12j/AF/gOaAVUBlKAoDI8XiMgwvK4BnAUCALigSedrB9kiakK0NISq29ZxQ2kAfUKLHsJmGl5PBN40fK4L/Ah5psAdgR2W5bXAU5Yfta2PK5diu/Xz8Bt5dFmQDcgAjhUFu0D7LGsqyzb3utBXHcDfpbHL9rF1cJ+vQL7cXp8V6/RzbhK7X0D1gEjLY+XAA+7G1eB5/8BzCqH9nKVH8rtM2bknnsUcFxrfUJrfRVYCwwsq4NprTO01vssjy8C3wBNCtlkILBWa31Fa/09cNwS842MeyCw0vJ4JTDIbvkqbfYVEKCUagTcA3yitf5Va/0b8AnQp5Ri6Q18p7Uu7ErkMmszrfUO4Fcnx/O4fSzP1dRaf6XN/wtX2e2rxHFprT/WWudafv0KKPQmm0Uc39VrLHFchSjR+2bpcfYC1pdmXJb9DgfWFLaPMmovV/mh3D5jRk7uTYCTdr+nU3iyLTVKqRZAOLDbsugRy1erZXZf41zFV1Zxa+BjpdRepdQky7IGWusMy+OfgQblFBvASBz/01WENiut9mlieVza8QFMwNxLs2qplNqvlPpMKdXVLl5Xx3f1Gt1VGu9bXeCc3R+w0mqvrsBprfUxu2U3vL0K5Idy+4wZObmXC6VUdeAd4M9a6wvAYuB2wARkYP5aWB66aK0jgHuBPymlutk/aflrXy7zXi3jqQOAty2LKkqb2ZRn+7iilHoGyAXesizKAJprrcOBx4H/KqVqFnd/pfAaK9z7VkAsjh2IG95eTvKDR/vzhJGT+09AM7vfm1qWlRmlVCXMb9xbWusNAFrr01rrPK11PvA65q+ihcVXJnFrrX+y/PwF2GiJ47Tl65z1q+gv5REb5j84+7TWpy0xVog2o/Ta5ycch048jk8pFQf0B0ZZkgKWYY9My+O9mMezA4s4vqvXWGKl+L5lYh6G8HMSr1ss+xoMJNjFe0Pby1l+KGR/Zf8ZK87Jgor4D/DDfLKhJb+frGlXhsdTmMe5XimwvJHd48cwjz0CtMPxJNMJzCeYSj1uoBpQw+7xF5jHyl/G8WTOS5bH/XA8mbNH/34y53vMJ3JqWx7XKYW2WwuML+82o8AJttJsH64/2dXXg7j6AEeA+gXWqw/4Wh63wvyfu9Dju3qNbsZVau8b5m9x9idUp7gbl12bfVZe7YXr/FBun7EySYQ36h/mM85HMf9FfqaMj9UF81eqA0Cy5V9fYDVw0LL8vQL/AZ6xxPYtdme2Sztuywc3xfLvsHWfmMc2E4FjwFa7D4kC/mU5/kEg0m5fEzCfEDuOXUL2ILZqmHtqteyW3fA2w/x1PQO4hnm88sHSbB8gEjhk2eY1LFd/uxnXcczjrtbP2RLLukMs728ysA+4r6jju3qNbsZVau+b5TO7x/Ja3waquBuXZfkKYHKBdW9ke7nKD+X2GZPyA0II4YWMPOYuhBDCBUnuQgjhhSS5CyGEF5LkLoQQXkiSuxBCeCFJ7kII4YUkuQshhBf6fzLr6vSgb1REAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "params = [(10, 2e-3), (200,2e-3), (20000, 2e-3)]\n",
        "NNs = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for (EPOCHS, LEARN_R) in params:\n",
        "  NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  for e in range(EPOCHS):\n",
        "      \n",
        "      # Forward pass and loss computation\n",
        "      Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "      # Backward pass\n",
        "      Loss.backward()\n",
        "      \n",
        "      # gradient descent update\n",
        "      update_parameters(parameters(NN), LEARN_R)\n",
        "      zero_gradients(parameters(NN))\n",
        "      \n",
        "      # Training loss\n",
        "      train_loss.append(Loss.v)\n",
        "      \n",
        "      # Validation\n",
        "      Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "      val_loss.append(Loss_validation.v)\n",
        "      \n",
        "      if e%10==0:\n",
        "          print(\"{:4d}\".format(e),\n",
        "                \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "                \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "  NNs.append(NN)\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "types = ['Underfitting', 'Just right', 'Overfitting']\n",
        "output_tests = []\n",
        "for i in range(len(NNs)):\n",
        "  output_tests.append(forward(x_test, NNs[i]))\n",
        "  plt.plot(range(len(train_losses[i])), train_losses[i], label=f'{types[i]} - Train loss');\n",
        "  plt.plot(range(len(val_losses[i])), val_losses[i], label=f'{types[i]} - Validation loss');\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_np = Var_to_nparray(x_test)\n",
        "x_train_np = Var_to_nparray(x_train)\n",
        "y_train_np = Var_to_nparray(y_train)\n",
        "if D1:\n",
        "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
        "    for x in range(len(output_tests)):\n",
        "      plt.scatter(x_test_np, Var_to_nparray(output_tests[x]), label=f\"{types[x]} - test prediction\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
        "    for x in range(len(output_tests)):\n",
        "      plt.scatter(x_test_np[:,1], Var_to_nparray(output_tests[x]), label=f\"{types[x]} - test data prediction\");\n",
        "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ],
      "metadata": {
        "id": "sRiVGCt8C94-",
        "outputId": "cbdfddc6-b056-4ff9-fc77-7d4b4a192825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo"
      },
      "source": [
        "## Exercise m) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC8QrI2tFtIp"
      },
      "outputs": [],
      "source": [
        "# Just add code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APqhJv3tta1O"
      },
      "source": [
        "## Exercise n) optional - Introduce a NeuralNetwork class\n",
        "\n",
        "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqfnor1ouMLq"
      },
      "outputs": [],
      "source": [
        "# just add some code"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "U4057_ljNvWB",
        "p_8n_SKnIW2F",
        "oLrGJytZFtGm",
        "jpIZPBpNI0pO",
        "_79HOAXrFtHK",
        "mqeyab9qFtGs",
        "-XyXBD37FtHk",
        "SrwSJ2UWFtHu",
        "zTBAmjsAFtIk",
        "qsVPul3QFtIo",
        "APqhJv3tta1O"
      ],
      "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}