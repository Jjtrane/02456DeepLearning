{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
        "\n",
        "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
        "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Nanograd automatic differentiation framework\n",
        "2. Finite difference method\n",
        "3. Data generation\n",
        "4. Defining and initializing the network\n",
        "5. Forward pass\n",
        "6. Training loop \n",
        "7. Testing your model\n",
        "8. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyXeAA-HuT7s"
      },
      "source": [
        "# Nanograd automatic differention framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6UWKCLKubgA"
      },
      "source": [
        "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd4CoEBNzNWS"
      },
      "outputs": [],
      "source": [
        "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
        "\n",
        "from math import exp, log\n",
        "\n",
        "class Var:\n",
        "    \"\"\"\n",
        "    A variable which holds a float and enables gradient computations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, val: float, grad_fn=lambda: []):\n",
        "        assert type(val) == float\n",
        "        self.v = val\n",
        "        self.grad_fn = grad_fn\n",
        "        self.grad = 0.0\n",
        "\n",
        "    def backprop(self, bp):\n",
        "        self.grad += bp\n",
        "        for input, grad in self.grad_fn():\n",
        "            input.backprop(grad * bp)\n",
        "\n",
        "    def backward(self):\n",
        "        self.backprop(1.0)\n",
        "\n",
        "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
        "\n",
        "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        assert type(power) in {float, int}, \"power must be float or int\"\n",
        "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
        "\n",
        "    def __neg__(self: 'Var') -> 'Var':\n",
        "        return Var(-1.0) * self\n",
        "\n",
        "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self + (-other)\n",
        "\n",
        "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self * other ** -1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
        "\n",
        "    def relu(self):\n",
        "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        return Var(1/(1+exp(-self.v)), lambda: [(self, exp(self.v)/((exp(self.v)+1)**2))])\n",
        "\n",
        "    def tanh(self):\n",
        "        return Var((exp(2*self.v)-1)/(exp(2*self.v)+1), lambda: [(self, 4/((exp(-self.v)+exp(self.v))**2))])\n",
        "\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDX67D6jzcte"
      },
      "source": [
        "A few examples illustrate how we can use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6PeLc3zwPT",
        "outputId": "f310e91c-166c-408a-8eb8-a08d5548c559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "f = a * b\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmKhYgsY0g_o",
        "outputId": "bbec2bea-8a4b-4fab-de77-ca178e85b30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=14.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "c = a * b\n",
        "d = Var(9.0)\n",
        "e = a * d\n",
        "f = c + e\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3B6uEH140p"
      },
      "source": [
        "## Exercise a) What is being calculated?\n",
        "\n",
        "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (a)**\n",
        "\n",
        "The result Var($v$,grad) tells you the value $v$ of that weight, and grad tells you how much the factor in which the value has changed. For example for a in cell [5], v=3.0, and grad=14, as 14\\*v=42, as a is a factor of f (f=a\\*d+a*b), so d+b. That is $∂f/∂a=d+b=14$. So a, b and d are the independent variables in this case, and c, e, and f the dependent ones. And for completeness sake: $∂f/∂b=∂f/∂d=a=3$."
      ],
      "metadata": {
        "id": "rfg1rpHGAATh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_Q0t2I3Ruj"
      },
      "source": [
        "## Exercise b) How does the backward function work?\n",
        "\n",
        "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
        "\n",
        "Go through the following four steps and answer the questions on the way:\n",
        "\n",
        "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
        "\n",
        "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
        "\n",
        "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. \n",
        "\n",
        "4. Write down the sequence of calls to backprop."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (b)**\n",
        "\n",
        "The answer provided for exercise (a), assuming it is correct, pretty much describes what happens, and the procedure behind the code, so I refer to there.\n",
        "\n",
        "The sequence of calls to backprop is:\n",
        "\\begin{align}\n",
        "\\partial_1^{3} &= y_j-t_j\n",
        "\\\\\n",
        "∂_1^{2}&=\\frac{∂f}{∂c}⋅w_{21}\\cdot h_2(a_1^{2})=w_{21}\\cdot h_2(a_1^{2})\n",
        "\\\\\n",
        "∂_2^{2}&=\\frac{∂f}{∂e}⋅w_{22}\\cdot h_2(a_2^{2})=w_{22}\\cdot h_2(a_2^{2})\n",
        "\\\\\n",
        "∂_1^{1}&=\\frac{∂c}{∂a}⋅w_{11}\\cdot h_1(a_1^{1})=5\\cdot w_{11}\\cdot h_1(a_1^{1})\n",
        "\\\\\n",
        "∂_2^{1}&=\\frac{∂c}{∂b}⋅w_{12}\\cdot h_1(a_2^{1})=3\\cdot w_{12}\\cdot h_1(a_2^{1})\n",
        "\\\\\n",
        "∂_3^{1}&=\\frac{∂e}{∂a}⋅w_{13}\\cdot h_1(a_3^{1})=9\\cdot w_{13}\\cdot h_1(a_3^{1})\n",
        "\\\\\n",
        "∂_4^{1}&=\\frac{∂e}{∂d}⋅w_{14}\\cdot h_1(a_4^{1})=3\\cdot w_{14}\\cdot h_1(a_4^{1})\n",
        "\\\\\n",
        "\\frac{∂f}{∂c}&=1\n",
        "\\\\\n",
        "\\frac{∂f}{∂e}&=1\n",
        "\\\\\n",
        "\\frac{∂c}{∂a}&=5\n",
        "\\\\\n",
        "\\frac{∂c}{∂b}&=3\n",
        "\\\\\n",
        "\\frac{∂e}{∂a}&=9\n",
        "\\\\\n",
        "\\frac{∂e}{∂d}&=3\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "iWfEpItIFZqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idGr71jYXl26"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "import graphviz\n",
        "\n",
        "#logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
        "\n",
        "#graphviz.__version__, graphviz.version()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "KPe30Q2QXzeG",
        "outputId": "845dae75-0422-469a-c17e-55605b8b890b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe364969e90>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: first expression Pages: 1 -->\n<svg width=\"201pt\" height=\"98pt\"\n viewBox=\"0.00 0.00 201.00 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>first expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 197,-94 197,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-72\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- f -->\n<g id=\"node2\" class=\"node\">\n<title>f</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"175\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"175\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n</g>\n<!-- a&#45;&gt;f -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M36.0099,-68.9028C63.2189,-64.2235 115.029,-55.3135 147.1324,-49.7925\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"147.938,-53.2054 157.2001,-48.0611 146.7516,-46.3067 147.938,-53.2054\"/>\n<text text-anchor=\"middle\" x=\"96.5\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/da=b+d=14</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;f -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M36.2654,-18.7974C60.154,-20.1385 103.1803,-23.5804 139,-32 142.0706,-32.7218 145.2403,-33.6228 148.3666,-34.6119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"147.4373,-37.994 158.0321,-37.9595 149.7283,-31.3794 147.4373,-37.994\"/>\n<text text-anchor=\"middle\" x=\"96.5\" y=\"-35.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/db=a=3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
        "\n",
        "e1.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e1.attr('node', shape='circle')\n",
        "e1.edge('a', 'f', label='df/da=b+d=14')\n",
        "e1.edge('b', 'f', label='df/db=a=3')\n",
        "\n",
        "e1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "0nittR-mZFeX",
        "outputId": "e590ed50-dec0-47ba-b0d1-3ec491c44bf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe3648c4cd0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: second expression Pages: 1 -->\n<svg width=\"297pt\" height=\"158pt\"\n viewBox=\"0.00 0.00 297.00 158.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 154)\">\n<title>second expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-154 293,-154 293,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- c -->\n<g id=\"node2\" class=\"node\">\n<title>c</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"153\" cy=\"-102\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"153\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">c</text>\n</g>\n<!-- a&#45;&gt;c -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.927,-78.5854C58.7217,-83.1443 98.3264,-91.0653 125.0893,-96.4179\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.5067,-99.8706 134.9989,-98.3998 125.8795,-93.0065 124.5067,-99.8706\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/da=b=5</text>\n</g>\n<!-- e -->\n<g id=\"node4\" class=\"node\">\n<title>e</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"153\" cy=\"-48\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"153\" y=\"-44.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">e</text>\n</g>\n<!-- a&#45;&gt;e -->\n<g id=\"edge3\" class=\"edge\">\n<title>a&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.0388,-68.6548C40.9759,-66.6224 47.7227,-64.51 54,-63 77.6391,-57.3137 104.9985,-53.3634 124.9572,-50.9495\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.4047,-54.4212 134.9366,-49.7957 124.6007,-47.4675 125.4047,-54.4212\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-66.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/da=d=9</text>\n</g>\n<!-- f -->\n<g id=\"node6\" class=\"node\">\n<title>f</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"271\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"271\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n</g>\n<!-- c&#45;&gt;f -->\n<g id=\"edge5\" class=\"edge\">\n<title>c&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.6658,-97.9578C189.8698,-93.5637 220.8151,-86.483 243.2661,-81.3459\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.2461,-84.7122 253.2134,-79.0698 242.6847,-77.8885 244.2461,-84.7122\"/>\n<text text-anchor=\"middle\" x=\"212\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/dc=1</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-132\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-128.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;c -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.6489,-128.078C58.4896,-123.0023 98.5365,-114.103 125.4117,-108.1307\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"126.3442,-111.509 135.3468,-105.9229 124.8256,-104.6757 126.3442,-111.509\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/db=a=3</text>\n</g>\n<!-- e&#45;&gt;f -->\n<g id=\"edge6\" class=\"edge\">\n<title>e&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M171.0339,-50.9637C187.7278,-53.8381 213.1872,-58.555 235,-64 238.0098,-64.7513 241.1413,-65.6022 244.2457,-66.4899\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"243.3051,-69.8618 253.8887,-69.3744 245.3112,-63.1554 243.3051,-69.8618\"/>\n<text text-anchor=\"middle\" x=\"212\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/de=1</text>\n</g>\n<!-- d -->\n<g id=\"node5\" class=\"node\">\n<title>d</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">d</text>\n</g>\n<!-- d&#45;&gt;e -->\n<g id=\"edge4\" class=\"edge\">\n<title>d&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M36.1633,-19.4114C56.2344,-21.2788 89.376,-25.278 117,-33 120.2604,-33.9114 123.6205,-35.039 126.917,-36.2636\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.869,-39.6148 136.4523,-40.109 128.4871,-33.1228 125.869,-39.6148\"/>\n<text text-anchor=\"middle\" x=\"85.5\" y=\"-36.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/dd=a=3</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
        "\n",
        "e2.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e2.attr('node', shape='circle')\n",
        "e2.edge('a', 'c', label='dc/da=b=5')\n",
        "e2.edge('b', 'c', label='dc/db=a=3')\n",
        "e2.edge('a', 'e', label='de/da=d=9')\n",
        "e2.edge('d', 'e', label='de/dd=a=3')\n",
        "e2.edge('c', 'f', label='df/dc=1')\n",
        "e2.edge('e', 'f', label='df/de=1')\n",
        "\n",
        "e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5oi21W4gpeM"
      },
      "source": [
        "## Exercise c) What happens if we run backward again?\n",
        "\n",
        "Try to execute the code below. Explain what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCtpJyr-gyX1",
        "outputId": "90e15fba-8fec-4646-d2e2-3595f45536f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=28.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n"
          ]
        }
      ],
      "source": [
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (c)**\n",
        "\n",
        "The gradient is multiplied by 2, as the same gradients are calculated, and are added back to the variable gradient (from code: bp*grad, and bp=2, for the number of backpropagations done)."
      ],
      "metadata": {
        "id": "LArElv7DO3x5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bPVq2VhsP-"
      },
      "source": [
        "## Exercise d) Zero gradient\n",
        "\n",
        "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnyPDQx9lJe0",
        "outputId": "7c024864-45be-49eb-9711-8363bb23b2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n",
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(2.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)\n",
        "\n",
        "f.backprop(-1.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (d)**\n",
        "\n",
        "Even though a is redefined, the old a is still used/saved in f, and can be accessed by grad_fn() calls. What backprop(-1) then does is go back a step. So running the previous code in tandem, it would do two backpropagations, and then undo one/do a forward propagation."
      ],
      "metadata": {
        "id": "B7NMCByXWVen"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4057_ljNvWB"
      },
      "source": [
        "## Exercise e) Test correctness of derivatives with the finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
        "$$\n",
        "for $da \\ll 1$.\n",
        "\n",
        "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TGil92lSXDN",
        "outputId": "8792d165-26e1-426a-e908-e647d5835957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "5.000000413701855\n"
          ]
        }
      ],
      "source": [
        "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
        "def f_function(a):\n",
        "  a = Var(a)\n",
        "  b = Var(5.0)\n",
        "  f = a * b\n",
        "  f.backward()\n",
        "  return a,b,f\n",
        "\n",
        "for v in f_function(3.0):\n",
        "  print(v)\n",
        "\n",
        "# Insert your finite difference code here\n",
        "def finite_difference(a, da=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    a:           Point of gradient\n",
        "    da:          The finite difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: numerical approximation to the derivative (float) \n",
        "    \"\"\"\n",
        "    a = float(a) if isinstance(a,int) else a\n",
        "    fa_da = f_function(a+da)[2].v\n",
        "    fa = f_function(a)[2].v\n",
        "\n",
        "    finite_difference = (fa_da - fa) / da\n",
        "    \n",
        "    return finite_difference\n",
        "\n",
        "print(finite_difference(a=3.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZar5RKaUkg"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6yfMAQ8aduj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YabfD43ajNh"
      },
      "outputs": [],
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1oDngHLapIz"
      },
      "outputs": [],
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysfa3FsBavlm",
        "outputId": "63fa02b6-5c7d-465d-d9e5-e844ff13e5f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zac2HHNlgbpm"
      },
      "outputs": [],
      "source": [
        "# convert from nparray to Var\n",
        "def nparray_to_Var(x):\n",
        "  if x.ndim==1:\n",
        "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
        "  else:\n",
        "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
        "  return y\n",
        "   \n",
        "x_train = nparray_to_Var(x_train)\n",
        "y_train = nparray_to_Var(y_train)\n",
        "x_validation = nparray_to_Var(x_validation)\n",
        "y_validation = nparray_to_Var(y_validation)\n",
        "x_test = nparray_to_Var(x_test)\n",
        "y_test = nparray_to_Var(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe"
      },
      "source": [
        "# Defining and initializing the network\n",
        "\n",
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
        "\n",
        "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij_ieRsAt7Xt"
      },
      "outputs": [],
      "source": [
        "class Initializer:\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb18N5phuIha"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class NormalInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, mean=0, std=0.1):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(0.0) for _ in range(n_out)]\n",
        "\n",
        "class ConstantInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, weight=1.0, bias=0.0):\n",
        "    self.weight = weight\n",
        "    self.bias = bias\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(self.bias) for _ in range(n_out)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOLYGnZKuM6W"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
        "        self.weights = initializer.init_weights(n_in, n_out)\n",
        "        self.bias = initializer.init_bias(n_out)\n",
        "        self.act_fn = act_fn\n",
        "    \n",
        "    def __repr__(self):    \n",
        "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
        "\n",
        "    def parameters(self) -> Sequence[Var]:\n",
        "      params = []\n",
        "      for r in self.weights:\n",
        "        params += r\n",
        "\n",
        "      return params + self.bias\n",
        "\n",
        "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input\n",
        "        # to the current layer matches the number of nodes in the current layer\n",
        "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "        weights = self.weights\n",
        "        out = []\n",
        "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "        # We therefore loop over the (number of) nodes in the current layer:\n",
        "        for j in range(len(weights[0])):\n",
        "            # Initialize the node value depending on its corresponding parameters.\n",
        "            node = self.bias[j]  # <- Insert code\n",
        "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "            for i in range(len(single_input)):\n",
        "                node += weights[i][j] * single_input[i]  # <- Insert code\n",
        "            node = self.act_fn(node)\n",
        "            out.append(node)\n",
        "\n",
        "        return out\n",
        "      \n",
        "      def describe(self):\n",
        "        # return f'Input nodes: {self.n_in}, output nodes: {self.n_out} ||' +  f'Weights of input{i}: ' for i in range(len(weights[0]))\n",
        "        n_out = len(self.weights[0])\n",
        "        n_in = len(self.weights)\n",
        "        params = self.parameters()\n",
        "        description = [[f'Weight from input {j} to node {i}: {params[n_out * j + i]} and bias {params[n_out * n_in + j]}' for i\n",
        "          in range(n_out)] for j in range(n_in)]\n",
        "        return sum(description, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIZPBpNI0pO"
      },
      "source": [
        "## Exercise f) Add more activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
        " \n",
        "Implement the following activation functions in the Var class:\n",
        "\n",
        "* Identity: $$\\mathrm{identity}(x) = x$$\n",
        "* Hyperbolic tangent: $$\\tanh(x)$$\n",
        "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
        "\n",
        "Hint: You can seek inspiration in the relu method in the Var class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (f)**\n",
        "\n",
        "The following code snippet is implemented in the previous Var class."
      ],
      "metadata": {
        "id": "_AabEBeucF4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def sigmoid(self):\n",
        "        return Var(1/(1+exp(-self.v)), lambda: [(self, exp(self.v)/((exp(self.v)+1)**2))])\n",
        "\n",
        "    def tanh(self):\n",
        "        return Var((exp(2*self.v)-1)/(exp(2*self.v)+1), lambda: [(self, 4/((exp(-self.v)+exp(self.v))**2))])\n",
        "\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])"
      ],
      "metadata": {
        "id": "hSKslIHFcBuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8n_SKnIW2F"
      },
      "source": [
        "## Exercise g) Complete the forward pass\n",
        "\n",
        "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDEjtePxE7Mv",
        "outputId": "4f1c54f2-1f9f-4091-9d56-0768497444ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Var(v=0.0002, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=-0.0086, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0089, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=-0.0055, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0063, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=-0.0041, grad=0.0000)], [Var(v=-0.0018, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0065, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=0.0006, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=-0.0081, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=-0.0071, grad=0.0000)], [Var(v=-0.0077, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0014, grad=0.0000)], [Var(v=-0.0088, grad=0.0000)], [Var(v=-0.0033, grad=0.0000)], [Var(v=-0.0062, grad=0.0000)], [Var(v=-0.0044, grad=0.0000)], [Var(v=-0.0031, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=-0.0053, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0019, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=-0.0043, grad=0.0000)], [Var(v=0.0006, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=-0.0076, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0054, grad=0.0000)], [Var(v=-0.0088, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=-0.0073, grad=0.0000)], [Var(v=-0.0078, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=-0.0056, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0010, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=-0.0008, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=-0.0039, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=-0.0049, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=-0.0080, grad=0.0000)], [Var(v=-0.0011, grad=0.0000)], [Var(v=-0.0040, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0015, grad=0.0000)], [Var(v=-0.0045, grad=0.0000)], [Var(v=-0.0083, grad=0.0000)], [Var(v=-0.0086, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=-0.0026, grad=0.0000)], [Var(v=-0.0048, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0068, grad=0.0000)], [Var(v=-0.0082, grad=0.0000)], [Var(v=0.0001, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0005, grad=0.0000)], [Var(v=-0.0048, grad=0.0000)], [Var(v=0.0003, grad=0.0000)], [Var(v=-0.0047, grad=0.0000)], [Var(v=-0.0072, grad=0.0000)], [Var(v=0.0002, grad=0.0000)], [Var(v=0.0005, grad=0.0000)]]\n"
          ]
        }
      ],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "def forward(input, network):\n",
        "\n",
        "  def forward_single(x, network):\n",
        "    for layer in network:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  output = [ forward_single(input[n], network) for n in range(len(input))]\n",
        "  return output\n",
        "\n",
        "print(forward(x_train, NN))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (g)**\n",
        "\n",
        "The following code snippet has been implemented ind the previous DenseLayer Class, as the function forward."
      ],
      "metadata": {
        "id": "nNWWvxQCeNf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "    # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input\n",
        "    # to the current layer matches the number of nodes in the current layer\n",
        "    assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "    weights = self.weights\n",
        "    out = []\n",
        "    # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "    # We therefore loop over the (number of) nodes in the current layer:\n",
        "    for j in range(len(weights[0])):\n",
        "        # Initialize the node value depending on its corresponding parameters.\n",
        "        node = self.bias[j]  # <- Insert code\n",
        "        # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "        for i in range(len(single_input)):\n",
        "            node += weights[i][j] * single_input[i]  # <- Insert code\n",
        "        node = self.act_fn(node)\n",
        "        out.append(node)"
      ],
      "metadata": {
        "id": "Eg210D_gectN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm"
      },
      "source": [
        "## Exercise h) Print all network parameters\n",
        "\n",
        "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer/solution (h)**\n",
        "\n",
        "The following code snippet has been implemented in the DenseLayer class."
      ],
      "metadata": {
        "id": "vc2akTHMeqFD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iac-VwYGFtGm"
      },
      "outputs": [],
      "source": [
        "    def describe(self):\n",
        "        # return f'Input nodes: {self.n_in}, output nodes: {self.n_out} ||' +  f'Weights of input{i}: ' for i in range(len(weights[0]))\n",
        "        n_out = len(self.weights[0])\n",
        "        n_in = len(self.weights)\n",
        "        params = self.parameters()\n",
        "        description = [[f'Weight from input {j} to node {i}: {params[n_out * j + i]} and bias {params[n_out * n_in + j]}' for i\n",
        "          in range(n_out)] for j in range(n_in)]\n",
        "        return sum(description, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FcylHqLTl-Z"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "\n",
        "# convert from Var to ndarray  \n",
        "def Var_to_nparray(x):\n",
        "  y = np.zeros((len(x),len(x[0])))\n",
        "  for i in range(len(x)):\n",
        "    for j in range(len(x[0])):\n",
        "      y[i,j] = x[i][j].v\n",
        "  return y\n",
        "\n",
        "# define 1-1 network with weight = 1 and relu activation \n",
        "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
        "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
        "\n",
        "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOL2UolJFtHL"
      },
      "outputs": [],
      "source": [
        "# Testing all activation layers\n",
        "\n",
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"identity\": lambda x: x.identity(),\n",
        "    #\"sigmoid\": lambda x: x.sigmoid(),  <- uncomment before sharing\n",
        "    \"relu\": lambda x: x.relu(),\n",
        "    #\"tanh\": lambda x: x.tanh() <- uncomment before sharing\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "The Glorot initialization has the form: \n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
        "\n",
        "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
        "\n",
        "The He initialization is very similar\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs"
      },
      "source": [
        "## Exercise i) Glorot and He initialization\n",
        " \n",
        "Using the Initializer class, implement functions that implement Glorot and He \n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
        "\n",
        "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyk01CgaFtGt"
      },
      "outputs": [],
      "source": [
        "## Glorot\n",
        "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
        "  std = 0.0 # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
        "\n",
        "## He\n",
        "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
        "  std = 0.0 # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk"
      },
      "source": [
        "## Exercise j) Forward pass unit test\n",
        "\n",
        "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
        "\n",
        "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0miqRUAFtHl"
      },
      "outputs": [],
      "source": [
        "# Insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp"
      },
      "source": [
        "# Loss functions\n",
        "\n",
        "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2eDYKvAFtHq"
      },
      "outputs": [],
      "source": [
        "def squared_loss(t, y):\n",
        "  \n",
        "  # add check that sizes agree\n",
        "  \n",
        "  def squared_loss_single(t, y):\n",
        "    Loss = Var(0.0)\n",
        "    for i in range(len(t)): # sum over outputs\n",
        "      Loss += (t[i]-y[i]) ** 2\n",
        "    return Loss\n",
        "\n",
        "  Loss = Var(0.0)\n",
        "  for n in range(len(t)): # sum over training data\n",
        "    Loss += squared_loss_single(t[n],y[n])\n",
        "  return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu"
      },
      "source": [
        "## Exercise j) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
        "$$\n",
        "with $p$ given by the the softmax function in terms of the logits $h$:\n",
        "$$\n",
        "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
        "$$\n",
        "Inserting $p$ in the expression for the loss gives\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
        "$$\n",
        "This is true for $t$ being a one-hot vector. \n",
        "\n",
        "Call the function to convince yourself it works. \n",
        "\n",
        "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
        "\n",
        "Help: You can add these methods in the Var class:\n",
        "\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "    \n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nMuxyfzFtHv"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(t, h):\n",
        "     \n",
        "    Loss = Var(0.0)\n",
        "    # Insert code here\n",
        "    return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy"
      },
      "source": [
        "# Backward pass\n",
        "\n",
        "Now the magic happens! We get the calculation of the gradients for free. Just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHyfPPI9Qqwu"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49biIAYKQ1oG"
      },
      "source": [
        "and the gradients will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGt1bq_Q7uk"
      },
      "outputs": [],
      "source": [
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9"
      },
      "source": [
        "# Backward pass unit test\n",
        "\n",
        "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks!\n",
        "\n",
        "We initialize again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ePmzBzRtdh"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 15, lambda x: x.relu()),\n",
        "    DenseLayer(15, 50, lambda x: x.relu()),\n",
        "    DenseLayer(50, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iRPiQ1ISHw"
      },
      "source": [
        "and make an update:\n",
        "\n",
        "We introduce a help function parameters to have a handle in all parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhAI7eyeznia"
      },
      "outputs": [],
      "source": [
        "print('Network before update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "def parameters(network):\n",
        "  params = []\n",
        "  for layer in range(len(network)):\n",
        "    params += network[layer].parameters()\n",
        "  return params\n",
        "\n",
        "def update_parameters(params, learning_rate=0.01):\n",
        "  for p in params:\n",
        "    p.v -= learning_rate*p.grad\n",
        "\n",
        "def zero_gradients(params):\n",
        "  for p in params:\n",
        "    p.grad = 0.0\n",
        "\n",
        "update_parameters(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "zero_gradients(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after zeroing gradients:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woWYpdw6FtIO"
      },
      "outputs": [],
      "source": [
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "#NN = [\n",
        "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
        "#    DenseLayer(16, 1, lambda x: x.identity())\n",
        "#]\n",
        "\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdqaqYBVFtIR"
      },
      "outputs": [],
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kfg76GMFtIW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VetyRWFwFtIY"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmNi7S-vFtIc"
      },
      "outputs": [],
      "source": [
        "output_test = forward(x_test, NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mmJOTSEFtIf"
      },
      "outputs": [],
      "source": [
        "y_test_np = Var_to_nparray(y_test)\n",
        "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
        "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODi0WlmQFtIh"
      },
      "outputs": [],
      "source": [
        "x_test_np = Var_to_nparray(x_test)\n",
        "x_train_np = Var_to_nparray(x_train)\n",
        "y_train_np = Var_to_nparray(y_train)\n",
        "if D1:\n",
        "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
        "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
        "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk"
      },
      "source": [
        "## Exercise k) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQZCn2dxFtIl"
      },
      "outputs": [],
      "source": [
        "# Insert your code for getting overfitting, underfitting and just right fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo"
      },
      "source": [
        "## Exercise l) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC8QrI2tFtIp"
      },
      "outputs": [],
      "source": [
        "# Just add code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APqhJv3tta1O"
      },
      "source": [
        "## Exercise m) optional - Introduce a NeuralNetwork class\n",
        "\n",
        "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqfnor1ouMLq"
      },
      "outputs": [],
      "source": [
        "# just add some code"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "U4057_ljNvWB",
        "p_8n_SKnIW2F",
        "oLrGJytZFtGm",
        "jpIZPBpNI0pO",
        "_79HOAXrFtHK",
        "mqeyab9qFtGs",
        "-XyXBD37FtHk",
        "SrwSJ2UWFtHu",
        "zTBAmjsAFtIk",
        "qsVPul3QFtIo",
        "APqhJv3tta1O"
      ],
      "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}