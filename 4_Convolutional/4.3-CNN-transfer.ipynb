{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZW0gaQO81Sq"
   },
   "source": [
    "# Transfer learning on the Caltech101 dataset\n",
    "\n",
    "In this notebook, we will consider a more complex dataset than MNIST or CIFAR10. The images in Caltech101 are RGB images (3 channels) with variable size. There are 101 different classes. We will try a very common practice in computer vision nowadays: transfer learning from a pre-trained ImageNet model. \n",
    "\n",
    "Roadmap:\n",
    "- Modify the network from the previous exercise (CIFAR-10) to work with 224x224 images.\n",
    "- Train the model for a while on Caltech101 and see how far we can get.\n",
    "- Take a ResNet34 that was pre-trained on ImageNet-1k and fine-tune it to Caltech101.\n",
    "  - Consider both training only the head (the linear classifier at the end of the network) or the entire network.\n",
    "  - We should be able to reach better performance than our original network in fewer training steps.\n",
    "- Optional: play around with other pre-trained models from `timm` (see info [here](https://github.com/rwightman/pytorch-image-models)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htyg7xxN81St"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3u2GIWr81Su",
    "outputId": "4bbb197c-5bf8-4f82-f74d-28fcddc175be"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from typing import List, Optional, Callable, Iterator\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "!pip install timm\n",
    "import timm\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    img = img.detach().cpu()\n",
    "    img = img.permute((1, 2, 0)).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean   # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.gca().tick_params(axis=\"both\", which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRJYSCs5BUOf"
   },
   "source": [
    "### Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZeTujLC81S3",
    "outputId": "b46495bc-cfc3-4f0b-cf5e-db4fc38cb372"
   },
   "outputs": [],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "default_imagenet_normalization = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.25, 1)),  # crop of random size and aspect ratio, resize to square\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # keep aspect ratio\n",
    "    transforms.CenterCrop(224),  # square crop\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "\n",
    "batch_size = 64  # both for training and testing\n",
    "\n",
    "# Load dataset (downloaded automatically if missing).\n",
    "dataset = torchvision.datasets.Caltech101(root='./data', download=True)\n",
    "\n",
    "\n",
    "def translate_label(y, keep_classes):\n",
    "    try:\n",
    "        return keep_classes.index(y)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Filter images such that no class has more than 100 examples.\n",
    "# Loop through dataset in random order, include each image (its index) only if there are \n",
    "# fewer than 100 images of the same class.\n",
    "# Here we also remove the classes \"Faces\" and \"Faces_easy\".\n",
    "skip_classes = [0, 1]   # \"Faces\" and \"Faces_easy\"\n",
    "keep_classes = [c for c in range(len(dataset.categories)) if c not in skip_classes]\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "new_indices = []\n",
    "current_class_size = {class_number: 0 for class_number in np.unique(dataset.y)}\n",
    "for i in indices:\n",
    "    class_number = dataset.y[i]\n",
    "    if class_number not in skip_classes and current_class_size[class_number] < 100:\n",
    "        new_indices.append(i)\n",
    "        current_class_size[class_number] += 1\n",
    "indices = new_indices\n",
    "\n",
    "# Compute subset of labels given new indices selection\n",
    "labels = [translate_label(dataset.y[i], keep_classes) for i in indices]\n",
    "assert max(labels) == 98\n",
    "\n",
    "test_size = 640\n",
    "train_size = len(indices) - test_size\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    random_state=42,  # always get the same split\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "\n",
    "class DatasetSubsetWithTransform(Subset):\n",
    "    \n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = super().__getitem__(idx)\n",
    "        # Convert PIL image to RGB (some of them are greyscale)\n",
    "        x = x.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = translate_label(y, keep_classes)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "train_set = DatasetSubsetWithTransform(dataset, train_idx, train_transform)\n",
    "test_set = DatasetSubsetWithTransform(dataset, test_idx, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnIEkOhCBa5k"
   },
   "source": [
    "### A closer look at the dataset\n",
    "\n",
    "We first plot the size of each class and observe the class distribution is not uniform.\n",
    "\n",
    "Then we show random examples from the dataset, annotated with the class label and index.\n",
    "Note that the training images include standard augmentations typically used for vision models (defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MV8Th7V6eYx0",
    "outputId": "26bdc902-aa19-41b1-a2ad-54174c1095d1"
   },
   "outputs": [],
   "source": [
    "label_idxs, counts = np.unique(labels, return_counts=True)\n",
    "plt.figure(figsize=(20, 4.2))\n",
    "sns.barplot(x=[dataset.categories[keep_classes[label]] for label in label_idxs], y=counts, color=sns.color_palette('muted')[0])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.title(\"Class distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def show_dataset_examples(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    with sns.axes_style(\"white\"):\n",
    "      fig, axes = plt.subplots(4, 8, figsize=(16, 9.5))\n",
    "    axes = [ax for axes_ in axes for ax in axes_]   # flatten\n",
    "    for j, (img, label) in enumerate(zip(images[:32], labels[:32])):\n",
    "        plt.sca(axes[j])\n",
    "        show_image(img, title=f\"{dataset.categories[keep_classes[label.item()]]} ({label.item()})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nTrain images (including augmentations):\")\n",
    "show_dataset_examples(train_loader)\n",
    "print(\"\\n\\nTest images:\")\n",
    "show_dataset_examples(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt3BVFMF81TI"
   },
   "source": [
    "## Define a neural network\n",
    "\n",
    "**Assignment 1:** Adapt the CNN from the previous lab (CIFAR-10) to handle 224x224 images. We recommend reducing significantly the size of the tensors before flattening, by adding either convolutional layers with stride>1 or [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layers (but see e.g. also [AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VbF_R2IMrDT",
    "outputId": "62ddf5f5-2408-4aef-cc03-b0e95a11af36"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = ...   # Your code here!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "model = Model(num_classes=len(np.unique(labels)))\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-IUg3sq81TQ"
   },
   "source": [
    "## Define loss function and optimizer\n",
    "\n",
    "**Assignment 2:** Implement the criterion and optimizer, as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48AX85QP81TR"
   },
   "outputs": [],
   "source": [
    "loss_fn = None  # Your code here!\n",
    "optimizer = None  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WneIN7C81TV"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWT0ULctWvm1",
    "outputId": "632ea54a-8070-4310-bcaa-f1b9b0f354c4"
   },
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model(torch.randn(2, 3, 224, 224).to(device))\n",
    "print(\"Output shape:\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "NkUanRRb81TW",
    "outputId": "f024acc3-a195-44e4-b4d9-9c512f6c05f7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "validation_every_steps = 50\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass.\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output = model(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qAsbC8I81Ta"
   },
   "source": [
    "## Test the network on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Smv6_BwF81Ti",
    "outputId": "a65a986a-3497-4aa4-dd5f-3bedc949e84d"
   },
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbn2kNxCRFoc"
   },
   "source": [
    "## Using a pre-trained model\n",
    "\n",
    "Here we will load a ResNet34 that was pre-trained on ImageNet. We then discard the linear classifier at the end of the network (the \"head\" of the network) and replace it with a new one that outputs the desired number of logits for classification. To get a rough idea of the structure of the model, we print it below.\n",
    "\n",
    "The argument `finetune_entire_model` in `initialize_model()` controls whether the entire pre-trained model is fine-tuned. When this is `False`, only the linear head is trained, and the rest of the model is fixed. The idea is that the features extracted by the ImageNet model, up to the final classification layer, are very informative also on other datasets (see, e.g., [this paper](https://arxiv.org/abs/1910.04867) on the transferability of deep representations in large vision models).\n",
    "\n",
    "We will start here by training only the linear head. You can experiment different models and variations.\n",
    "\n",
    "Below, we define the model and forget the one we just trained. After that, you can go back to the section \"Define loss function and optimizer\" and re-execute the notebook from there, to train and evaluate the new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4W_fQASeYx1",
    "outputId": "40878ae3-c3fa-41d3-af75-43f31822f115"
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):\n",
    "    \"\"\"Returns a pretrained model with a new last layer, and a dict with additional info.\n",
    "\n",
    "    The dict now contains the number of model parameters, computed as the number of\n",
    "    trainable parameters as soon as the model is loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Loading model '{model_name}', with \"\n",
    "        f\"finetune_entire_model={finetune_entire_model}, changing the \"\n",
    "        f\"last layer to output {num_classes} logits.\"\n",
    "    )\n",
    "    model = timm.create_model(\n",
    "        model_name, pretrained=True, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    num_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_model_parameters += p.numel()\n",
    "\n",
    "    if not finetune_entire_model:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Layer names are not consistent, so we have to consider a few cases. This might \n",
    "        # break for arbitrary models from timm (we have not checked all available models).\n",
    "        layer = None\n",
    "        if hasattr(model, \"classifier\"):\n",
    "            if isinstance(model.classifier, nn.Linear):\n",
    "                layer = model.classifier\n",
    "        elif hasattr(model, \"head\"):\n",
    "            if isinstance(model.head, nn.Linear):\n",
    "                layer = model.head\n",
    "            elif hasattr(model.head, \"fc\") and isinstance(model.head.fc, nn.Linear):\n",
    "                layer = model.head.fc\n",
    "            elif hasattr(model.head, \"l\") and isinstance(model.head.l, nn.Linear):\n",
    "                layer = model.head.l\n",
    "        elif hasattr(model, \"fc\"):\n",
    "            if isinstance(model.fc, nn.Linear):\n",
    "                layer = model.fc\n",
    "        if layer is None:\n",
    "            raise ValueError(f\"Couldn't automatically find last layer of model.\")\n",
    "        \n",
    "        # Make the last layer trainable.\n",
    "        layer.weight.requires_grad_()\n",
    "        layer.bias.requires_grad_()\n",
    "\n",
    "    num_trainable_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_trainable_model_parameters += p.numel()\n",
    "\n",
    "    return model, {\n",
    "        \"num_model_parameters\": num_model_parameters,\n",
    "        \"num_trainable_model_parameters\": num_trainable_model_parameters,\n",
    "    }\n",
    "\n",
    "model, data = initialize_model('resnet34d', num_classes=len(np.unique(labels)), finetune_entire_model=False)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of model parameters:\", data[\"num_model_parameters\"])\n",
    "print(\"Number of trainable parameters:\", data[\"num_trainable_model_parameters\"])\n",
    "\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocnQOBAl81Tn"
   },
   "source": [
    "**Assignment 3:** \n",
    "\n",
    "1. Train the linear classifier on top of the pre-trained network, and observe how quickly you can get pretty good results, compared to training a smaller network from scratch as above.\n",
    "\n",
    "2. Go back and change argument to finetune entire network, maybe adjust learning rate, see if you can get better performance than before and if you run into any issues.\n",
    "\n",
    "3. Optional: experiment with `timm`: try smaller or larger models, including state-of-the-art models, e.g. based on vision transformers (ViT) or MLP-Mixers.\n",
    "\n",
    "4. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
    "Did anything surprise you during the exercise?\n",
    "\n",
    "5. Write down key lessons/insights you got during this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4.3-EXE-transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
